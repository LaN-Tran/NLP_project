{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of BertModel_Project.ipynb","provenance":[{"file_id":"1rGXHEGJlHmqCdN7IS9-l6xmk12IMyjtP","timestamp":1619808075184}],"collapsed_sections":[],"authorship_tag":"ABX9TyOIVy/i4NTCAPbMEU3rFSju"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"47b290bb238347bfb55fa927e256d7fb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f3b7c5e482644b318c64effaa9997f89","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6013050799734160be3aa1cf5076b5a9","IPY_MODEL_2fe0afe9e437476ab6d33772510e05bf"]}},"f3b7c5e482644b318c64effaa9997f89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6013050799734160be3aa1cf5076b5a9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_16acaefee4704d72997b4b46a1369677","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e32da7d116734365b0a5af777198edc4"}},"2fe0afe9e437476ab6d33772510e05bf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_62d71b4162e24e4b94c96a81b65ea4bb","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:58&lt;00:00, 3.98kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6a0cf4e28987412da1a1723f0979aa1e"}},"16acaefee4704d72997b4b46a1369677":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e32da7d116734365b0a5af777198edc4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"62d71b4162e24e4b94c96a81b65ea4bb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6a0cf4e28987412da1a1723f0979aa1e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5d4e558f05244f93b7d1b7565cfa356e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6deb7113b3da4419a246d3a4f9f2353d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e0bd0a5ad6d04d7f9dcfed5d77d67674","IPY_MODEL_66731c1bcd17431791b92a74461942f7"]}},"6deb7113b3da4419a246d3a4f9f2353d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e0bd0a5ad6d04d7f9dcfed5d77d67674":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_518c153519c6436abec651ccb38a0b95","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":466062,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":466062,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_61c24bf914be4dd889a68bcdd38bcf19"}},"66731c1bcd17431791b92a74461942f7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e2a537e16774451d934f0baaaa63b7a6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 466k/466k [00:06&lt;00:00, 71.5kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bfa679e78bc1480b9a355bb7c1fcb22d"}},"518c153519c6436abec651ccb38a0b95":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"61c24bf914be4dd889a68bcdd38bcf19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e2a537e16774451d934f0baaaa63b7a6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"bfa679e78bc1480b9a355bb7c1fcb22d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ae851b8c30f94ab7b49a7d165563232a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_84d3235814004c95be510b2a87e94043","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_800c5d9fc1a347af97a5f9d22e1f4348","IPY_MODEL_1bc1cbed996e4ba49c2064592d4998a5"]}},"84d3235814004c95be510b2a87e94043":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"800c5d9fc1a347af97a5f9d22e1f4348":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_fce1ca22f27147db82516d44f37c2ec3","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":28,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d2a9700a83274ad599a82228bcd78ce9"}},"1bc1cbed996e4ba49c2064592d4998a5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ef2f6886e8c34c40980173fa6c21455e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 28.0/28.0 [00:06&lt;00:00, 4.62B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d0c64aa9e9e54d22bde3683c80a35f0a"}},"fce1ca22f27147db82516d44f37c2ec3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d2a9700a83274ad599a82228bcd78ce9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ef2f6886e8c34c40980173fa6c21455e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d0c64aa9e9e54d22bde3683c80a35f0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b7fa0420be3440b5b3cdbf69f85a4879":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_097ab935f58f44d6b71f0751ad138662","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a52174f841c74fe7b155d0ce65b4c90e","IPY_MODEL_ad891c93cb0f4c199d32bdd294012e69"]}},"097ab935f58f44d6b71f0751ad138662":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a52174f841c74fe7b155d0ce65b4c90e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_870f66b2dd814ffe9acd59bf94ee069a","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":267967963,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":267967963,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1d2ee3330dc34f9a9ba302bf336a7ec5"}},"ad891c93cb0f4c199d32bdd294012e69":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3fa39c6531804843966ba51beb5eb9c5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 268M/268M [00:25&lt;00:00, 10.4MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_98ac8bfad7544378910ecf6d5f959cbc"}},"870f66b2dd814ffe9acd59bf94ee069a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1d2ee3330dc34f9a9ba302bf336a7ec5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3fa39c6531804843966ba51beb5eb9c5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"98ac8bfad7544378910ecf6d5f959cbc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dd750c7387a6432faf0313a71a113fff":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c7f53c184ba94708a30ec126900e17ef","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7ca34c09c03145dea745a9b51dda5175","IPY_MODEL_d5125b1faebc440cb50c7a0c4438c46a"]}},"c7f53c184ba94708a30ec126900e17ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7ca34c09c03145dea745a9b51dda5175":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_26562fa1432d407fb6103acbd18530fc","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":267967963,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":267967963,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_987766dd951344a5baf1a13b62d03ec8"}},"d5125b1faebc440cb50c7a0c4438c46a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7b0b676fe4774580bfe2f8db1880df5b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 268M/268M [00:07&lt;00:00, 34.4MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3196e196a4cc47b3a79c9936d5b77a98"}},"26562fa1432d407fb6103acbd18530fc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"987766dd951344a5baf1a13b62d03ec8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7b0b676fe4774580bfe2f8db1880df5b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3196e196a4cc47b3a79c9936d5b77a98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"9xRzApEvQ3Ex"},"source":["#PREPARE GPU FOR the training model"]},{"cell_type":"markdown","metadata":{"id":"TRdXGjr9lw_M"},"source":["#REFERENCES:"]},{"cell_type":"markdown","metadata":{"id":"wGFVI6dulzLn"},"source":["## Reference_1: Fine tune Bert https://www.youtube.com/watch?v=x66kkDnbzi4 by ChrisMcCormickAI\n"]},{"cell_type":"markdown","metadata":{"id":"_kqyH5Nrl68Y"},"source":["## Reference_2: applying SQUAD 1.0 dataset to BertForAnsweringQuestion already trained with SQUAD: https://www.youtube.com/watch?v=l8ZYCvgGu0o&list=WL&index=118&t=878s by ChrisMcCormickAI"]},{"cell_type":"markdown","metadata":{"id":"8bB-NNl9mKlK"},"source":["## Reference_3: 'Question Answering with SQuAD 2.0' section from: https://huggingface.co/transformers/custom_datasets"]},{"cell_type":"markdown","metadata":{"id":"Xpehpj9xmDHU"},"source":["## Reference_4: Basic knowledge about fine tuning, input formate and output format of BERT models : https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/?fbclid=IwAR3uWlc8mUlrJ3QnYoYyQOfze3yDYkacgVyKSk24YjYE04Gs-7XiM3b9gTA "]},{"cell_type":"markdown","metadata":{"id":"x0RdLqN2mg1v"},"source":["#SET UP GPU FOR TRAINING MODEL:"]},{"cell_type":"markdown","metadata":{"id":"mjFzznLlln9t"},"source":["**FIRSTLY, Setup GPU for training**\n","edit -> Notebook setting -> Hardware accelerator -> GPU"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":203},"id":"2UWDmvmpQ6cT","executionInfo":{"status":"error","timestamp":1620570846665,"user_tz":-120,"elapsed":735,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"9ef4898b-e39c-4f2d-dc05-eb18b1bed389"},"source":["import tensorflow as tf\n","\n","# Get GPU device name:\n","device_name= tf.test.gpu_device_name()\n","\n","# GPU device should have the following name:\n","if device_name == \"/device:GPU:0\":\n","  print(\"Found GPU at: \" + device_name)\n","else:\n","  raise SystemError(\"GPU not found\") # \"GPU not found\" a parameter to pass to the SystemError for printing out  "],"execution_count":2,"outputs":[{"output_type":"error","ename":"SystemError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-15214783ad44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Found GPU at: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GPU not found\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# \"GPU not found\" a parameter to pass to the SystemError for printing out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mSystemError\u001b[0m: GPU not found"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pSkc_2hLSt_h","executionInfo":{"status":"ok","timestamp":1620570853538,"user_tz":-120,"elapsed":3481,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"1a73f0cd-4a74-475f-9dbd-bd59d4eb99ad"},"source":["import torch\n","\n","# if there is a GPU device available..\n","if torch.cuda.is_available():\n","\n","  # Tell TORCH to use this GPU:\n","  device= torch.device(\"cuda\")\n","\n","  print(\"There are %d GPU(s) available\" % torch.cuda.device_count())\n","\n","  print('we will use the GPU: ', torch.cuda.get_device_name(0))\n","\n","# if not:\n","else:\n","  print('NO GPU available, using CPU instead')\n","  device = torch.device(\"cpu\")"],"execution_count":3,"outputs":[{"output_type":"stream","text":["NO GPU available, using CPU instead\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XRl4SHS9-DHW"},"source":["#IMPORT DATASET"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":581},"id":"6IVbSVon9-cd","executionInfo":{"status":"ok","timestamp":1620570857195,"user_tz":-120,"elapsed":585,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"539f57b8-9531-47df-8885-4e812eb13be6"},"source":["\n","\n","# libraries for project:\n","import pandas as pd\n","import tensorflow as tf\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from collections import defaultdict\n","# necessary libraries for project:\n","\n","dataset = pd.read_table('./data/final.tsv')  \n","dataset"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>question</th>\n","      <th>is_impossible</th>\n","      <th>text</th>\n","      <th>answer_start</th>\n","      <th>context</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>When did Beyonce start becoming popular?</td>\n","      <td>False</td>\n","      <td>in the late 1990s</td>\n","      <td>269</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>What areas did Beyonce compete in when she was...</td>\n","      <td>False</td>\n","      <td>singing and dancing</td>\n","      <td>207</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>When did Beyonce leave Destiny's Child and bec...</td>\n","      <td>False</td>\n","      <td>2003</td>\n","      <td>526</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>In what city and state did Beyonce  grow up?</td>\n","      <td>False</td>\n","      <td>Houston, Texas</td>\n","      <td>166</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>In which decade did Beyonce become famous?</td>\n","      <td>False</td>\n","      <td>late 1990s</td>\n","      <td>276</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9996</th>\n","      <td>What famous World War II battle was the Canadi...</td>\n","      <td>False</td>\n","      <td>the Normandy Landings</td>\n","      <td>166</td>\n","      <td>Battles which are particularly notable to the ...</td>\n","    </tr>\n","    <tr>\n","      <th>9997</th>\n","      <td>What effort was the Canadian Military known fo...</td>\n","      <td>False</td>\n","      <td>the strategic bombing of German cities</td>\n","      <td>288</td>\n","      <td>Battles which are particularly notable to the ...</td>\n","    </tr>\n","    <tr>\n","      <th>9998</th>\n","      <td>What Battle in France was the Canadian Militar...</td>\n","      <td>False</td>\n","      <td>the Battle of Vimy Ridge</td>\n","      <td>72</td>\n","      <td>Battles which are particularly notable to the ...</td>\n","    </tr>\n","    <tr>\n","      <th>9999</th>\n","      <td>What country was the latest Canadian Military ...</td>\n","      <td>False</td>\n","      <td>Croatia</td>\n","      <td>377</td>\n","      <td>Battles which are particularly notable to the ...</td>\n","    </tr>\n","    <tr>\n","      <th>10000</th>\n","      <td>Who are the Battle of Normandy Landings and th...</td>\n","      <td>True</td>\n","      <td>the Canadian military</td>\n","      <td>42</td>\n","      <td>Battles which are particularly notable to the ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10001 rows × 5 columns</p>\n","</div>"],"text/plain":["                                                question  ...                                            context\n","0               When did Beyonce start becoming popular?  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","1      What areas did Beyonce compete in when she was...  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","2      When did Beyonce leave Destiny's Child and bec...  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","3          In what city and state did Beyonce  grow up?   ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","4             In which decade did Beyonce become famous?  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","...                                                  ...  ...                                                ...\n","9996   What famous World War II battle was the Canadi...  ...  Battles which are particularly notable to the ...\n","9997   What effort was the Canadian Military known fo...  ...  Battles which are particularly notable to the ...\n","9998   What Battle in France was the Canadian Militar...  ...  Battles which are particularly notable to the ...\n","9999   What country was the latest Canadian Military ...  ...  Battles which are particularly notable to the ...\n","10000  Who are the Battle of Normandy Landings and th...  ...  Battles which are particularly notable to the ...\n","\n","[10001 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"BX4IdqIz-lL4"},"source":["##TRIM (Or SAMPLE) DOWN THE DATASET FOR TRAINING:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":581},"id":"lqSTataa-hpY","executionInfo":{"status":"ok","timestamp":1620539455095,"user_tz":-120,"elapsed":606,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"3fe9ef09-21ac-44c9-9fac-3813c985b482"},"source":["# shuffling the dataset first beforing triming down:\n","# reference: https://stackoverflow.com/questions/29576430/shuffle-dataframe-rows\n","#dataset = dataset.sample(frac=1).reset_index(drop=True)\n","dataset"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>question</th>\n","      <th>is_impossible</th>\n","      <th>text</th>\n","      <th>answer_start</th>\n","      <th>context</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>When did Beyonce start becoming popular?</td>\n","      <td>False</td>\n","      <td>in the late 1990s</td>\n","      <td>269</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>What areas did Beyonce compete in when she was...</td>\n","      <td>False</td>\n","      <td>singing and dancing</td>\n","      <td>207</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>When did Beyonce leave Destiny's Child and bec...</td>\n","      <td>False</td>\n","      <td>2003</td>\n","      <td>526</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>In what city and state did Beyonce  grow up?</td>\n","      <td>False</td>\n","      <td>Houston, Texas</td>\n","      <td>166</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>In which decade did Beyonce become famous?</td>\n","      <td>False</td>\n","      <td>late 1990s</td>\n","      <td>276</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9996</th>\n","      <td>What famous World War II battle was the Canadi...</td>\n","      <td>False</td>\n","      <td>the Normandy Landings</td>\n","      <td>166</td>\n","      <td>Battles which are particularly notable to the ...</td>\n","    </tr>\n","    <tr>\n","      <th>9997</th>\n","      <td>What effort was the Canadian Military known fo...</td>\n","      <td>False</td>\n","      <td>the strategic bombing of German cities</td>\n","      <td>288</td>\n","      <td>Battles which are particularly notable to the ...</td>\n","    </tr>\n","    <tr>\n","      <th>9998</th>\n","      <td>What Battle in France was the Canadian Militar...</td>\n","      <td>False</td>\n","      <td>the Battle of Vimy Ridge</td>\n","      <td>72</td>\n","      <td>Battles which are particularly notable to the ...</td>\n","    </tr>\n","    <tr>\n","      <th>9999</th>\n","      <td>What country was the latest Canadian Military ...</td>\n","      <td>False</td>\n","      <td>Croatia</td>\n","      <td>377</td>\n","      <td>Battles which are particularly notable to the ...</td>\n","    </tr>\n","    <tr>\n","      <th>10000</th>\n","      <td>Who are the Battle of Normandy Landings and th...</td>\n","      <td>True</td>\n","      <td>the Canadian military</td>\n","      <td>42</td>\n","      <td>Battles which are particularly notable to the ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10001 rows × 5 columns</p>\n","</div>"],"text/plain":["                                                question  ...                                            context\n","0               When did Beyonce start becoming popular?  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","1      What areas did Beyonce compete in when she was...  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","2      When did Beyonce leave Destiny's Child and bec...  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","3          In what city and state did Beyonce  grow up?   ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","4             In which decade did Beyonce become famous?  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","...                                                  ...  ...                                                ...\n","9996   What famous World War II battle was the Canadi...  ...  Battles which are particularly notable to the ...\n","9997   What effort was the Canadian Military known fo...  ...  Battles which are particularly notable to the ...\n","9998   What Battle in France was the Canadian Militar...  ...  Battles which are particularly notable to the ...\n","9999   What country was the latest Canadian Military ...  ...  Battles which are particularly notable to the ...\n","10000  Who are the Battle of Normandy Landings and th...  ...  Battles which are particularly notable to the ...\n","\n","[10001 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":581},"id":"bg9aHfVx-wYJ","executionInfo":{"status":"ok","timestamp":1620570863143,"user_tz":-120,"elapsed":654,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"bf242140-d0a4-4fc6-d912-4a344541028d"},"source":["# SAMPLE DOWN NUMBER OF DATASET FOR TRAINING AND EVALUATION:\n","dataset=dataset.iloc[:10000,:]\n","dataset"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>question</th>\n","      <th>is_impossible</th>\n","      <th>text</th>\n","      <th>answer_start</th>\n","      <th>context</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>When did Beyonce start becoming popular?</td>\n","      <td>False</td>\n","      <td>in the late 1990s</td>\n","      <td>269</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>What areas did Beyonce compete in when she was...</td>\n","      <td>False</td>\n","      <td>singing and dancing</td>\n","      <td>207</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>When did Beyonce leave Destiny's Child and bec...</td>\n","      <td>False</td>\n","      <td>2003</td>\n","      <td>526</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>In what city and state did Beyonce  grow up?</td>\n","      <td>False</td>\n","      <td>Houston, Texas</td>\n","      <td>166</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>In which decade did Beyonce become famous?</td>\n","      <td>False</td>\n","      <td>late 1990s</td>\n","      <td>276</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9995</th>\n","      <td>What type of military vehicle was maintained a...</td>\n","      <td>True</td>\n","      <td>an aircraft carrier</td>\n","      <td>528</td>\n","      <td>Since 1947, Canadian military units have parti...</td>\n","    </tr>\n","    <tr>\n","      <th>9996</th>\n","      <td>What famous World War II battle was the Canadi...</td>\n","      <td>False</td>\n","      <td>the Normandy Landings</td>\n","      <td>166</td>\n","      <td>Battles which are particularly notable to the ...</td>\n","    </tr>\n","    <tr>\n","      <th>9997</th>\n","      <td>What effort was the Canadian Military known fo...</td>\n","      <td>False</td>\n","      <td>the strategic bombing of German cities</td>\n","      <td>288</td>\n","      <td>Battles which are particularly notable to the ...</td>\n","    </tr>\n","    <tr>\n","      <th>9998</th>\n","      <td>What Battle in France was the Canadian Militar...</td>\n","      <td>False</td>\n","      <td>the Battle of Vimy Ridge</td>\n","      <td>72</td>\n","      <td>Battles which are particularly notable to the ...</td>\n","    </tr>\n","    <tr>\n","      <th>9999</th>\n","      <td>What country was the latest Canadian Military ...</td>\n","      <td>False</td>\n","      <td>Croatia</td>\n","      <td>377</td>\n","      <td>Battles which are particularly notable to the ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10000 rows × 5 columns</p>\n","</div>"],"text/plain":["                                               question  ...                                            context\n","0              When did Beyonce start becoming popular?  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","1     What areas did Beyonce compete in when she was...  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","2     When did Beyonce leave Destiny's Child and bec...  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","3         In what city and state did Beyonce  grow up?   ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","4            In which decade did Beyonce become famous?  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","...                                                 ...  ...                                                ...\n","9995  What type of military vehicle was maintained a...  ...  Since 1947, Canadian military units have parti...\n","9996  What famous World War II battle was the Canadi...  ...  Battles which are particularly notable to the ...\n","9997  What effort was the Canadian Military known fo...  ...  Battles which are particularly notable to the ...\n","9998  What Battle in France was the Canadian Militar...  ...  Battles which are particularly notable to the ...\n","9999  What country was the latest Canadian Military ...  ...  Battles which are particularly notable to the ...\n","\n","[10000 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"TUaLEj7c_CS2"},"source":["#EXTRACTING THE START AND END TOKENS OF AN ANSWER IN A CONTEXT"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":688},"id":"UhhB5pVh_J3i","executionInfo":{"status":"ok","timestamp":1620570868875,"user_tz":-120,"elapsed":651,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"244a1c35-a628-4f8a-9d8a-3da1f8ee31a1"},"source":["# Small run:\n","# drop the question mark in the 'question' column:\n","def Drop_question_mark (quest):\n","  l= len(quest)\n","  return quest[:l-1]\n","\n","dataset['question'] = dataset['question'].apply(Drop_question_mark)\n","dataset"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  import sys\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>question</th>\n","      <th>is_impossible</th>\n","      <th>text</th>\n","      <th>answer_start</th>\n","      <th>context</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>When did Beyonce start becoming popular</td>\n","      <td>False</td>\n","      <td>in the late 1990s</td>\n","      <td>269</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>What areas did Beyonce compete in when she was...</td>\n","      <td>False</td>\n","      <td>singing and dancing</td>\n","      <td>207</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>When did Beyonce leave Destiny's Child and bec...</td>\n","      <td>False</td>\n","      <td>2003</td>\n","      <td>526</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>In what city and state did Beyonce  grow up?</td>\n","      <td>False</td>\n","      <td>Houston, Texas</td>\n","      <td>166</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>In which decade did Beyonce become famous</td>\n","      <td>False</td>\n","      <td>late 1990s</td>\n","      <td>276</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9995</th>\n","      <td>What type of military vehicle was maintained a...</td>\n","      <td>True</td>\n","      <td>an aircraft carrier</td>\n","      <td>528</td>\n","      <td>Since 1947, Canadian military units have parti...</td>\n","    </tr>\n","    <tr>\n","      <th>9996</th>\n","      <td>What famous World War II battle was the Canadi...</td>\n","      <td>False</td>\n","      <td>the Normandy Landings</td>\n","      <td>166</td>\n","      <td>Battles which are particularly notable to the ...</td>\n","    </tr>\n","    <tr>\n","      <th>9997</th>\n","      <td>What effort was the Canadian Military known fo...</td>\n","      <td>False</td>\n","      <td>the strategic bombing of German cities</td>\n","      <td>288</td>\n","      <td>Battles which are particularly notable to the ...</td>\n","    </tr>\n","    <tr>\n","      <th>9998</th>\n","      <td>What Battle in France was the Canadian Militar...</td>\n","      <td>False</td>\n","      <td>the Battle of Vimy Ridge</td>\n","      <td>72</td>\n","      <td>Battles which are particularly notable to the ...</td>\n","    </tr>\n","    <tr>\n","      <th>9999</th>\n","      <td>What country was the latest Canadian Military ...</td>\n","      <td>False</td>\n","      <td>Croatia</td>\n","      <td>377</td>\n","      <td>Battles which are particularly notable to the ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10000 rows × 5 columns</p>\n","</div>"],"text/plain":["                                               question  ...                                            context\n","0               When did Beyonce start becoming popular  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","1     What areas did Beyonce compete in when she was...  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","2     When did Beyonce leave Destiny's Child and bec...  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","3          In what city and state did Beyonce  grow up?  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","4             In which decade did Beyonce become famous  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","...                                                 ...  ...                                                ...\n","9995  What type of military vehicle was maintained a...  ...  Since 1947, Canadian military units have parti...\n","9996  What famous World War II battle was the Canadi...  ...  Battles which are particularly notable to the ...\n","9997  What effort was the Canadian Military known fo...  ...  Battles which are particularly notable to the ...\n","9998  What Battle in France was the Canadian Militar...  ...  Battles which are particularly notable to the ...\n","9999  What country was the latest Canadian Military ...  ...  Battles which are particularly notable to the ...\n","\n","[10000 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"b0Eugu8h_ixA"},"source":["##Import BertTokenizer for extracting the end and start tokens of an answer (in this case, the 'text') in a 'context'. [BertTokenizer also later used for tokenized and add special tokens [CLS], [SEP] for the input data of the Bert model]"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"csiKNpsL_e7D","executionInfo":{"status":"ok","timestamp":1620570880059,"user_tz":-120,"elapsed":8416,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"2b69e92f-7574-4c57-bf00-929fe704cb61"},"source":["# INSTALL TRANSFORMER TO IMPORT BERT:\n","! pip install transformers"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.1MB 6.3MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 32.5MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 35.3MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Installing collected packages: tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AnXal1rUANd1","executionInfo":{"status":"ok","timestamp":1620570889985,"user_tz":-120,"elapsed":622,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["from transformers import DistilBertTokenizerFast"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"EFch0LTeAQtp","colab":{"base_uri":"https://localhost:8080/","height":166,"referenced_widgets":["47b290bb238347bfb55fa927e256d7fb","f3b7c5e482644b318c64effaa9997f89","6013050799734160be3aa1cf5076b5a9","2fe0afe9e437476ab6d33772510e05bf","16acaefee4704d72997b4b46a1369677","e32da7d116734365b0a5af777198edc4","62d71b4162e24e4b94c96a81b65ea4bb","6a0cf4e28987412da1a1723f0979aa1e","5d4e558f05244f93b7d1b7565cfa356e","6deb7113b3da4419a246d3a4f9f2353d","e0bd0a5ad6d04d7f9dcfed5d77d67674","66731c1bcd17431791b92a74461942f7","518c153519c6436abec651ccb38a0b95","61c24bf914be4dd889a68bcdd38bcf19","e2a537e16774451d934f0baaaa63b7a6","bfa679e78bc1480b9a355bb7c1fcb22d","ae851b8c30f94ab7b49a7d165563232a","84d3235814004c95be510b2a87e94043","800c5d9fc1a347af97a5f9d22e1f4348","1bc1cbed996e4ba49c2064592d4998a5","fce1ca22f27147db82516d44f37c2ec3","d2a9700a83274ad599a82228bcd78ce9","ef2f6886e8c34c40980173fa6c21455e","d0c64aa9e9e54d22bde3683c80a35f0a"]},"executionInfo":{"status":"ok","timestamp":1620570893817,"user_tz":-120,"elapsed":1772,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"7629d30e-1972-49bf-a60b-33f9b9d155e6"},"source":["#Load the BERT tokenizer:\n","tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"],"execution_count":9,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"47b290bb238347bfb55fa927e256d7fb","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5d4e558f05244f93b7d1b7565cfa356e","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ae851b8c30f94ab7b49a7d165563232a","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"f9OHS7DoAX12"},"source":["##Extracting the necessary data from the 'dataset', such as 'context', 'text', 'question' as lists of data elements for easily hanlding this task. [Later these data lists also used as input for step TRANSFORMING the dataset into appropriate format input of Bert model]"]},{"cell_type":"code","metadata":{"id":"ac_PNbRYBAK1","executionInfo":{"status":"ok","timestamp":1620570897107,"user_tz":-120,"elapsed":532,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["# list of all the 'context' from the dataset\n","context= dataset.context.values\n","\n","# list of all the 'question' from the dataset\n","question = dataset.question.values\n","\n","# list of all the 'answer' from the dataset\n","answers = dataset.text.values\n"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RlpqKw7-BNMk"},"source":["##THE MAIN STEP : for extracting the end and start tokens of an 'answer' in a 'context'"]},{"cell_type":"code","metadata":{"id":"MnfbCBj0BUSX","executionInfo":{"status":"ok","timestamp":1620570900399,"user_tz":-120,"elapsed":560,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["def extract_answer_start_end_tokens(answer, context):\n","  for i in range (0, len(context)-(len(answer)-2)+1):\n","    if context[i] == answer[1]: # find the first token of the answer in the context\n","      \n","      for j in range (1, len(answer)-1):\n","        if context[i+j-1] != answer[j]: # if the next tokens in the context are not in the answer\n","          break; # stop\n","      \n","      if j == len(answer)-2: # reach the end of the answer, in other words, the 'for-loop' of j reaches the end:\n","        # we have found the answer start and end indices in the context:\n","        start_token = i\n","        end_token= i+j-1\n","        return start_token, end_token\n","      # else: we move on to the next value in the context to keep searching for the answer start and end indices in the context.\n","  return 0, 0 # can not find the answer in the context"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tX9IvUrTBWCU","executionInfo":{"status":"ok","timestamp":1620370879770,"user_tz":-120,"elapsed":963,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"490a87fe-2eae-44b4-e337-7c221adfaa30"},"source":["# TEST:\n","# tokenize a 'context'\n","tokenized_context= tokenizer(context[1])\n","\n","# extract only the input_ids from the tokenized result:\n","input_ids_tokenized_context = tokenized_context['input_ids'] \n","\n","# tokenize an 'answer'\n","tokenized_answer= tokenizer(answers[1])\n","\n","# extract only the input_ids from the tokenized result:\n","input_ids_tokenized_answer = tokenized_answer['input_ids']\n","\n","start, end = extract_answer_start_end_tokens(input_ids_tokenized_answer, input_ids_tokenized_context)\n","print(start)\n","print(end)\n","test_list=[]\n","for i in range (start, end+1):\n","  test_list.append(input_ids_tokenized_context[i])\n","\n","tokens= tokenizer.convert_ids_to_tokens(test_list)\n","\n","for token, id in zip(tokens, test_list):\n","  if id == tokenizer.sep_token_id:\n","    print(\" \")\n","  print('{:12} {:>6}'.format(token,id))\n","  if id == tokenizer.sep_token_id:\n","    print(\" \")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["55\n","57\n","singing        4823\n","and            1998\n","dancing        5613\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OFIWI6W-Bo2M"},"source":["**EXTRACTING START AND END TOKENS OF AN 'ANSWER' IN EVERY 'CONTEXT' IN THE DATASET**"]},{"cell_type":"code","metadata":{"id":"ts7D9IS1Byz6","executionInfo":{"status":"ok","timestamp":1620570905462,"user_tz":-120,"elapsed":545,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["start_labels = []\n","end_labels = []"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"15gysNjEB0Gq","executionInfo":{"status":"ok","timestamp":1620539499194,"user_tz":-120,"elapsed":570,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["assert len(context) == len(answers) # must be TRUE"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"3WtF2MU9B2Np","executionInfo":{"status":"ok","timestamp":1620570908297,"user_tz":-120,"elapsed":542,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["import random\n","\n","def fit_max_length (input, max_len):\n","\n","  # find the range of the 'context' tokens:\n","  for i in range (0, len(input)):\n","\n","    if input[i] == 102: # encouter the first [SEP], which is the end of 'context\n","      range_context= i\n","      break\n","  \n","  # randomly drop some tokens in the 'context' to make the input len = 512:\n","\n","  len_difference= len(input) - max_len\n","  \n","  # generate a list of random indices from 0 to range_context (not include (0 and value of range_context) ):\n","  # reference:\n","  \n","  for i in range (0, len_difference):\n","    d_index=random.randint( 1, range_context-1)\n","    del input[d_index]\n","\n","    # the range_conext should be decreased by 1 due to the deleted tokens:\n","    range_context= range_context-1\n","  \n","  return input"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"bmLeqDPAB2f1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620570917592,"user_tz":-120,"elapsed":6365,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"c4669ef2-686c-4924-c98b-6a3a32946365"},"source":["for i in range (0, len(context)):\n","  # tokenize a 'context'\n","  tokenized_context= tokenizer(context[i])\n","\n","  # extract only the input_ids from the tokenized result:\n","  input_ids_tokenized_context = tokenized_context['input_ids'] \n","\n","  #resize the 'context' ids to maximum length of 512:\n","  input_ids_tokenized_context =fit_max_length(input_ids_tokenized_context,512) \n","\n","  # tokenize an 'answer'\n","  tokenized_answer= tokenizer(answers[i])\n","\n","  # extract only the input_ids from the tokenized result:\n","  input_ids_tokenized_answer = tokenized_answer['input_ids']\n","\n","  start, end = extract_answer_start_end_tokens(input_ids_tokenized_answer, input_ids_tokenized_context)\n","\n","  start_labels.append(start)\n","  end_labels.append(end)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (720 > 512). Running this sequence through the model will result in indexing errors\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"37lG9XREB9lx","executionInfo":{"status":"ok","timestamp":1620570932791,"user_tz":-120,"elapsed":527,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["# TEST:\n","assert len(start_labels) == len(end_labels)\n","assert len(start_labels) == len(context)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UqW4qZfPB-Ot","executionInfo":{"status":"ok","timestamp":1620570935585,"user_tz":-120,"elapsed":595,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"d0c92e54-a746-4de3-fc92-caa359e67e01"},"source":["# TEST:\n","# TEST:\n","# tokenize a 'context'\n","index= 4 # choose an examplary 'context', by choose a random value for the index in range (0, len(context))\n","\n","tokenized_context= tokenizer(context[index])\n","\n","# extract only the input_ids from the tokenized result:\n","input_ids_tokenized_context = tokenized_context['input_ids'] \n","\n","# tokenize an 'answer'\n","tokenized_answer= tokenizer(answers[index])\n","\n","# extract only the input_ids from the tokenized result:\n","input_ids_tokenized_answer = tokenized_answer['input_ids']\n","\n","tst_start = start_labels[index]\n","tst_end = end_labels[index]\n","print(f'{tst_start}\\t{tst_end}')\n","\n","test_list=[]\n","for i in range (tst_start, tst_end+1):\n","  test_list.append(input_ids_tokenized_context[i])\n","\n","tokens= tokenizer.convert_ids_to_tokens(test_list)\n","\n","for token, id in zip(tokens, test_list):\n","  if id == tokenizer.sep_token_id:\n","    print(\" \")\n","  print('{:12} {:>6}'.format(token,id))\n","  if id == tokenizer.sep_token_id:\n","    print(\" \")"],"execution_count":16,"outputs":[{"output_type":"stream","text":["69\t70\n","late           2397\n","1990s          4134\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5UnCR8MUChKn"},"source":["#TRANSFORMING the dataset into appropriate format input of Bert model"]},{"cell_type":"markdown","metadata":{"id":"_B_yWIe4EcVb"},"source":["##STEP 1: Tokenize the dataset and add special tokens [CLS], [SEP]. Then convert the tokenized the dataset into appropriate ids which are the indices of the lookup vocab table of the Bert model [Because the BertTokenizer is used for this task]"]},{"cell_type":"code","metadata":{"id":"ykJxrsx1Co88","executionInfo":{"status":"ok","timestamp":1620539522120,"user_tz":-120,"elapsed":545,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["# the maximum length of input sequence for bert-base-uncase is 512\n","\n","# check the length of the input sequence:\n","# because input sequence = a 'context' + a'question' \n","# the tokenized input is [CLS] + a 'context' + [SEP] + a'question'  +[SEP]\n","# => length of the tokenized input <= 512\n","\n","# => Check the len(tokenized input), if it > 512, drop some tokens in the 'context' to make len = 512.\n","\n","import random\n","\n","def fit_max_length (input, max_len):\n","\n","  # find the range of the 'context' tokens:\n","  for i in range (0, len(input)):\n","\n","    if input[i] == 102: # encouter the first [SEP], which is the end of 'context\n","      range_context= i\n","      break\n","  \n","  # randomly drop some tokens in the 'context' to make the input len = 512:\n","\n","  len_difference= len(input) - max_len\n","  \n","  # generate a list of random indices from 0 to range_context (not include (0 and value of range_context) ):\n","  # reference:\n","  \n","  for i in range (0, len_difference):\n","    d_index=random.randint( 1, range_context-1)\n","    del input[d_index]\n","\n","    # the range_conext should be decreased by 1 due to the deleted tokens:\n","    range_context= range_context-1\n","  \n","  return input"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"negOrGKsC5bq","executionInfo":{"status":"ok","timestamp":1620370906233,"user_tz":-120,"elapsed":955,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"22c5a6b4-9d25-4501-c8d9-7ebfac6051e4"},"source":["# Test: fit_max_length()\n","tokenized_sentences = tokenizer(context[10],question[10], add_special_tokens= True)\n","# BECAUSE input for the Bert model will be [CLS] + 'context' + [SEP] + 'question' + [SEP] (requirement_1),\n","# => tokenizer(context[10],question[10], add_special_tokens= True) takes care of the requirement_1.\n","# context[c_index], question[1_index] : c_index must be the same as q_question ('conext' must correspond to its own 'question')\n","# these indices can be a integer number in range (0, len (context))\n","\n","ids= tokenized_sentences['input_ids']\n","\n","\n","max_len=200 # test with a random length\n","\n","new_input_ids= fit_max_length(ids,max_len)\n","\n","#len(new_input_ids), new_input_ids\n","\n","tokens= tokenizer.convert_ids_to_tokens(new_input_ids)\n","\n","for token, id in zip(tokens, new_input_ids):\n","  if id == tokenizer.sep_token_id:\n","    print(\" \")\n","  print('{:12} {:>6}'.format(token,id))\n","  if id == tokenizer.sep_token_id:\n","    print(\" \")\n","\n","len(new_input_ids)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[CLS]           101\n","beyonce       20773\n","gi            21025\n","##selle       19358\n","knowles       22815\n","-              1011\n","carter         5708\n","(              1006\n","/              1013\n","bi            12170\n","##ː           23432\n","##ˈ           29715\n","##j            3501\n","##ɒ           29678\n","##nse         12325\n","##ɪ           29685\n","/              1013\n","bee           10506\n","-              1011\n","yo            10930\n","##n            2078\n","-              1011\n","say            2360\n",")              1007\n","(              1006\n","born           2141\n","september      2244\n","4              1018\n",",              1010\n","1981           3261\n",")              1007\n","is             2003\n","an             2019\n","american       2137\n","singer         3220\n",",              1010\n","songwriter     6009\n",",              1010\n","record         2501\n","producer       3135\n","and            1998\n","actress        3883\n",".              1012\n","born           2141\n","and            1998\n","raised         2992\n","in             1999\n","houston        5395\n",",              1010\n","texas          3146\n",",              1010\n","she            2016\n","performed      2864\n","in             1999\n","various        2536\n","singing        4823\n","and            1998\n","dancing        5613\n","competitions   6479\n","as             2004\n","a              1037\n","child          2775\n",",              1010\n","and            1998\n","rose           3123\n","to             2000\n","fame           4476\n","in             1999\n","the            1996\n","late           2397\n","1990s          4134\n","as             2004\n","lead           2599\n","singer         3220\n","of             1997\n","r              1054\n","&              1004\n","b              1038\n","girl           2611\n","-              1011\n","group          2177\n","destiny       10461\n","'              1005\n","s              1055\n","child          2775\n",".              1012\n","managed        3266\n","by             2011\n","her            2014\n","father         2269\n",",              1010\n","mathew        25436\n","knowles       22815\n",",              1010\n","the            1996\n","group          2177\n","became         2150\n","one            2028\n","of             1997\n","the            1996\n","world          2088\n","'              1005\n","s              1055\n","best           2190\n","-              1011\n","selling        4855\n","girl           2611\n","groups         2967\n","of             1997\n","all            2035\n","time           2051\n",".              1012\n","their          2037\n","hiatus        14221\n","saw            2387\n","the            1996\n","release        2713\n","of             1997\n","beyonce       20773\n","'              1005\n","s              1055\n","debut          2834\n","album          2201\n",",              1010\n","dangerously   20754\n","in             1999\n","love           2293\n","(              1006\n","2003           2494\n",")              1007\n",",              1010\n","which          2029\n","established    2511\n","her            2014\n","as             2004\n","a              1037\n","solo           3948\n","artist         3063\n","worldwide      4969\n",",              1010\n","earned         3687\n","five           2274\n","grammy         8922\n","awards         2982\n","and            1998\n","featured       2956\n","the            1996\n","billboard      4908\n","hot            2980\n","100            2531\n","number         2193\n","-              1011\n","one            2028\n","singles        3895\n","\"              1000\n","crazy          4689\n","in             1999\n","love           2293\n","\"              1000\n","and            1998\n","\"              1000\n","baby           3336\n","boy            2879\n","\"              1000\n",".              1012\n"," \n","[SEP]           102\n"," \n","what           2054\n","was            2001\n","the            1996\n","first          2034\n","album          2201\n","beyonce       20773\n","released       2207\n","as             2004\n","a              1037\n","solo           3948\n","artist         3063\n"," \n","[SEP]           102\n"," \n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["178"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"eeX7c1udDyZy","executionInfo":{"status":"ok","timestamp":1620570954497,"user_tz":-120,"elapsed":5780,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["# Each entry at ith index of the input_ids is 'ids' like in the 'TEST' cell right above.\n","# Maximum sequence length for this model (512)\n","# => Each entry of the input_ids must be 512.\n","\n","input_ids=[]\n","max_len= 512\n","for cntx, quest in zip(context, question):\n","\n","  tokenized_sentences = tokenizer(cntx,quest, add_special_tokens= True)\n","  ids= tokenized_sentences['input_ids']\n","\n","  if len(ids) > 512: # if the sequence length > 512\n","    ids= fit_max_length(ids, max_len) # decrease it to 512\n","\n","  input_ids.append(ids)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I9ViYSDIELPD","executionInfo":{"status":"ok","timestamp":1620376109609,"user_tz":-120,"elapsed":833,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"1bb9fa45-55f3-42f1-f5d3-ffe6aa5a65d9"},"source":["# TEST:\n","test_1=input_ids[4]\n","\n","tokens= tokenizer.convert_ids_to_tokens(test_1)\n","\n","for token, id in zip(tokens, test_1):\n","  if id == tokenizer.sep_token_id:\n","    print(\" \")\n","  print('{:12} {:>6}'.format(token,id))\n","  if id == tokenizer.sep_token_id:\n","    print(\" \")\n","print(len(test_1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[CLS]           101\n","battles        7465\n","which          2029\n","are            2024\n","particularly   3391\n","notable        3862\n","to             2000\n","the            1996\n","canadian       3010\n","military       2510\n","include        2421\n","the            1996\n","battle         2645\n","of             1997\n","vi             6819\n","##my           8029\n","ridge          5526\n",",              1010\n","the            1996\n","die            3280\n","##ppe         21512\n","raid           8118\n",",              1010\n","the            1996\n","battle         2645\n","of             1997\n","orton         25161\n","##a            2050\n",",              1010\n","the            1996\n","battle         2645\n","of             1997\n","pass           3413\n","##chen         8661\n","##dae          6858\n","##le           2571\n",",              1010\n","the            1996\n","normandy      13298\n","landings      16805\n",",              1010\n","the            1996\n","battle         2645\n","for            2005\n","ca             6187\n","##en           2368\n",",              1010\n","the            1996\n","battle         2645\n","of             1997\n","the            1996\n","sc             8040\n","##held        24850\n","##t            2102\n",",              1010\n","the            1996\n","battle         2645\n","of             1997\n","britain        3725\n",",              1010\n","the            1996\n","battle         2645\n","of             1997\n","the            1996\n","atlantic       4448\n",",              1010\n","the            1996\n","strategic      6143\n","bombing        8647\n","of             1997\n","german         2446\n","cities         3655\n",",              1010\n","and            1998\n","more           2062\n","recently       3728\n","the            1996\n","battle         2645\n","of             1997\n","med           19960\n","##ak           4817\n","pocket         4979\n",",              1010\n","in             1999\n","croatia        8097\n",".              1012\n"," \n","[SEP]           102\n"," \n","what           2054\n","country        2406\n","was            2001\n","the            1996\n","latest         6745\n","french         2413\n","military       2510\n","effort         3947\n"," \n","[SEP]           102\n"," \n","96\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"omi0sUy4EVW-","executionInfo":{"status":"ok","timestamp":1620570960080,"user_tz":-120,"elapsed":578,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["# CHECK:\n","input_ids[10]\n","for i in range (0,len(input_ids)):\n","  if len(input_ids[i]) > 512:\n","    print(\"NOT OK\")\n","    break"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i49UiYw1FAgO"},"source":["##PADDING: to make all the input sequences (in this case, an entry of the 'input_ids' list) the same length [because Bert model requires such a thing]"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SPD1bbAdFRSt","executionInfo":{"status":"ok","timestamp":1620570963452,"user_tz":-120,"elapsed":580,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"573a5917-a046-43b3-f8b2-6d45df132f75"},"source":["# CHECK The maximum length of each sequence in input_ids:\n","max_len_input_ids=0\n","for i in range (0, len(input_ids)):\n","  if len(input_ids[i])> max_len_input_ids:\n","    max_len_input_ids= len(input_ids[i])\n","\n","max_len_input_ids "],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["512"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a0WC05ZdFUQ0","executionInfo":{"status":"ok","timestamp":1620570966689,"user_tz":-120,"elapsed":861,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"9ced20ac-245e-4574-a470-8632a077cef0"},"source":["# PADDING: FOR the input_ids\n","# [PAD] in Bert has value of 0\n","\n","from keras.preprocessing.sequence import pad_sequences\n","\n","# set the Max_len:\n","Max_len= 512 # set as the same value with the 'max_len_input_ids'\n","\n","# because '[PAD]' in Bert vocab look-up take has id (or index) =0 \n","# => we can padd 0 values at the end of each entries of 'input_ids'\n","pad_input_ids= pad_sequences(input_ids, maxlen=Max_len, dtype='long', value=0, truncating='post', padding='post')\n","\n","# check whether the padding and truncating after padding work as we expect:\n","assert len(pad_input_ids[10]) == Max_len\n","print(len(input_ids[0])) # original length of the input_ids[0]\n"],"execution_count":20,"outputs":[{"output_type":"stream","text":["173\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bZPD0Yg0FthU","executionInfo":{"status":"ok","timestamp":1620453219493,"user_tz":-120,"elapsed":631,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"d067ed9d-78ce-4748-a7e6-ba8aa4a6cc87"},"source":["# TEST:\n","test_1=pad_input_ids[5]\n","\n","tokens= tokenizer.convert_ids_to_tokens(test_1)\n","\n","for token, id in zip(tokens, test_1):\n","  if id == tokenizer.sep_token_id:\n","    print(\" \")\n","  print('{:12} {:>6}'.format(token,id))\n","  if id == tokenizer.sep_token_id:\n","    print(\" \")\n","print(len(test_1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[CLS]           101\n","beyonce       20773\n","gi            21025\n","##selle       19358\n","knowles       22815\n","-              1011\n","carter         5708\n","(              1006\n","/              1013\n","bi            12170\n","##ː           23432\n","##ˈ           29715\n","##j            3501\n","##ɒ           29678\n","##nse         12325\n","##ɪ           29685\n","/              1013\n","bee           10506\n","-              1011\n","yo            10930\n","##n            2078\n","-              1011\n","say            2360\n",")              1007\n","(              1006\n","born           2141\n","september      2244\n","4              1018\n",",              1010\n","1981           3261\n",")              1007\n","is             2003\n","an             2019\n","american       2137\n","singer         3220\n",",              1010\n","songwriter     6009\n",",              1010\n","record         2501\n","producer       3135\n","and            1998\n","actress        3883\n",".              1012\n","born           2141\n","and            1998\n","raised         2992\n","in             1999\n","houston        5395\n",",              1010\n","texas          3146\n",",              1010\n","she            2016\n","performed      2864\n","in             1999\n","various        2536\n","singing        4823\n","and            1998\n","dancing        5613\n","competitions   6479\n","as             2004\n","a              1037\n","child          2775\n",",              1010\n","and            1998\n","rose           3123\n","to             2000\n","fame           4476\n","in             1999\n","the            1996\n","late           2397\n","1990s          4134\n","as             2004\n","lead           2599\n","singer         3220\n","of             1997\n","r              1054\n","&              1004\n","b              1038\n","girl           2611\n","-              1011\n","group          2177\n","destiny       10461\n","'              1005\n","s              1055\n","child          2775\n",".              1012\n","managed        3266\n","by             2011\n","her            2014\n","father         2269\n",",              1010\n","mathew        25436\n","knowles       22815\n",",              1010\n","the            1996\n","group          2177\n","became         2150\n","one            2028\n","of             1997\n","the            1996\n","world          2088\n","'              1005\n","s              1055\n","best           2190\n","-              1011\n","selling        4855\n","girl           2611\n","groups         2967\n","of             1997\n","all            2035\n","time           2051\n",".              1012\n","their          2037\n","hiatus        14221\n","saw            2387\n","the            1996\n","release        2713\n","of             1997\n","beyonce       20773\n","'              1005\n","s              1055\n","debut          2834\n","album          2201\n",",              1010\n","dangerously   20754\n","in             1999\n","love           2293\n","(              1006\n","2003           2494\n",")              1007\n",",              1010\n","which          2029\n","established    2511\n","her            2014\n","as             2004\n","a              1037\n","solo           3948\n","artist         3063\n","worldwide      4969\n",",              1010\n","earned         3687\n","five           2274\n","grammy         8922\n","awards         2982\n","and            1998\n","featured       2956\n","the            1996\n","billboard      4908\n","hot            2980\n","100            2531\n","number         2193\n","-              1011\n","one            2028\n","singles        3895\n","\"              1000\n","crazy          4689\n","in             1999\n","love           2293\n","\"              1000\n","and            1998\n","\"              1000\n","baby           3336\n","boy            2879\n","\"              1000\n",".              1012\n"," \n","[SEP]           102\n"," \n","in             1999\n","what           2054\n","r              1054\n","&              1004\n","b              1038\n","group          2177\n","was            2001\n","she            2016\n","the            1996\n","lead           2599\n","singer         3220\n"," \n","[SEP]           102\n"," \n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","512\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Sybc2GZRF1fE"},"source":["#BESIDES, the input sequences, the SEGMENT MASK is also required as input for the Bert model"]},{"cell_type":"markdown","metadata":{"id":"CgVS2jp2F8oE"},"source":["as a sequence input in the 'input_ids' has format:\n","[CLS] + 'context' + [SEP] + 'question' + [SEP] + [PAD]s.\n","\n","Then we would assign a sequence of 1s (1, 1, 1, ...) for the first part [CLS] + 'context' + [SEP]; and assign a sequence of 0s (0, 0, 0, ...) for the second part 'question' + [SEP] + [PAD]s."]},{"cell_type":"code","metadata":{"id":"HALuVq_dGmU8","executionInfo":{"status":"ok","timestamp":1620570972073,"user_tz":-120,"elapsed":593,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["segment_masks=[] # consider [PAD]s belonging to the second sequence\n","for i in range (0, len(pad_input_ids)):\n","\n","  convert_to_list= pad_input_ids[i].tolist()\n","  sep_index= convert_to_list.index(tokenizer.sep_token_id)\n","\n","  # number of the 'context' (='answer') tokens includes the [SEP] also\n","  num_seg_a= sep_index+1\n","\n","  # the remainder is the 'question':\n","  num_seg_b= len(convert_to_list) - num_seg_a\n","\n","  # construct list of 0s and 1s:\n","  segment_ids= [1]*num_seg_a + [0]*num_seg_b # a segment mask for [CLS]+ a 'context' +[SEP] + 'text' + [SEP]\n","\n","  # add the segment_ids to the list of segment masks:\n","  segment_masks.append(segment_ids)"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"bx0DdaaHG69n"},"source":["# TEST:\n","number_of_ones=0\n","position= 99 # take a segment mask in the list segment_maks for testing\n","for i in range (0, len(segment_masks[position])):\n","  if segment_masks[position][i] == 1:\n","    number_of_ones= number_of_ones+1\n","\n","number_of_zeros = len(segment_masks[position]) - number_of_ones\n","\n","# the real number of ones in the segment masks:\n","convert_to_list= pad_input_ids[position].tolist()\n","sep_index= convert_to_list.index(tokenizer.sep_token_id)\n","real_number_of_ones= sep_index+1\n","real_number_of_zeros= len(convert_to_list) - real_number_of_ones\n","#\n","assert number_of_ones== real_number_of_ones\n","assert number_of_zeros== real_number_of_zeros"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mrbFRGw3InJf","executionInfo":{"status":"ok","timestamp":1619807435548,"user_tz":-120,"elapsed":725,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"2c25ae3c-64e0-4dce-e2eb-e14807dfa11e"},"source":["# TEST:\n","\n","for i in range (0, 15): # all segment_masks must have the same length\n","  print(len(segment_masks[i]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["512\n","512\n","512\n","512\n","512\n","512\n","512\n","512\n","512\n","512\n","512\n","512\n","512\n","512\n","512\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2loM3Md0IzF-"},"source":["#ATTTENTION MASK is also required together with the segment mask"]},{"cell_type":"markdown","metadata":{"id":"kFZ0qW_nJAU6"},"source":["as a sequence input in the 'input_ids' has format:\n","[CLS] + 'context' + [SEP] + 'question' + [SEP] + [PAD]s.\n","\n","Then we would assign a sequence of 1s (1, 1, 1, ...) for the first part [CLS] + 'context' + [SEP] + 'question' + [SEP]; and assign a sequence of 0s (0, 0, 0, ...) for the second part [PAD]s."]},{"cell_type":"code","metadata":{"id":"D-6gIcfPIxDR","executionInfo":{"status":"ok","timestamp":1620570979094,"user_tz":-120,"elapsed":3992,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["# Create attention mask:\n","attention_masks= []\n","\n","for pad_sequence in pad_input_ids:\n","\n","  # because [PAD] has id = 0 => we could use this condition to apply the attension mask:\n","  attention_mask=[int(token_id >0) for token_id in pad_sequence]\n","\n","  # aggregate each mask of each padded sequence into a list attention_masks\n","  # with this method, we could preserve the corresponding order between a padded sequence and its mask:\n","  attention_masks.append(attention_mask)\n"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q2PlyuTGJ9M3","executionInfo":{"status":"ok","timestamp":1619807447359,"user_tz":-120,"elapsed":723,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"ef6b97d6-8151-49ad-cad3-d692999df2b6"},"source":["# CHECK attention_masks:\n","# check the mask for first sentence:\n","\n","# the first encoded sentence:\n","#print(pad_input_ids[0])\n","\n","# the mask of the first encoded sentence:\n","#print(attention_masks[0]) # there should be 19 values of '1' at the beginning.\n","\n","# count '1' values in the first attention mask, the result should be 174\n","c=0\n","for i in range (0, len(attention_masks[2])):\n","  if attention_masks[2][i]==1:\n","    c=c+1\n","print(c)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["218\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ref1hjyaZDGn","executionInfo":{"status":"ok","timestamp":1620570983772,"user_tz":-120,"elapsed":569,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["assert len(attention_masks[0]) == 512"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5OdG8mx5Ljch"},"source":["#SPLIT the dataset, the segment masks, the attention maks, the start labels, the end labels into the train set and evaluation set"]},{"cell_type":"code","metadata":{"id":"TY_e8T1kL-FT","executionInfo":{"status":"ok","timestamp":1620570987613,"user_tz":-120,"elapsed":2017,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["from sklearn.model_selection import train_test_split\n","\n","# split for the conformed input dataset of BERT (input_ids) and the labels list\n","train_inputs, evl_inputs, train_start_labels, evl_start_labels= train_test_split(pad_input_ids, start_labels, random_state=2018, test_size=0.1)\n","\n","# do the same for segment masks of conformed input dataset:\n","train_segment_masks, evl_segment_masks, train_end_labels, evl_end_labels= train_test_split(segment_masks, end_labels, random_state=2018, test_size=0.1)\n","# _,_ is used because masks need no labels.\n","\n","# do the same for segment masks of conformed input dataset:\n","train_attention_masks, evl_attention_masks,_,_= train_test_split(attention_masks, start_labels, random_state=2018, test_size=0.1)\n","# _,_ is used because masks need no labels."],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tkX8y4T4MCOg","executionInfo":{"status":"ok","timestamp":1620570991414,"user_tz":-120,"elapsed":535,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"48072d46-0d62-49b4-b767-83063f855ca3"},"source":["#train_inputs.shape, train_labels.shape\n","print(train_inputs.shape)"],"execution_count":25,"outputs":[{"output_type":"stream","text":["(9000, 512)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cf-KetB2Mkzv"},"source":["#CONVERT the train set and the evaluation set into 'torch.tensor' type becuase Bert model requires 'torch.tensor' type as its valid input type."]},{"cell_type":"code","metadata":{"id":"pdGY_8yYMjwE","executionInfo":{"status":"ok","timestamp":1620570995272,"user_tz":-120,"elapsed":1668,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["# for the input data:\n","train_inputs= torch.tensor([train_inputs])\n","evl_inputs= torch.tensor([evl_inputs])\n","\n","# for the start_labels:\n","train_start_labels= torch.tensor(train_start_labels)\n","evl_start_labels= torch.tensor(evl_start_labels)\n","\n","# for the end_labels:\n","train_end_labels= torch.tensor(train_end_labels)\n","evl_end_labels= torch.tensor(evl_end_labels)\n","\n","# for segment masks:\n","train_segment_masks= torch.tensor([train_segment_masks])\n","evl_segment_masks= torch.tensor([evl_segment_masks])\n","\n","# for attention masks:\n","train_attention_masks=torch.tensor([train_attention_masks])\n","evl_attention_masks=torch.tensor([evl_attention_masks])"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"swJd6nxPNJOA","executionInfo":{"status":"ok","timestamp":1620570998390,"user_tz":-120,"elapsed":599,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"f228466c-4816-4055-d8b4-37dc0e23aafe"},"source":["# check the shape:\n","train_inputs.shape, train_start_labels.shape, train_end_labels.shape, train_segment_masks.shape, train_attention_masks.shape"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([1, 9000, 512]),\n"," torch.Size([9000]),\n"," torch.Size([9000]),\n"," torch.Size([1, 9000, 512]),\n"," torch.Size([1, 9000, 512]))"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"wCqi6FVaNOJg"},"source":["#Generate the train set and evaluation set in batches:"]},{"cell_type":"markdown","metadata":{"id":"WKUpzSs7NWqB"},"source":["**FUNCTION FOR GENERATING BATCHES with batch size chosen by user**"]},{"cell_type":"code","metadata":{"id":"Y8gWBSk0NV1m","executionInfo":{"status":"ok","timestamp":1620571001413,"user_tz":-120,"elapsed":558,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["# LEARN FROM PREVIOUS LECTURES AND LABS in the class:\n","class BatchedIterator:\n","    def __init__(self, *tensors, batch_size,**kwarg):\n","        # all tensors must have the same first dimension\n","        assert len(set(len(tensor) for tensor in tensors)) == 1\n","        #print(type(tensors))\n","        #print(tensors[1])\n","        self.tensors = tensors\n","        self.batch_size = batch_size\n","\n","        for keyword, value in kwarg.items():\n","            if keyword == \"shuffle\":\n","                self.shuffle=value\n","    \n","    def iterate_once(self):\n","        num_data = len(self.tensors[0][0]) # the length of the data\n","\n","        if self.shuffle== False:\n","          for start in range(0, num_data, self.batch_size):\n","              end = start + self.batch_size\n","              yield tuple(tensor[0][start:end] for tensor in self.tensors) \n","              #must be tensor[0], to access to real data\n","              # cause tensor size [1,..]; 1: is unecessary dimension\n","              # => must exclude it by using tensor[0]  \n","        else:\n","          all_batches=[] # to gather all the batches formed form the dataset\n","          for start in range(0, num_data, self.batch_size):\n","              end = start + self.batch_size\n","              all_batches.append(tuple(tensor[0][start:end] for tensor in self.tensors))\n","          \n","          # shuffle the batches: \n","          # reference: https://note.nkmk.me/en/python-random-shuffle/#:~:text=To%20randomly%20shuffle%20elements%20of,Python%2C%20use%20the%20random%20module.&text=random%20provides%20shuffle()%20that,used%20for%20strings%20and%20tuples.\n","          shuf_batches = random.sample(all_batches, len(all_batches))\n","\n","          # yield a batch in the list at a time:\n","          for i in range (0,len(shuf_batches)):\n","            yield shuf_batches[i]"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K-EKsJ47N_4r","executionInfo":{"status":"ok","timestamp":1619807872622,"user_tz":-120,"elapsed":655,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"2eca4af8-5024-4158-dfe9-2f44df6e1aca"},"source":["# extract for test sample:\n","test_train_inputs= train_inputs[:,:10,:]\n","\n","test_train_start_labels= train_start_labels[:10]\n","test_train_end_labels= train_end_labels[:10]\n","\n","test_train_segment_masks = train_segment_masks[:,:10,:]\n","test_train_attention_masks = train_attention_masks[:,:10,:]\n","\n","test_train_inputs.shape, test_train_start_labels.shape, test_train_end_labels.shape, test_train_segment_masks.shape, test_train_attention_masks.shape,"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([1, 10, 512]),\n"," torch.Size([10]),\n"," torch.Size([10]),\n"," torch.Size([1, 10, 512]),\n"," torch.Size([1, 10, 512]))"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bm4YTKKtOg60","executionInfo":{"status":"ok","timestamp":1619807873957,"user_tz":-120,"elapsed":529,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"549c58dc-4bd8-4930-f18a-49e5f8620461"},"source":["# the torch.size must be torch.size([1,...])\n","# but the test_train_labels has the torch.size([10])\n","# must make it into torch.size([1,10])\n","good_test_train_start_labels=torch.unsqueeze(test_train_start_labels, 0)\n","good_test_train_end_labels=torch.unsqueeze(test_train_end_labels, 0)\n","\n","#test:\n","good_test_train_start_labels.shape, good_test_train_end_labels.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([1, 10]), torch.Size([1, 10]))"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"40z8-fA1Oak8","executionInfo":{"status":"ok","timestamp":1619807877610,"user_tz":-120,"elapsed":701,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"01deb324-c6d8-4851-9cf8-b7f9eab395f0"},"source":["# Test out the BatchedIterator:\n","\n","batch_size = 5\n","train_iter = BatchedIterator(test_train_inputs, good_test_train_start_labels,good_test_train_end_labels, test_train_segment_masks,test_train_attention_masks, batch_size=batch_size,shuffle=True)\n","\n","for train_batch, start_batch, end_batch, segment_mask_batch, attention_batch in train_iter.iterate_once():\n","  print(f'train_batch.type= {train_batch.shape}\\tstart_batch.type={start_batch.shape}\\tend_batch={end_batch.shape}\\tsegment_mask_batch={segment_mask_batch.shape}\\tattention_mask_batch={attention_batch.shape}')\n","  print(train_batch)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["train_batch.type= torch.Size([5, 512])\tstart_batch.type=torch.Size([5])\tend_batch=torch.Size([5])\tsegment_mask_batch=torch.Size([5, 512])\tattention_mask_batch=torch.Size([5, 512])\n","tensor([[  101,  1999,  2249,  ...,     0,     0,     0],\n","        [  101, 20773, 21025,  ...,     0,     0,     0],\n","        [  101,  2087,  1997,  ...,     0,     0,     0],\n","        [  101,  1999,  2238,  ...,     0,     0,     0],\n","        [  101,  1037,  2193,  ...,     0,     0,     0]])\n","train_batch.type= torch.Size([5, 512])\tstart_batch.type=torch.Size([5])\tend_batch=torch.Size([5])\tsegment_mask_batch=torch.Size([5, 512])\tattention_mask_batch=torch.Size([5, 512])\n","tensor([[  101,  1996,  4145,  ...,     0,     0,     0],\n","        [  101,  1996,  2048,  ...,     0,     0,     0],\n","        [  101, 20773,  1005,  ...,     0,     0,     0],\n","        [  101,  2006,  2410,  ...,     0,     0,     0],\n","        [  101,  5148,  1024,  ...,     0,     0,     0]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"19vUl9BMOtxW"},"source":["#DEFINE BERT MODEL FOR TRAINING"]},{"cell_type":"markdown","metadata":{"id":"8dvypaBoOyDo"},"source":["**Bert model is too large, i would use DistlledBert model which is 40 percent smaller than Bert model but still preserves 97 percent performace of the Bert model**"]},{"cell_type":"code","metadata":{"id":"Hn3nIldXPF4A","executionInfo":{"status":"ok","timestamp":1620571006749,"user_tz":-120,"elapsed":532,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["# https://huggingface.co/transformers/model_doc/bert.html#bertforquestionanswering\n","from transformers import DistilBertForQuestionAnswering\n","\n","# reference for config a pretrained model : https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n","from transformers import DistilBertConfig"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"f-uej9ESNyqw","executionInfo":{"status":"ok","timestamp":1620571009450,"user_tz":-120,"elapsed":545,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["class ForQuestionAnsweringClassifier(nn.Module):\n","    def __init__(self,model_out_sequence_length, output_size, freeze_bert = True):\n","        super(ForQuestionAnsweringClassifier, self).__init__()\n","        # Configure DistilBERT's initialization\n","        self.bert_config = DistilBertConfig(n_layers=1, n_heads=2,qa_dropout=0.2)\n","                          \n","        self.Distil_Bert_model= DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased',config=self.bert_config)\n","\n","        # Make DistilBERT layers untrainable\n","        for p in self.Distil_Bert_model.parameters():\n","          p.requires_grad = False\n","          p.trainable = False\n","                \n","        #Classification layer\n","        self.start_cls_layer = nn.Linear(model_out_sequence_length, output_size)\n","        self.end_cls_layer = nn.Linear(model_out_sequence_length, output_size)\n","\n","    def forward(self, data, attn_masks, start_label, end_label):\n","        bertOut= self.Distil_Bert_model(data, # the tokens representing our input\n","                attention_mask=attn_masks, start_positions= start_label, end_positions= end_label ) # the segment ids to differentiate the question and the answer\n","        start_labels= self.start_cls_layer(bertOut.start_logits)\n","        end_labels=self.start_cls_layer(bertOut.end_logits)\n","        return start_labels, end_labels"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"4DmKLTg-SNYJ","colab":{"base_uri":"https://localhost:8080/","height":173,"referenced_widgets":["b7fa0420be3440b5b3cdbf69f85a4879","097ab935f58f44d6b71f0751ad138662","a52174f841c74fe7b155d0ce65b4c90e","ad891c93cb0f4c199d32bdd294012e69","870f66b2dd814ffe9acd59bf94ee069a","1d2ee3330dc34f9a9ba302bf336a7ec5","3fa39c6531804843966ba51beb5eb9c5","98ac8bfad7544378910ecf6d5f959cbc"]},"executionInfo":{"status":"ok","timestamp":1620157660731,"user_tz":-120,"elapsed":7101,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"6657cc2c-acd3-4484-d684-e1b6e5ab81d4"},"source":["model = ForQuestionAnsweringClassifier(512,512) #model_out_sequence_length = output_size"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b7fa0420be3440b5b3cdbf69f85a4879","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=267967963.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"qddgpBfwi-Qy"},"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3UsLwAn0S8Dp","executionInfo":{"status":"ok","timestamp":1620157676460,"user_tz":-120,"elapsed":11012,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"517b2d78-fd1c-442d-bcff-67b6214a0212"},"source":["# Tell Pytorch to run this model on the GPU:\n","#for p in model.parameters():\n","#      p.requires_grad = False\n","model.to(device)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ForQuestionAnsweringClassifier(\n","  (Distil_Bert_model): DistilBertForQuestionAnswering(\n","    (distilbert): DistilBertModel(\n","      (embeddings): Embeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (transformer): Transformer(\n","        (layer): ModuleList(\n","          (0): TransformerBlock(\n","            (attention): MultiHeadSelfAttention(\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (ffn): FFN(\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          )\n","        )\n","      )\n","    )\n","    (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n","    (dropout): Dropout(p=0.2, inplace=False)\n","  )\n","  (start_cls_layer): Linear(in_features=512, out_features=512, bias=True)\n","  (end_cls_layer): Linear(in_features=512, out_features=512, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"markdown","metadata":{"id":"OZKtKU2CPv1l"},"source":["#TRAINING AND EVALUATION:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oyq8pf0tPygT","executionInfo":{"status":"ok","timestamp":1620571015424,"user_tz":-120,"elapsed":549,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"28089c18-3ca5-48e8-edcf-3c59caedd3d9"},"source":["# Convert the data into tensor so that the model can run on the data\n","# extract for test sample:\n","train_inputs\n","evl_inputs\n","\n","train_inputs= train_inputs.to(device)\n","evl_inputs = evl_inputs.to(device)\n","\n","train_start_labels\n","train_end_labels\n","\n","train_start_labels= train_start_labels.to(device)\n","train_end_labels= train_end_labels.to(device) \n","\n","\n","evl_start_labels\n","evl_end_labels\n","\n","evl_start_labels= evl_start_labels.to(device)\n","evl_end_labels= evl_end_labels.to(device)\n","\n","train_segment_masks\n","evl_segment_masks\n","\n","\n","train_segment_masks= train_segment_masks.to(device) \n","evl_segment_masks= evl_segment_masks.to(device) \n","\n","\n","\n","train_attention_masks\n","evl_attention_masks\n","\n","\n","train_attention_masks = train_attention_masks.to(device) \n","evl_attention_masks= evl_attention_masks.to(device) \n","\n","train_inputs.shape, train_start_labels.shape, train_end_labels.shape, train_segment_masks.shape, train_attention_masks.shape"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([1, 9000, 512]),\n"," torch.Size([9000]),\n"," torch.Size([9000]),\n"," torch.Size([1, 9000, 512]),\n"," torch.Size([1, 9000, 512]))"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H1tbHvc8P5Z3","executionInfo":{"status":"ok","timestamp":1620571018602,"user_tz":-120,"elapsed":556,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"92d57fb1-21c1-429a-9fd6-b5e0d71cdad4"},"source":["# the torch.size must be torch.size([1,...])\n","# but the test_train_labels has the torch.size([10])\n","# must make it into torch.size([1,10])\n","good_train_start_labels=torch.unsqueeze(train_start_labels, 0)\n","good_train_end_labels=torch.unsqueeze(train_end_labels, 0)\n","\n","good_evl_start_labels=torch.unsqueeze(evl_start_labels, 0)\n","good_evl_end_labels=torch.unsqueeze(evl_end_labels, 0)\n","\n","#test:\n","good_train_start_labels.shape, good_train_end_labels.shape"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([1, 9000]), torch.Size([1, 9000]))"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"CwUtfXUHP7GI","executionInfo":{"status":"ok","timestamp":1620571021698,"user_tz":-120,"elapsed":529,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["batch_size = 90\n","train_iter = BatchedIterator(train_inputs, good_train_start_labels, good_train_end_labels, train_segment_masks, train_attention_masks, batch_size=batch_size, shuffle=True)"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"s9n2XMBJQB4r","executionInfo":{"status":"ok","timestamp":1620571028184,"user_tz":-120,"elapsed":562,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["criterion = nn.CrossEntropyLoss()\n","#criterion = criterion.cuda() # run this line only when the GPU is provided\n"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"P8_VyXDFj3gx"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XHZIuN2UDiU8","executionInfo":{"status":"ok","timestamp":1620571080874,"user_tz":-120,"elapsed":539,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["optimizer = optim.Adam(model.parameters(), lr=1e-5)"],"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q6iVr4YOQDv1","executionInfo":{"status":"ok","timestamp":1620571085045,"user_tz":-120,"elapsed":550,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["num_epochs = 4"],"execution_count":39,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6rUns3QxQEhH","outputId":"97f7396b-f3d9-46ce-a136-036dcec13c70"},"source":["for epoch in range(num_epochs):\n","    model.train()\n","    # Training on train data\n","  \n","    for train_batch, start_batch, end_batch, segment_mask_batch, attention_batch in train_iter.iterate_once():\n","        \n","        #train_batch = train_batch.to(device)\n","        #attention_batch = attention_batch.to(device)\n","        #start_batch = start_batch.to(device)\n","        #end_batch = end_batch.to(device)\n","        # run the model on the inputs:\n","        #print(f'train_batch.shape= {train_batch.shape}\\nattention_batch.shape= {attention_batch.shape}\\nstart_batch.shape= {start_batch.shape}\\nend_batch.shape= {end_batch.shape}')\n","        start_predicts, end_predicts = model(train_batch, # the tokens representing our input\n","                attention_batch, start_batch, end_batch ) # the segment ids to differentiate the question and the answer\n","        \n","\n","        # FOR TASK 1:\n","        \n","        #y_out = model(X_batch)\n","        #print(y_out)\n","        # FOR TASK 1:\n","        \n","        # To understand the ouput of the model:\n","        # y_out.shape = [batch_size, output_size] # output_size = number of labels\n","        #print(f'y_out.shape= {y_out.shape} \\ny_batch.shape= {y_batch.shape} ')\n","\n","        #print(f'y_out.shape= {y_out.shape} \\ny_batch.shape= {y_batch.shape} ')\n","        #train_loss= outputs.loss\n","        #print(f'start_predict.shape={start_predicts.shape}\\tstart_batch={start_batch.shape}')\n","        start_loss= criterion(start_predicts, start_batch)\n","        end_loss= criterion(end_predicts, end_batch)\n","        #print(\"loss:\")\n","        #print(loss)\n","        optimizer.zero_grad()\n","        \n","        start_loss.backward() # train the classification for start token\n","        end_loss.backward() # train the classification for end token\n","        optimizer.step()\n","        \n","    model.eval()  # or model.train(False)\n","    \n","    #Move the training data for evaluation\n","    #train_inputs[0] = train_inputs[0].to(device)\n","    #train_attention_masks[0] = train_attention_masks[0].to(device)\n","    #train_start_labels = train_start_labels.to(device)\n","    #train_end_labels = train_end_labels.to(device)\n","        \n","    # evaluation on train data:\n","    total_start_train_loss=0\n","    total_end_train_loss=0\n","\n","    total_train_start_acc = 0\n","    total_train_end_acc = 0\n","\n","    number_of_trains = 0\n","\n","    for train_batch, start_batch, end_batch, segment_mask_batch, attention_batch in train_iter.iterate_once():\n","      number_of_trains= number_of_trains +1\n","\n","      start_predicts, end_predicts = model(train_batch, # the tokens representing our input\n","                  attention_batch, start_batch , end_batch) # the segment ids to differentiate the question and the answer\n","\n","      start_labels= start_predicts.argmax(axis=1)\n","      end_labels=end_predicts.argmax(axis=1)\n","\n","      #train_loss= outputs.loss\n","      start_train_loss = criterion(start_predicts, start_batch).item()\n","      total_start_train_loss = total_start_train_loss + start_train_loss\n","\n","      end_train_loss = criterion(end_predicts, end_batch).item()\n","      total_end_train_loss= total_end_train_loss + end_train_loss\n","\n","      train_start_accuracy = (torch.eq(start_labels, start_batch).sum()).item()\n","      total_train_start_acc = total_train_start_acc + train_start_accuracy\n","\n","      train_end_accuracy = (torch.eq(end_labels, end_batch).sum()).item()\n","      total_train_end_acc = total_train_end_acc+ train_end_accuracy  \n","  \n","    real_train_start_acc = total_train_start_acc / float(len(train_start_labels))\n","    real_train_end_acc= total_train_end_acc / float(len(train_end_labels))\n","    #train_accuracy = (real_train_start_acc + real_train_end_acc) /2\n","    train_loss= ((total_start_train_loss/number_of_trains) + (total_end_train_loss/number_of_trains))/2\n","    \n","    # evaluation on eval data:\n","    #Move the training data for evaluation\n","    #evl_inputs[0] = evl_inputs[0].to(device)\n","    #evl_attention_masks[0] = evl_attention_masks[0].to(device)\n","    #evl_start_labels = evl_start_labels.to(device)\n","    #evl_end_labels = evl_end_labels.to(device)\n","    total_start_evl_loss=0\n","    total_end_evl_loss=0\n","\n","    total_evl_start_acc = 0\n","    total_evl_end_acc = 0\n","\n","    number_of_evls = 0\n","    evl_iter = BatchedIterator(evl_inputs, good_evl_start_labels, good_evl_end_labels, evl_segment_masks, evl_attention_masks, batch_size=batch_size, shuffle=True)\n","    for evl_batch, evl_start_batch, evl_end_batch, evl_segment_mask_batch, evl_attention_batch in evl_iter.iterate_once():\n","      number_of_evls= number_of_evls +1\n","\n","      evl_start_predicts, evl_end_predicts = model(evl_batch, # the tokens representing our input\n","                  evl_attention_batch, evl_start_batch , evl_end_batch) # the segment ids to differentiate the question and the answer\n","\n","      start_labels= evl_start_predicts.argmax(axis=1)\n","      end_labels=evl_end_predicts.argmax(axis=1)\n","\n","      start_evl_loss = criterion(evl_start_predicts, evl_start_batch).item()\n","      total_start_evl_loss = total_start_evl_loss + start_evl_loss\n","\n","      end_evl_loss = criterion(evl_end_predicts, evl_end_batch).item()\n","      total_end_evl_loss= total_end_evl_loss + end_evl_loss\n","    \n","\n","      evl_start_accuracy = (torch.eq(start_labels, evl_start_batch).sum()).item()\n","      total_evl_start_acc = total_evl_start_acc + evl_start_accuracy\n","\n","      evl_end_accuracy = (torch.eq(end_labels, evl_end_batch).sum()).item()\n","      total_evl_end_acc = total_evl_end_acc+ evl_end_accuracy  \n","  \n","    real_evl_start_acc = total_evl_start_acc / float(len(evl_start_labels))\n","    real_evl_end_acc= total_evl_end_acc / float(len(evl_end_labels))\n","    #evl_accuracy = (real_evl_start_acc + real_evl_end_acc) /2\n","    evl_loss= ((total_start_evl_loss/number_of_evls) + (total_end_evl_loss/number_of_evls))/2\n","      \n","\n","    print(f\"Epoch: {epoch} -- train loss: {train_loss} - train start acc: {real_train_start_acc*100} - train end acc: {real_train_end_acc*100} \"\n","          f\"dev loss: {evl_loss} - dev start acc: {real_evl_start_acc*100} - dev end acc: {real_evl_end_acc*100} \")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch: 0 -- train loss: 3.2455472254753115 - train start acc: 22.11111111111111 - train end acc: 28.7 dev loss: 4.808918595314026 - dev start acc: 6.800000000000001 - dev end acc: 8.0 \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pRh6bYOkhmy3"},"source":["#SAVE & LOAD trained model:"]},{"cell_type":"markdown","metadata":{"id":"UFmtvnp1iJdv"},"source":["##TO SAVE"]},{"cell_type":"code","metadata":{"id":"udLMKfl3hrFK","executionInfo":{"status":"ok","timestamp":1620542562660,"user_tz":-120,"elapsed":995,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["torch.save(model.state_dict(), './model/Trained_model.tsv')"],"execution_count":49,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qt2hSNthiZ9z"},"source":["##TO LOAD:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":175,"referenced_widgets":["dd750c7387a6432faf0313a71a113fff","c7f53c184ba94708a30ec126900e17ef","7ca34c09c03145dea745a9b51dda5175","d5125b1faebc440cb50c7a0c4438c46a","26562fa1432d407fb6103acbd18530fc","987766dd951344a5baf1a13b62d03ec8","7b0b676fe4774580bfe2f8db1880df5b","3196e196a4cc47b3a79c9936d5b77a98"]},"id":"swovu72_ib74","executionInfo":{"status":"ok","timestamp":1620571046535,"user_tz":-120,"elapsed":9087,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"1174e76e-1dbd-4f81-9c08-f4fe550d9d05"},"source":["#device = torch.device(\"cuda\") # if gpu is provided, otherwise run: device = torch.device(\"cpu\")\n","model = ForQuestionAnsweringClassifier(512,512)\n"],"execution_count":35,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dd750c7387a6432faf0313a71a113fff","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=267967963.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rK5zsAGLCQN3","executionInfo":{"status":"ok","timestamp":1620571064282,"user_tz":-120,"elapsed":1181,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"9268953e-d178-45ba-8886-9ca89774dad3"},"source":["model.load_state_dict(torch.load('./model/Trained_model.tsv',map_location=torch.device('cpu'))) # run when gpu is not provided\n","#model.load_state_dict(torch.load('./model/Trained_model.tsv')) # run only when the gpu is provided\n","model.to(device)"],"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ForQuestionAnsweringClassifier(\n","  (Distil_Bert_model): DistilBertForQuestionAnswering(\n","    (distilbert): DistilBertModel(\n","      (embeddings): Embeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (transformer): Transformer(\n","        (layer): ModuleList(\n","          (0): TransformerBlock(\n","            (attention): MultiHeadSelfAttention(\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (ffn): FFN(\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          )\n","        )\n","      )\n","    )\n","    (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n","    (dropout): Dropout(p=0.2, inplace=False)\n","  )\n","  (start_cls_layer): Linear(in_features=512, out_features=512, bias=True)\n","  (end_cls_layer): Linear(in_features=512, out_features=512, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"markdown","metadata":{"id":"0Q_PbhaRkz02"},"source":["**To Run on the loaded model, just go back to 'TRAINING AND EVALUATION' section.**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":183},"id":"2ky4CxrWbzDx","executionInfo":{"status":"error","timestamp":1619273686423,"user_tz":-120,"elapsed":733,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"38804bde-cc97-4e9b-a949-3587f65159f5"},"source":["import gc\n","del model\n","gc.collect()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-56-03fd7919080e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'gc' is not defined"]}]}]}