{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BertModel_Project.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMRXJ2J7Pt1k2hCxSc8K2aQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"9xRzApEvQ3Ex"},"source":["#PREPARE GPU FOR the training model"]},{"cell_type":"markdown","metadata":{"id":"TRdXGjr9lw_M"},"source":["#REFERENCES:"]},{"cell_type":"markdown","metadata":{"id":"wGFVI6dulzLn"},"source":["## Reference_1: Fine tune Bert https://www.youtube.com/watch?v=x66kkDnbzi4 by ChrisMcCormickAI\n"]},{"cell_type":"markdown","metadata":{"id":"_kqyH5Nrl68Y"},"source":["## Reference_2: applying SQUAD 1.0 dataset to BertForAnsweringQuestion already trained with SQUAD: https://www.youtube.com/watch?v=l8ZYCvgGu0o&list=WL&index=118&t=878s by ChrisMcCormickAI"]},{"cell_type":"markdown","metadata":{"id":"8bB-NNl9mKlK"},"source":["## Reference_3: 'Question Answering with SQuAD 2.0' section from: https://huggingface.co/transformers/custom_datasets"]},{"cell_type":"markdown","metadata":{"id":"Xpehpj9xmDHU"},"source":["## Reference_4: Basic knowledge about fine tuning, input formate and output format of BERT models : https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/?fbclid=IwAR3uWlc8mUlrJ3QnYoYyQOfze3yDYkacgVyKSk24YjYE04Gs-7XiM3b9gTA "]},{"cell_type":"markdown","metadata":{"id":"x0RdLqN2mg1v"},"source":["#SET UP GPU FOR TRAINING MODEL:"]},{"cell_type":"markdown","metadata":{"id":"mjFzznLlln9t"},"source":["**FIRSTLY, Setup GPU for training**\n","edit -> Notebook setting -> Hardware accelerator -> GPU"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2UWDmvmpQ6cT","executionInfo":{"status":"ok","timestamp":1619807735628,"user_tz":-120,"elapsed":8400,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"0cdc9306-c93b-4ce1-921f-55d0c025b323"},"source":["import tensorflow as tf\n","\n","# Get GPU device name:\n","device_name= tf.test.gpu_device_name()\n","\n","# GPU device should have the following name:\n","if device_name == \"/device:GPU:0\":\n","  print(\"Found GPU at: \" + device_name)\n","else:\n","  raise SystemError(\"GPU not found\") # \"GPU not found\" a parameter to pass to the SystemError for printing out  "],"execution_count":1,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pSkc_2hLSt_h","executionInfo":{"status":"ok","timestamp":1619807744656,"user_tz":-120,"elapsed":4189,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"3d17a432-9f1e-49cb-fd77-9b43ed2be974"},"source":["import torch\n","\n","# if there is a GPU device available..\n","if torch.cuda.is_available():\n","\n","  # Tell TORCH to use this GPU:\n","  device= torch.device(\"cuda\")\n","\n","  print(\"There are %d GPU(s) available\" % torch.cuda.device_count())\n","\n","  print('we will use the GPU: ', torch.cuda.get_device_name(0))\n","\n","# if not:\n","else:\n","  print('NO GPU available, using CPU instead')\n","  device = torch.device(\"cpu\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available\n","we will use the GPU:  Tesla T4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XRl4SHS9-DHW"},"source":["#IMPORT DATASET"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":589},"id":"6IVbSVon9-cd","executionInfo":{"status":"ok","timestamp":1619807749073,"user_tz":-120,"elapsed":635,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"09d25264-6501-4825-9d36-25b8b7173fbf"},"source":["\n","\n","# libraries for project:\n","import pandas as pd\n","import tensorflow as tf\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from collections import defaultdict\n","# necessary libraries for project:\n","\n","dataset = pd.read_table('./data/final.tsv')  \n","dataset"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>question</th>\n","      <th>is_impossible</th>\n","      <th>text</th>\n","      <th>answer_start</th>\n","      <th>context</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>When did Beyonce start becoming popular?</td>\n","      <td>False</td>\n","      <td>in the late 1990s</td>\n","      <td>269</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>What areas did Beyonce compete in when she was...</td>\n","      <td>False</td>\n","      <td>singing and dancing</td>\n","      <td>207</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>When did Beyonce leave Destiny's Child and bec...</td>\n","      <td>False</td>\n","      <td>2003</td>\n","      <td>526</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>In what city and state did Beyonce  grow up?</td>\n","      <td>False</td>\n","      <td>Houston, Texas</td>\n","      <td>166</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>In which decade did Beyonce become famous?</td>\n","      <td>False</td>\n","      <td>late 1990s</td>\n","      <td>276</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9996</th>\n","      <td>What famous World War II battle was the Canadi...</td>\n","      <td>False</td>\n","      <td>the Normandy Landings</td>\n","      <td>166</td>\n","      <td>Battles which are particularly notable to the ...</td>\n","    </tr>\n","    <tr>\n","      <th>9997</th>\n","      <td>What effort was the Canadian Military known fo...</td>\n","      <td>False</td>\n","      <td>the strategic bombing of German cities</td>\n","      <td>288</td>\n","      <td>Battles which are particularly notable to the ...</td>\n","    </tr>\n","    <tr>\n","      <th>9998</th>\n","      <td>What Battle in France was the Canadian Militar...</td>\n","      <td>False</td>\n","      <td>the Battle of Vimy Ridge</td>\n","      <td>72</td>\n","      <td>Battles which are particularly notable to the ...</td>\n","    </tr>\n","    <tr>\n","      <th>9999</th>\n","      <td>What country was the latest Canadian Military ...</td>\n","      <td>False</td>\n","      <td>Croatia</td>\n","      <td>377</td>\n","      <td>Battles which are particularly notable to the ...</td>\n","    </tr>\n","    <tr>\n","      <th>10000</th>\n","      <td>Who are the Battle of Normandy Landings and th...</td>\n","      <td>True</td>\n","      <td>the Canadian military</td>\n","      <td>42</td>\n","      <td>Battles which are particularly notable to the ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10001 rows × 5 columns</p>\n","</div>"],"text/plain":["                                                question  ...                                            context\n","0               When did Beyonce start becoming popular?  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","1      What areas did Beyonce compete in when she was...  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","2      When did Beyonce leave Destiny's Child and bec...  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","3          In what city and state did Beyonce  grow up?   ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","4             In which decade did Beyonce become famous?  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","...                                                  ...  ...                                                ...\n","9996   What famous World War II battle was the Canadi...  ...  Battles which are particularly notable to the ...\n","9997   What effort was the Canadian Military known fo...  ...  Battles which are particularly notable to the ...\n","9998   What Battle in France was the Canadian Militar...  ...  Battles which are particularly notable to the ...\n","9999   What country was the latest Canadian Military ...  ...  Battles which are particularly notable to the ...\n","10000  Who are the Battle of Normandy Landings and th...  ...  Battles which are particularly notable to the ...\n","\n","[10001 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"BX4IdqIz-lL4"},"source":["##TRIM (Or SAMPLE) DOWN THE DATASET FOR TRAINING:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":589},"id":"lqSTataa-hpY","executionInfo":{"status":"ok","timestamp":1619807753394,"user_tz":-120,"elapsed":735,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"ecff2410-0b3e-47d5-9d82-4c049a9ebbcc"},"source":["# shuffling the dataset first beforing triming down:\n","# reference: https://stackoverflow.com/questions/29576430/shuffle-dataframe-rows\n","dataset = dataset.sample(frac=1).reset_index(drop=True)\n","dataset"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>question</th>\n","      <th>is_impossible</th>\n","      <th>text</th>\n","      <th>answer_start</th>\n","      <th>context</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>What are the largest photovoltaic solar power ...</td>\n","      <td>False</td>\n","      <td>The 250 MW Agua Caliente Solar Project, in the...</td>\n","      <td>325</td>\n","      <td>Commercial CSP plants were first developed in ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>When is the Taoiseach reelected?</td>\n","      <td>True</td>\n","      <td>after every general election</td>\n","      <td>171</td>\n","      <td>Some states, however, do have a term of office...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Because of a dog's resourcefulness to people, ...</td>\n","      <td>False</td>\n","      <td>man's best friend</td>\n","      <td>356</td>\n","      <td>The dogs' value to early human hunter-gatherer...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>What do you get when dividing tandem repeats b...</td>\n","      <td>True</td>\n","      <td>The proportion of repetitive DNA</td>\n","      <td>0</td>\n","      <td>The proportion of repetitive DNA is calculated...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Quantum is a division of what other organizati...</td>\n","      <td>False</td>\n","      <td>Spectre</td>\n","      <td>33</td>\n","      <td>Despite being an original story, Spectre draws...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9996</th>\n","      <td>What does craving carry with it?</td>\n","      <td>False</td>\n","      <td>defilements</td>\n","      <td>126</td>\n","      <td>In Theravāda Buddhism, the cause of human exis...</td>\n","    </tr>\n","    <tr>\n","      <th>9997</th>\n","      <td>What was the most searched term on Google for ...</td>\n","      <td>False</td>\n","      <td>Beyonce pregnant</td>\n","      <td>719</td>\n","      <td>In August, the couple attended the 2011 MTV Vi...</td>\n","    </tr>\n","    <tr>\n","      <th>9998</th>\n","      <td>Queens is located on what part of Long Island?</td>\n","      <td>False</td>\n","      <td>the west end</td>\n","      <td>221</td>\n","      <td>New York City is located on one of the world's...</td>\n","    </tr>\n","    <tr>\n","      <th>9999</th>\n","      <td>How many scientists believe that symbiosis sho...</td>\n","      <td>True</td>\n","      <td>130</td>\n","      <td>276</td>\n","      <td>The definition of symbiosis has varied among s...</td>\n","    </tr>\n","    <tr>\n","      <th>10000</th>\n","      <td>In what year did Apple face multiple intellect...</td>\n","      <td>False</td>\n","      <td>2005</td>\n","      <td>3</td>\n","      <td>In 2005, Apple faced two lawsuits claiming pat...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10001 rows × 5 columns</p>\n","</div>"],"text/plain":["                                                question  ...                                            context\n","0      What are the largest photovoltaic solar power ...  ...  Commercial CSP plants were first developed in ...\n","1                       When is the Taoiseach reelected?  ...  Some states, however, do have a term of office...\n","2      Because of a dog's resourcefulness to people, ...  ...  The dogs' value to early human hunter-gatherer...\n","3      What do you get when dividing tandem repeats b...  ...  The proportion of repetitive DNA is calculated...\n","4      Quantum is a division of what other organizati...  ...  Despite being an original story, Spectre draws...\n","...                                                  ...  ...                                                ...\n","9996                    What does craving carry with it?  ...  In Theravāda Buddhism, the cause of human exis...\n","9997   What was the most searched term on Google for ...  ...  In August, the couple attended the 2011 MTV Vi...\n","9998      Queens is located on what part of Long Island?  ...  New York City is located on one of the world's...\n","9999   How many scientists believe that symbiosis sho...  ...  The definition of symbiosis has varied among s...\n","10000  In what year did Apple face multiple intellect...  ...  In 2005, Apple faced two lawsuits claiming pat...\n","\n","[10001 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":589},"id":"bg9aHfVx-wYJ","executionInfo":{"status":"ok","timestamp":1619807760869,"user_tz":-120,"elapsed":647,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"f396ada5-ffa9-4669-c46d-453082655e3e"},"source":["# SAMPLE DOWN NUMBER OF DATASET FOR TRAINING AND EVALUATION:\n","dataset=dataset.iloc[:1000,:]\n","dataset"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>question</th>\n","      <th>is_impossible</th>\n","      <th>text</th>\n","      <th>answer_start</th>\n","      <th>context</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>What are the largest photovoltaic solar power ...</td>\n","      <td>False</td>\n","      <td>The 250 MW Agua Caliente Solar Project, in the...</td>\n","      <td>325</td>\n","      <td>Commercial CSP plants were first developed in ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>When is the Taoiseach reelected?</td>\n","      <td>True</td>\n","      <td>after every general election</td>\n","      <td>171</td>\n","      <td>Some states, however, do have a term of office...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Because of a dog's resourcefulness to people, ...</td>\n","      <td>False</td>\n","      <td>man's best friend</td>\n","      <td>356</td>\n","      <td>The dogs' value to early human hunter-gatherer...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>What do you get when dividing tandem repeats b...</td>\n","      <td>True</td>\n","      <td>The proportion of repetitive DNA</td>\n","      <td>0</td>\n","      <td>The proportion of repetitive DNA is calculated...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Quantum is a division of what other organizati...</td>\n","      <td>False</td>\n","      <td>Spectre</td>\n","      <td>33</td>\n","      <td>Despite being an original story, Spectre draws...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>995</th>\n","      <td>Which friend took on the role of several jobs ...</td>\n","      <td>False</td>\n","      <td>Julian Fontana</td>\n","      <td>133</td>\n","      <td>Two Polish friends in Paris were also to play ...</td>\n","    </tr>\n","    <tr>\n","      <th>996</th>\n","      <td>Who decided to place Beyonce's group in Star S...</td>\n","      <td>False</td>\n","      <td>Arne Frager</td>\n","      <td>303</td>\n","      <td>At age eight, Beyoncé and childhood friend Kel...</td>\n","    </tr>\n","    <tr>\n","      <th>997</th>\n","      <td>What was federally-governed Maastricht?</td>\n","      <td>True</td>\n","      <td>several border territories</td>\n","      <td>31</td>\n","      <td>After the Peace of Westphalia, several border ...</td>\n","    </tr>\n","    <tr>\n","      <th>998</th>\n","      <td>What kind of system is a solar chimney?</td>\n","      <td>False</td>\n","      <td>passive solar ventilation</td>\n","      <td>59</td>\n","      <td>A solar chimney (or thermal chimney, in this c...</td>\n","    </tr>\n","    <tr>\n","      <th>999</th>\n","      <td>How many people are estimated to have died as ...</td>\n","      <td>False</td>\n","      <td>14,000</td>\n","      <td>685</td>\n","      <td>The area north of the Congo River came under F...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1000 rows × 5 columns</p>\n","</div>"],"text/plain":["                                              question  ...                                            context\n","0    What are the largest photovoltaic solar power ...  ...  Commercial CSP plants were first developed in ...\n","1                     When is the Taoiseach reelected?  ...  Some states, however, do have a term of office...\n","2    Because of a dog's resourcefulness to people, ...  ...  The dogs' value to early human hunter-gatherer...\n","3    What do you get when dividing tandem repeats b...  ...  The proportion of repetitive DNA is calculated...\n","4    Quantum is a division of what other organizati...  ...  Despite being an original story, Spectre draws...\n","..                                                 ...  ...                                                ...\n","995  Which friend took on the role of several jobs ...  ...  Two Polish friends in Paris were also to play ...\n","996  Who decided to place Beyonce's group in Star S...  ...  At age eight, Beyoncé and childhood friend Kel...\n","997            What was federally-governed Maastricht?  ...  After the Peace of Westphalia, several border ...\n","998            What kind of system is a solar chimney?  ...  A solar chimney (or thermal chimney, in this c...\n","999  How many people are estimated to have died as ...  ...  The area north of the Congo River came under F...\n","\n","[1000 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"TUaLEj7c_CS2"},"source":["#EXTRACTING THE START AND END TOKENS OF AN ANSWER IN A CONTEXT"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":713},"id":"UhhB5pVh_J3i","executionInfo":{"status":"ok","timestamp":1619807764380,"user_tz":-120,"elapsed":674,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"34877727-7626-469f-c2cc-87cdac2dd60c"},"source":["# Small run:\n","# drop the question mark in the 'question' column:\n","def Drop_question_mark (quest):\n","  l= len(quest)\n","  return quest[:l-1]\n","\n","dataset['question'] = dataset['question'].apply(Drop_question_mark)\n","dataset"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  import sys\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>question</th>\n","      <th>is_impossible</th>\n","      <th>text</th>\n","      <th>answer_start</th>\n","      <th>context</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>What are the largest photovoltaic solar power ...</td>\n","      <td>False</td>\n","      <td>The 250 MW Agua Caliente Solar Project, in the...</td>\n","      <td>325</td>\n","      <td>Commercial CSP plants were first developed in ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>When is the Taoiseach reelected</td>\n","      <td>True</td>\n","      <td>after every general election</td>\n","      <td>171</td>\n","      <td>Some states, however, do have a term of office...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Because of a dog's resourcefulness to people, ...</td>\n","      <td>False</td>\n","      <td>man's best friend</td>\n","      <td>356</td>\n","      <td>The dogs' value to early human hunter-gatherer...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>What do you get when dividing tandem repeats b...</td>\n","      <td>True</td>\n","      <td>The proportion of repetitive DNA</td>\n","      <td>0</td>\n","      <td>The proportion of repetitive DNA is calculated...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Quantum is a division of what other organization?</td>\n","      <td>False</td>\n","      <td>Spectre</td>\n","      <td>33</td>\n","      <td>Despite being an original story, Spectre draws...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>995</th>\n","      <td>Which friend took on the role of several jobs ...</td>\n","      <td>False</td>\n","      <td>Julian Fontana</td>\n","      <td>133</td>\n","      <td>Two Polish friends in Paris were also to play ...</td>\n","    </tr>\n","    <tr>\n","      <th>996</th>\n","      <td>Who decided to place Beyonce's group in Star S...</td>\n","      <td>False</td>\n","      <td>Arne Frager</td>\n","      <td>303</td>\n","      <td>At age eight, Beyoncé and childhood friend Kel...</td>\n","    </tr>\n","    <tr>\n","      <th>997</th>\n","      <td>What was federally-governed Maastricht</td>\n","      <td>True</td>\n","      <td>several border territories</td>\n","      <td>31</td>\n","      <td>After the Peace of Westphalia, several border ...</td>\n","    </tr>\n","    <tr>\n","      <th>998</th>\n","      <td>What kind of system is a solar chimney</td>\n","      <td>False</td>\n","      <td>passive solar ventilation</td>\n","      <td>59</td>\n","      <td>A solar chimney (or thermal chimney, in this c...</td>\n","    </tr>\n","    <tr>\n","      <th>999</th>\n","      <td>How many people are estimated to have died as ...</td>\n","      <td>False</td>\n","      <td>14,000</td>\n","      <td>685</td>\n","      <td>The area north of the Congo River came under F...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1000 rows × 5 columns</p>\n","</div>"],"text/plain":["                                              question  ...                                            context\n","0    What are the largest photovoltaic solar power ...  ...  Commercial CSP plants were first developed in ...\n","1                      When is the Taoiseach reelected  ...  Some states, however, do have a term of office...\n","2    Because of a dog's resourcefulness to people, ...  ...  The dogs' value to early human hunter-gatherer...\n","3    What do you get when dividing tandem repeats b...  ...  The proportion of repetitive DNA is calculated...\n","4    Quantum is a division of what other organization?  ...  Despite being an original story, Spectre draws...\n","..                                                 ...  ...                                                ...\n","995  Which friend took on the role of several jobs ...  ...  Two Polish friends in Paris were also to play ...\n","996  Who decided to place Beyonce's group in Star S...  ...  At age eight, Beyoncé and childhood friend Kel...\n","997             What was federally-governed Maastricht  ...  After the Peace of Westphalia, several border ...\n","998             What kind of system is a solar chimney  ...  A solar chimney (or thermal chimney, in this c...\n","999  How many people are estimated to have died as ...  ...  The area north of the Congo River came under F...\n","\n","[1000 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"b0Eugu8h_ixA"},"source":["##Import BertTokenizer for extracting the end and start tokens of an answer (in this case, the 'text') in a 'context'. [BertTokenizer also later used for tokenized and add special tokens [CLS], [SEP] for the input data of the Bert model]"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"csiKNpsL_e7D","executionInfo":{"status":"ok","timestamp":1619807773682,"user_tz":-120,"elapsed":3728,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"9ada4def-294a-4a21-cb96-6cad199ba6d4"},"source":["# INSTALL TRANSFORMER TO IMPORT BERT:\n","! pip install transformers"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AnXal1rUANd1","executionInfo":{"status":"ok","timestamp":1619807775814,"user_tz":-120,"elapsed":837,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["from transformers import DistilBertTokenizerFast"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"EFch0LTeAQtp","executionInfo":{"status":"ok","timestamp":1619807777214,"user_tz":-120,"elapsed":669,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["#Load the BERT tokenizer:\n","tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f9OHS7DoAX12"},"source":["##Extracting the necessary data from the 'dataset', such as 'context', 'text', 'question' as lists of data elements for easily hanlding this task. [Later these data lists also used as input for step TRANSFORMING the dataset into appropriate format input of Bert model]"]},{"cell_type":"code","metadata":{"id":"ac_PNbRYBAK1","executionInfo":{"status":"ok","timestamp":1619807779489,"user_tz":-120,"elapsed":639,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["# list of all the 'context' from the dataset\n","context= dataset.context.values\n","\n","# list of all the 'question' from the dataset\n","question = dataset.question.values\n","\n","# list of all the 'answer' from the dataset\n","answers = dataset.text.values\n"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RlpqKw7-BNMk"},"source":["##THE MAIN STEP : for extracting the end and start tokens of an 'answer' in a 'context'"]},{"cell_type":"code","metadata":{"id":"MnfbCBj0BUSX","executionInfo":{"status":"ok","timestamp":1619807781192,"user_tz":-120,"elapsed":541,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["def extract_answer_start_end_tokens(answer, context):\n","  for i in range (0, len(context)-(len(answer)-2)+1):\n","    if context[i] == answer[1]: # find the first token of the answer in the context\n","      \n","      for j in range (1, len(answer)-1):\n","        if context[i+j-1] != answer[j]: # if the next tokens in the context are not in the answer\n","          break; # stop\n","      \n","      if j == len(answer)-2: # reach the end of the answer, in other words, the 'for-loop' of j reaches the end:\n","        # we have found the answer start and end indices in the context:\n","        start_token = i\n","        end_token= i+j-1\n","        return start_token, end_token\n","      # else: we move on to the next value in the context to keep searching for the answer start and end indices in the context.\n","  return 0, 0 # can not find the answer in the context"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tX9IvUrTBWCU","executionInfo":{"status":"ok","timestamp":1619807784371,"user_tz":-120,"elapsed":534,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"316ee058-e4ae-49b7-f385-bcb5a89d8c02"},"source":["# TEST:\n","# tokenize a 'context'\n","tokenized_context= tokenizer(context[1])\n","\n","# extract only the input_ids from the tokenized result:\n","input_ids_tokenized_context = tokenized_context['input_ids'] \n","\n","# tokenize an 'answer'\n","tokenized_answer= tokenizer(answers[1])\n","\n","# extract only the input_ids from the tokenized result:\n","input_ids_tokenized_answer = tokenized_answer['input_ids']\n","\n","start, end = extract_answer_start_end_tokens(input_ids_tokenized_answer, input_ids_tokenized_context)\n","print(start)\n","print(end)\n","test_list=[]\n","for i in range (start, end+1):\n","  test_list.append(input_ids_tokenized_context[i])\n","\n","tokens= tokenizer.convert_ids_to_tokens(test_list)\n","\n","for token, id in zip(tokens, test_list):\n","  if id == tokenizer.sep_token_id:\n","    print(\" \")\n","  print('{:12} {:>6}'.format(token,id))\n","  if id == tokenizer.sep_token_id:\n","    print(\" \")"],"execution_count":12,"outputs":[{"output_type":"stream","text":["38\n","41\n","after          2044\n","every          2296\n","general        2236\n","election       2602\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OFIWI6W-Bo2M"},"source":["**EXTRACTING START AND END TOKENS OF AN 'ANSWER' IN EVERY 'CONTEXT' IN THE DATASET**"]},{"cell_type":"code","metadata":{"id":"ts7D9IS1Byz6","executionInfo":{"status":"ok","timestamp":1619807787935,"user_tz":-120,"elapsed":683,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["start_labels = []\n","end_labels = []"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"15gysNjEB0Gq","executionInfo":{"status":"ok","timestamp":1619807789485,"user_tz":-120,"elapsed":621,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["assert len(context) == len(answers) # must be TRUE"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"3WtF2MU9B2Np","executionInfo":{"status":"ok","timestamp":1619807790895,"user_tz":-120,"elapsed":738,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["import random\n","\n","def fit_max_length (input, max_len):\n","\n","  # find the range of the 'context' tokens:\n","  for i in range (0, len(input)):\n","\n","    if input[i] == 102: # encouter the first [SEP], which is the end of 'context\n","      range_context= i\n","      break\n","  \n","  # randomly drop some tokens in the 'context' to make the input len = 512:\n","\n","  len_difference= len(input) - max_len\n","  \n","  # generate a list of random indices from 0 to range_context (not include (0 and value of range_context) ):\n","  # reference:\n","  \n","  for i in range (0, len_difference):\n","    d_index=random.randint( 1, range_context-1)\n","    del input[d_index]\n","\n","    # the range_conext should be decreased by 1 due to the deleted tokens:\n","    range_context= range_context-1\n","  \n","  return input"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"bmLeqDPAB2f1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619807794389,"user_tz":-120,"elapsed":1011,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"d86c4778-4d2f-42a5-878b-4cce5fcfab5d"},"source":["for i in range (0, len(context)):\n","  # tokenize a 'context'\n","  tokenized_context= tokenizer(context[i])\n","\n","  # extract only the input_ids from the tokenized result:\n","  input_ids_tokenized_context = tokenized_context['input_ids'] \n","\n","  #resize the 'context' ids to maximum length of 512:\n","  input_ids_tokenized_context =fit_max_length(input_ids_tokenized_context,512) \n","\n","  # tokenize an 'answer'\n","  tokenized_answer= tokenizer(answers[i])\n","\n","  # extract only the input_ids from the tokenized result:\n","  input_ids_tokenized_answer = tokenized_answer['input_ids']\n","\n","  start, end = extract_answer_start_end_tokens(input_ids_tokenized_answer, input_ids_tokenized_context)\n","\n","  start_labels.append(start)\n","  end_labels.append(end)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (720 > 512). Running this sequence through the model will result in indexing errors\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"37lG9XREB9lx","executionInfo":{"status":"ok","timestamp":1619807808591,"user_tz":-120,"elapsed":751,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["# TEST:\n","assert len(start_labels) == len(end_labels)\n","assert len(start_labels) == len(context)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UqW4qZfPB-Ot","executionInfo":{"status":"ok","timestamp":1619297469436,"user_tz":-120,"elapsed":552,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"7bcade99-e36e-4642-e2b6-d7c0d6142d72"},"source":["# TEST:\n","# TEST:\n","# tokenize a 'context'\n","index= 4 # choose an examplary 'context', by choose a random value for the index in range (0, len(context))\n","\n","tokenized_context= tokenizer(context[index])\n","\n","# extract only the input_ids from the tokenized result:\n","input_ids_tokenized_context = tokenized_context['input_ids'] \n","\n","# tokenize an 'answer'\n","tokenized_answer= tokenizer(answers[index])\n","\n","# extract only the input_ids from the tokenized result:\n","input_ids_tokenized_answer = tokenized_answer['input_ids']\n","\n","tst_start = start_labels[index]\n","tst_end = end_labels[index]\n","print(f'{tst_start}\\t{tst_end}')\n","\n","test_list=[]\n","for i in range (tst_start, tst_end+1):\n","  test_list.append(input_ids_tokenized_context[i])\n","\n","tokens= tokenizer.convert_ids_to_tokens(test_list)\n","\n","for token, id in zip(tokens, test_list):\n","  if id == tokenizer.sep_token_id:\n","    print(\" \")\n","  print('{:12} {:>6}'.format(token,id))\n","  if id == tokenizer.sep_token_id:\n","    print(\" \")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["69\t70\n","late           2397\n","1990s          4134\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5UnCR8MUChKn"},"source":["#TRANSFORMING the dataset into appropriate format input of Bert model"]},{"cell_type":"markdown","metadata":{"id":"_B_yWIe4EcVb"},"source":["##STEP 1: Tokenize the dataset and add special tokens [CLS], [SEP]. Then convert the tokenized the dataset into appropriate ids which are the indices of the lookup vocab table of the Bert model [Because the BertTokenizer is used for this task]"]},{"cell_type":"code","metadata":{"id":"ykJxrsx1Co88","executionInfo":{"status":"ok","timestamp":1619807813384,"user_tz":-120,"elapsed":534,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["# the maximum length of input sequence for bert-base-uncase is 512\n","\n","# check the length of the input sequence:\n","# because input sequence = a 'context' + a'question' \n","# the tokenized input is [CLS] + a 'context' + [SEP] + a'question'  +[SEP]\n","# => length of the tokenized input <= 512\n","\n","# => Check the len(tokenized input), if it > 512, drop some tokens in the 'context' to make len = 512.\n","\n","import random\n","\n","def fit_max_length (input, max_len):\n","\n","  # find the range of the 'context' tokens:\n","  for i in range (0, len(input)):\n","\n","    if input[i] == 102: # encouter the first [SEP], which is the end of 'context\n","      range_context= i\n","      break\n","  \n","  # randomly drop some tokens in the 'context' to make the input len = 512:\n","\n","  len_difference= len(input) - max_len\n","  \n","  # generate a list of random indices from 0 to range_context (not include (0 and value of range_context) ):\n","  # reference:\n","  \n","  for i in range (0, len_difference):\n","    d_index=random.randint( 1, range_context-1)\n","    del input[d_index]\n","\n","    # the range_conext should be decreased by 1 due to the deleted tokens:\n","    range_context= range_context-1\n","  \n","  return input"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"negOrGKsC5bq","executionInfo":{"status":"ok","timestamp":1619807816963,"user_tz":-120,"elapsed":589,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"a1e25422-b98e-49fe-da95-61e5c4a218b5"},"source":["# Test: fit_max_length()\n","tokenized_sentences = tokenizer(context[10],question[10], add_special_tokens= True)\n","# BECAUSE input for the Bert model will be [CLS] + 'context' + [SEP] + 'question' + [SEP] (requirement_1),\n","# => tokenizer(context[10],question[10], add_special_tokens= True) takes care of the requirement_1.\n","# context[c_index], question[1_index] : c_index must be the same as q_question ('conext' must correspond to its own 'question')\n","# these indices can be a integer number in range (0, len (context))\n","\n","ids= tokenized_sentences['input_ids']\n","\n","\n","max_len=200 # test with a random length\n","\n","new_input_ids= fit_max_length(ids,max_len)\n","\n","#len(new_input_ids), new_input_ids\n","\n","tokens= tokenizer.convert_ids_to_tokens(new_input_ids)\n","\n","for token, id in zip(tokens, new_input_ids):\n","  if id == tokenizer.sep_token_id:\n","    print(\" \")\n","  print('{:12} {:>6}'.format(token,id))\n","  if id == tokenizer.sep_token_id:\n","    print(\" \")\n","\n","len(new_input_ids)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["[CLS]           101\n","in             1999\n","higher         3020\n","education      2495\n",",              1010\n","polite        13205\n","##c            2278\n","##nic          8713\n","##o            2080\n","refers         5218\n","to             2000\n","a              1037\n","technical      4087\n","university     2118\n","awarding      21467\n","degrees        5445\n","in             1999\n","engineering    3330\n",".              1012\n","historically   7145\n","there          2045\n","were           2020\n","two            2048\n","polite        13205\n","##c            2278\n","##nic          8713\n","##i            2072\n",",              1010\n","one            2028\n","in             1999\n","each           2169\n","of             1997\n","the            1996\n","two            2048\n","largest        2922\n","industrial     3919\n","cities         3655\n","of             1997\n","the            1996\n","north          2167\n",":              1024\n"," \n","[SEP]           102\n"," \n","what           2054\n","term           2744\n","in             1999\n","higher         3020\n","education      2495\n","refers         5218\n","to             2000\n","technical      4087\n","universities   5534\n","that           2008\n","award          2400\n","engineering    3330\n","degrees        5445\n"," \n","[SEP]           102\n"," \n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["56"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"eeX7c1udDyZy","executionInfo":{"status":"ok","timestamp":1619807823570,"user_tz":-120,"elapsed":992,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["# Each entry at ith index of the input_ids is 'ids' like in the 'TEST' cell right above.\n","# Maximum sequence length for this model (512)\n","# => Each entry of the input_ids must be 512.\n","\n","input_ids=[]\n","max_len= 512\n","for cntx, quest in zip(context, question):\n","\n","  tokenized_sentences = tokenizer(cntx,quest, add_special_tokens= True)\n","  ids= tokenized_sentences['input_ids']\n","\n","  if len(ids) > 512: # if the sequence length > 512\n","    ids= fit_max_length(ids, max_len) # decrease it to 512\n","\n","  input_ids.append(ids)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I9ViYSDIELPD","executionInfo":{"status":"ok","timestamp":1619807825247,"user_tz":-120,"elapsed":671,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"abccd09f-2aee-49ab-9f59-aa98b88f3d72"},"source":["# TEST:\n","test_1=input_ids[4]\n","\n","tokens= tokenizer.convert_ids_to_tokens(test_1)\n","\n","for token, id in zip(tokens, test_1):\n","  if id == tokenizer.sep_token_id:\n","    print(\" \")\n","  print('{:12} {:>6}'.format(token,id))\n","  if id == tokenizer.sep_token_id:\n","    print(\" \")\n","print(len(test_1))"],"execution_count":21,"outputs":[{"output_type":"stream","text":["[CLS]           101\n","despite        2750\n","being          2108\n","an             2019\n","original       2434\n","story          2466\n",",              1010\n","spec          28699\n","##tre          7913\n","draws          9891\n","on             2006\n","ian            4775\n","fleming       13779\n","'              1005\n","s              1055\n","source         3120\n","material       3430\n",",              1010\n","most           2087\n","notably        5546\n","in             1999\n","the            1996\n","character      2839\n","of             1997\n","franz          8965\n","obe           15578\n","##rh          25032\n","##aus         20559\n","##er           2121\n",",              1010\n","played         2209\n","by             2011\n","christoph     21428\n","waltz         17569\n",".              1012\n","obe           15578\n","##rh          25032\n","##aus         20559\n","##er           2121\n","shares         6661\n","his            2010\n","name           2171\n","with           2007\n","han            7658\n","##nes          5267\n","obe           15578\n","##rh          25032\n","##aus         20559\n","##er           2121\n",",              1010\n","a              1037\n","background     4281\n","character      2839\n","in             1999\n","the            1996\n","short          2460\n","story          2466\n","\"              1000\n","octopus       24318\n","##sy           6508\n","\"              1000\n","from           2013\n","the            1996\n","octopus       24318\n","##sy           6508\n","and            1998\n","the            1996\n","living         2542\n","daylight      11695\n","##s            2015\n","collection     3074\n",",              1010\n","and            1998\n","who            2040\n","is             2003\n","named          2315\n","in             1999\n","the            1996\n","film           2143\n","as             2004\n","having         2383\n","been           2042\n","a              1037\n","temporary      5741\n","legal          3423\n","guardian       6697\n","of             1997\n","a              1037\n","young          2402\n","bond           5416\n","in             1999\n","1983           3172\n",".              1012\n","similarly      6660\n",",              1010\n","charm         11084\n","##ian          2937\n","bond           5416\n","is             2003\n","shown          3491\n","to             2000\n","have           2031\n","been           2042\n","his            2010\n","full           2440\n","-              1011\n","time           2051\n","guardian       6697\n",",              1010\n","observing     14158\n","the            1996\n","back           2067\n","story          2466\n","established    2511\n","by             2011\n","fleming       13779\n",".              1012\n","with           2007\n","the            1996\n","acquisition    7654\n","of             1997\n","the            1996\n","rights         2916\n","to             2000\n","spec          28699\n","##tre          7913\n","and            1998\n","its            2049\n","associated     3378\n","characters     3494\n",",              1010\n","screenwriter  11167\n","##s            2015\n","neal          11030\n","pu            16405\n","##rvis        29074\n","and            1998\n","robert         2728\n","wade          10653\n","revealed       3936\n","that           2008\n","the            1996\n","film           2143\n","would          2052\n","provide        3073\n","a              1037\n","minor          3576\n","re             2128\n","##tc          13535\n","##on           2239\n","to             2000\n","the            1996\n","continuity    13717\n","of             1997\n","the            1996\n","previous       3025\n","films          3152\n",",              1010\n","with           2007\n","the            1996\n","quantum        8559\n","organisation   5502\n","all            2035\n","##uded        13936\n","to             2000\n","in             1999\n","casino         9270\n","royale        24483\n","and            1998\n","introduced     3107\n","in             1999\n","quantum        8559\n","of             1997\n","sol           14017\n","##ace         10732\n","rei           24964\n","##ma           2863\n","##gin         11528\n","##ed           2098\n","as             2004\n","a              1037\n","division       2407\n","within         2306\n","spec          28699\n","##tre          7913\n","rather         2738\n","than           2084\n","an             2019\n","independent    2981\n","organisation   5502\n",".              1012\n"," \n","[SEP]           102\n"," \n","quantum        8559\n","is             2003\n","a              1037\n","division       2407\n","of             1997\n","what           2054\n","other          2060\n","organization   3029\n","?              1029\n"," \n","[SEP]           102\n"," \n","202\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"omi0sUy4EVW-"},"source":["# CHECK:\n","input_ids[10]\n","for i in range (0,len(input_ids)):\n","  if len(input_ids[i]) > 512:\n","    print(\"NOT OK\")\n","    break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i49UiYw1FAgO"},"source":["##PADDING: to make all the input sequences (in this case, an entry of the 'input_ids' list) the same length [because Bert model requires such a thing]"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SPD1bbAdFRSt","executionInfo":{"status":"ok","timestamp":1619807831242,"user_tz":-120,"elapsed":638,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"6ff937d8-7f02-40bf-b481-f81b3a0e5517"},"source":["# CHECK The maximum length of each sequence in input_ids:\n","max_len_input_ids=0\n","for i in range (0, len(input_ids)):\n","  if len(input_ids[i])> max_len_input_ids:\n","    max_len_input_ids= len(input_ids[i])\n","\n","max_len_input_ids "],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["512"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a0WC05ZdFUQ0","executionInfo":{"status":"ok","timestamp":1619807832719,"user_tz":-120,"elapsed":692,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"da75c5cc-0eb8-45cf-bf8b-0d4ddb7fa816"},"source":["# PADDING: FOR the input_ids\n","# [PAD] in Bert has value of 0\n","\n","from keras.preprocessing.sequence import pad_sequences\n","\n","# set the Max_len:\n","Max_len= 512 # set as the same value with the 'max_len_input_ids'\n","\n","# because '[PAD]' in Bert vocab look-up take has id (or index) =0 \n","# => we can padd 0 values at the end of each entries of 'input_ids'\n","pad_input_ids= pad_sequences(input_ids, maxlen=Max_len, dtype='long', value=0, truncating='post', padding='post')\n","\n","# check whether the padding and truncating after padding work as we expect:\n","assert len(pad_input_ids[10]) == Max_len\n","print(len(input_ids[0])) # original length of the input_ids[0]\n"],"execution_count":23,"outputs":[{"output_type":"stream","text":["186\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bZPD0Yg0FthU","executionInfo":{"status":"ok","timestamp":1619807837773,"user_tz":-120,"elapsed":982,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"2e5e23e1-a2ef-4079-85c9-3f5bbe52b9be"},"source":["# TEST:\n","test_1=pad_input_ids[5]\n","\n","tokens= tokenizer.convert_ids_to_tokens(test_1)\n","\n","for token, id in zip(tokens, test_1):\n","  if id == tokenizer.sep_token_id:\n","    print(\" \")\n","  print('{:12} {:>6}'.format(token,id))\n","  if id == tokenizer.sep_token_id:\n","    print(\" \")\n","print(len(test_1))"],"execution_count":24,"outputs":[{"output_type":"stream","text":["[CLS]           101\n","beyonce       20773\n","attended       3230\n","st             2358\n",".              1012\n","mary           2984\n","'              1005\n","s              1055\n","elementary     4732\n","school         2082\n","in             1999\n","frederick      5406\n","##sburg        9695\n",",              1010\n","texas          3146\n",",              1010\n","where          2073\n","she            2016\n","enrolled       8302\n","in             1999\n","dance          3153\n","classes        4280\n",".              1012\n","her            2014\n","singing        4823\n","talent         5848\n","was            2001\n","discovered     3603\n","when           2043\n","dance          3153\n","instructor     9450\n","dar           18243\n","##lette       27901\n","johnson        3779\n","began          2211\n","humming       20364\n","a              1037\n","song           2299\n","and            1998\n","she            2016\n","finished       2736\n","it             2009\n",",              1010\n","able           2583\n","to             2000\n","hit            2718\n","the            1996\n","high           2152\n","-              1011\n","pitched        8219\n","notes          3964\n",".              1012\n","beyonce       20773\n","'              1005\n","s              1055\n","interest       3037\n","in             1999\n","music          2189\n","and            1998\n","performing     4488\n","continued      2506\n","after          2044\n","winning        3045\n","a              1037\n","school         2082\n","talent         5848\n","show           2265\n","at             2012\n","age            2287\n","seven          2698\n",",              1010\n","singing        4823\n","john           2198\n","lennon        14294\n","'              1005\n","s              1055\n","\"              1000\n","imagine        5674\n","\"              1000\n","to             2000\n","beat           3786\n","15             2321\n","/              1013\n","16             2385\n","-              1011\n","year           2095\n","-              1011\n","olds          19457\n",".              1012\n","in             1999\n","fall           2991\n","of             1997\n","1990           2901\n",",              1010\n","beyonce       20773\n","enrolled       8302\n","in             1999\n","parker         6262\n","elementary     4732\n","school         2082\n",",              1010\n","a              1037\n","music          2189\n","magnet        16853\n","school         2082\n","in             1999\n","houston        5395\n",",              1010\n","where          2073\n","she            2016\n","would          2052\n","perform        4685\n","with           2007\n","the            1996\n","school         2082\n","'              1005\n","s              1055\n","choir          6596\n",".              1012\n","she            2016\n","also           2036\n","attended       3230\n","the            1996\n","high           2152\n","school         2082\n","for            2005\n","the            1996\n","performing     4488\n","and            1998\n","visual         5107\n","arts           2840\n","and            1998\n","later          2101\n","ali            4862\n","##ef          12879\n","el             3449\n","##si           5332\n","##k            2243\n","high           2152\n","school         2082\n",".              1012\n","beyonce       20773\n","was            2001\n","also           2036\n","a              1037\n","member         2266\n","of             1997\n","the            1996\n","choir          6596\n","at             2012\n","st             2358\n",".              1012\n","john           2198\n","'              1005\n","s              1055\n","united         2142\n","methodist      8938\n","church         2277\n","as             2004\n","a              1037\n","soloist       16504\n","for            2005\n","two            2048\n","years          2086\n",".              1012\n"," \n","[SEP]           102\n"," \n","beyonce       20773\n","moved          2333\n","to             2000\n","which          2029\n","town           2237\n","after          2044\n","she            2016\n","left           2187\n","her            2014\n","first          2034\n","elementary     4732\n","school         2082\n"," \n","[SEP]           102\n"," \n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","512\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Sybc2GZRF1fE"},"source":["#BESIDES, the input sequences, the SEGMENT MASK is also required as input for the Bert model"]},{"cell_type":"markdown","metadata":{"id":"CgVS2jp2F8oE"},"source":["as a sequence input in the 'input_ids' has format:\n","[CLS] + 'context' + [SEP] + 'question' + [SEP] + [PAD]s.\n","\n","Then we would assign a sequence of 1s (1, 1, 1, ...) for the first part [CLS] + 'context' + [SEP]; and assign a sequence of 0s (0, 0, 0, ...) for the second part 'question' + [SEP] + [PAD]s."]},{"cell_type":"code","metadata":{"id":"HALuVq_dGmU8","executionInfo":{"status":"ok","timestamp":1619807842146,"user_tz":-120,"elapsed":523,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["segment_masks=[] # consider [PAD]s belonging to the second sequence\n","for i in range (0, len(pad_input_ids)):\n","\n","  convert_to_list= pad_input_ids[i].tolist()\n","  sep_index= convert_to_list.index(tokenizer.sep_token_id)\n","\n","  # number of the 'context' (='answer') tokens includes the [SEP] also\n","  num_seg_a= sep_index+1\n","\n","  # the remainder is the 'question':\n","  num_seg_b= len(convert_to_list) - num_seg_a\n","\n","  # construct list of 0s and 1s:\n","  segment_ids= [1]*num_seg_a + [0]*num_seg_b # a segment mask for [CLS]+ a 'context' +[SEP] + 'text' + [SEP]\n","\n","  # add the segment_ids to the list of segment masks:\n","  segment_masks.append(segment_ids)"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"bx0DdaaHG69n","executionInfo":{"status":"ok","timestamp":1619807433728,"user_tz":-120,"elapsed":679,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["# TEST:\n","number_of_ones=0\n","position= 99 # take a segment mask in the list segment_maks for testing\n","for i in range (0, len(segment_masks[position])):\n","  if segment_masks[position][i] == 1:\n","    number_of_ones= number_of_ones+1\n","\n","number_of_zeros = len(segment_masks[position]) - number_of_ones\n","\n","# the real number of ones in the segment masks:\n","convert_to_list= pad_input_ids[position].tolist()\n","sep_index= convert_to_list.index(tokenizer.sep_token_id)\n","real_number_of_ones= sep_index+1\n","real_number_of_zeros= len(convert_to_list) - real_number_of_ones\n","#\n","assert number_of_ones== real_number_of_ones\n","assert number_of_zeros== real_number_of_zeros"],"execution_count":143,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mrbFRGw3InJf","executionInfo":{"status":"ok","timestamp":1619807435548,"user_tz":-120,"elapsed":725,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"2c25ae3c-64e0-4dce-e2eb-e14807dfa11e"},"source":["# TEST:\n","\n","for i in range (0, 15): # all segment_masks must have the same length\n","  print(len(segment_masks[i]))"],"execution_count":144,"outputs":[{"output_type":"stream","text":["512\n","512\n","512\n","512\n","512\n","512\n","512\n","512\n","512\n","512\n","512\n","512\n","512\n","512\n","512\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2loM3Md0IzF-"},"source":["#ATTTENTION MASK is also required together with the segment mask"]},{"cell_type":"markdown","metadata":{"id":"kFZ0qW_nJAU6"},"source":["as a sequence input in the 'input_ids' has format:\n","[CLS] + 'context' + [SEP] + 'question' + [SEP] + [PAD]s.\n","\n","Then we would assign a sequence of 1s (1, 1, 1, ...) for the first part [CLS] + 'context' + [SEP] + 'question' + [SEP]; and assign a sequence of 0s (0, 0, 0, ...) for the second part [PAD]s."]},{"cell_type":"code","metadata":{"id":"D-6gIcfPIxDR","executionInfo":{"status":"ok","timestamp":1619807846657,"user_tz":-120,"elapsed":737,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["# Create attention mask:\n","attention_masks= []\n","\n","for pad_sequence in pad_input_ids:\n","\n","  # because [PAD] has id = 0 => we could use this condition to apply the attension mask:\n","  attention_mask=[int(token_id >0) for token_id in pad_sequence]\n","\n","  # aggregate each mask of each padded sequence into a list attention_masks\n","  # with this method, we could preserve the corresponding order between a padded sequence and its mask:\n","  attention_masks.append(attention_mask)\n"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q2PlyuTGJ9M3","executionInfo":{"status":"ok","timestamp":1619807447359,"user_tz":-120,"elapsed":723,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"ef6b97d6-8151-49ad-cad3-d692999df2b6"},"source":["# CHECK attention_masks:\n","# check the mask for first sentence:\n","\n","# the first encoded sentence:\n","#print(pad_input_ids[0])\n","\n","# the mask of the first encoded sentence:\n","#print(attention_masks[0]) # there should be 19 values of '1' at the beginning.\n","\n","# count '1' values in the first attention mask, the result should be 174\n","c=0\n","for i in range (0, len(attention_masks[2])):\n","  if attention_masks[2][i]==1:\n","    c=c+1\n","print(c)"],"execution_count":146,"outputs":[{"output_type":"stream","text":["218\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ref1hjyaZDGn","executionInfo":{"status":"ok","timestamp":1619807851084,"user_tz":-120,"elapsed":888,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["assert len(attention_masks[10]) == 512"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5OdG8mx5Ljch"},"source":["#SPLIT the dataset, the segment masks, the attention maks, the start labels, the end labels into the train set and evaluation set"]},{"cell_type":"code","metadata":{"id":"TY_e8T1kL-FT","executionInfo":{"status":"ok","timestamp":1619807852875,"user_tz":-120,"elapsed":929,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["from sklearn.model_selection import train_test_split\n","\n","# split for the conformed input dataset of BERT (input_ids) and the labels list\n","train_inputs, evl_inputs, train_start_labels, evl_start_labels= train_test_split(pad_input_ids, start_labels, random_state=2018, test_size=0.1)\n","\n","# do the same for segment masks of conformed input dataset:\n","train_segment_masks, evl_segment_masks, train_end_labels, evl_end_labels= train_test_split(segment_masks, end_labels, random_state=2018, test_size=0.1)\n","# _,_ is used because masks need no labels.\n","\n","# do the same for segment masks of conformed input dataset:\n","train_attention_masks, evl_attention_masks,_,_= train_test_split(attention_masks, start_labels, random_state=2018, test_size=0.1)\n","# _,_ is used because masks need no labels."],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tkX8y4T4MCOg","executionInfo":{"status":"ok","timestamp":1619807855872,"user_tz":-120,"elapsed":516,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"16be8a34-7fa8-45c9-a6ec-5251f0e55578"},"source":["#train_inputs.shape, train_labels.shape\n","print(train_inputs.shape)"],"execution_count":29,"outputs":[{"output_type":"stream","text":["(900, 512)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cf-KetB2Mkzv"},"source":["#CONVERT the train set and the evaluation set into 'torch.tensor' type becuase Bert model requires 'torch.tensor' type as its valid input type."]},{"cell_type":"code","metadata":{"id":"pdGY_8yYMjwE","executionInfo":{"status":"ok","timestamp":1619807858727,"user_tz":-120,"elapsed":724,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["# for the input data:\n","train_inputs= torch.tensor([train_inputs])\n","evl_inputs= torch.tensor([evl_inputs])\n","\n","# for the start_labels:\n","train_start_labels= torch.tensor(train_start_labels)\n","evl_start_labels= torch.tensor(evl_start_labels)\n","\n","# for the end_labels:\n","train_end_labels= torch.tensor(train_end_labels)\n","evl_end_labels= torch.tensor(evl_end_labels)\n","\n","# for segment masks:\n","train_segment_masks= torch.tensor([train_segment_masks])\n","evl_segment_masks= torch.tensor([evl_segment_masks])\n","\n","# for attention masks:\n","train_attention_masks=torch.tensor([train_attention_masks])\n","evl_attention_masks=torch.tensor([evl_attention_masks])"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"swJd6nxPNJOA","executionInfo":{"status":"ok","timestamp":1619807862781,"user_tz":-120,"elapsed":661,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"fa7ca80a-79ba-4ce4-ba1a-965e0b67d6b3"},"source":["# check the shape:\n","train_inputs.shape, train_start_labels.shape, train_end_labels.shape, train_segment_masks.shape, train_attention_masks.shape"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([1, 900, 512]),\n"," torch.Size([900]),\n"," torch.Size([900]),\n"," torch.Size([1, 900, 512]),\n"," torch.Size([1, 900, 512]))"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"wCqi6FVaNOJg"},"source":["#Generate the train set and evaluation set in batches:"]},{"cell_type":"markdown","metadata":{"id":"WKUpzSs7NWqB"},"source":["**FUNCTION FOR GENERATING BATCHES with batch size chosen by user**"]},{"cell_type":"code","metadata":{"id":"Y8gWBSk0NV1m","executionInfo":{"status":"ok","timestamp":1619807866975,"user_tz":-120,"elapsed":542,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["# LEARN FROM PREVIOUS LECTURES AND LABS in the class:\n","class BatchedIterator:\n","    def __init__(self, *tensors, batch_size,**kwarg):\n","        # all tensors must have the same first dimension\n","        assert len(set(len(tensor) for tensor in tensors)) == 1\n","        #print(type(tensors))\n","        #print(tensors[1])\n","        self.tensors = tensors\n","        self.batch_size = batch_size\n","\n","        for keyword, value in kwarg.items():\n","            if keyword == \"shuffle\":\n","                self.shuffle=value\n","    \n","    def iterate_once(self):\n","        num_data = len(self.tensors[0][0]) # the length of the data\n","\n","        if self.shuffle== False:\n","          for start in range(0, num_data, self.batch_size):\n","              end = start + self.batch_size\n","              yield tuple(tensor[0][start:end] for tensor in self.tensors) \n","              #must be tensor[0], to access to real data\n","              # cause tensor size [1,..]; 1: is unecessary dimension\n","              # => must exclude it by using tensor[0]  \n","        else:\n","          all_batches=[] # to gather all the batches formed form the dataset\n","          for start in range(0, num_data, self.batch_size):\n","              end = start + self.batch_size\n","              all_batches.append(tuple(tensor[0][start:end] for tensor in self.tensors))\n","          \n","          # shuffle the batches: \n","          # reference: https://note.nkmk.me/en/python-random-shuffle/#:~:text=To%20randomly%20shuffle%20elements%20of,Python%2C%20use%20the%20random%20module.&text=random%20provides%20shuffle()%20that,used%20for%20strings%20and%20tuples.\n","          shuf_batches = random.sample(all_batches, len(all_batches))\n","\n","          # yield a batch in the list at a time:\n","          for i in range (0,len(shuf_batches)):\n","            yield shuf_batches[i]"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K-EKsJ47N_4r","executionInfo":{"status":"ok","timestamp":1619807872622,"user_tz":-120,"elapsed":655,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"2eca4af8-5024-4158-dfe9-2f44df6e1aca"},"source":["# extract for test sample:\n","test_train_inputs= train_inputs[:,:10,:]\n","\n","test_train_start_labels= train_start_labels[:10]\n","test_train_end_labels= train_end_labels[:10]\n","\n","test_train_segment_masks = train_segment_masks[:,:10,:]\n","test_train_attention_masks = train_attention_masks[:,:10,:]\n","\n","test_train_inputs.shape, test_train_start_labels.shape, test_train_end_labels.shape, test_train_segment_masks.shape, test_train_attention_masks.shape,"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([1, 10, 512]),\n"," torch.Size([10]),\n"," torch.Size([10]),\n"," torch.Size([1, 10, 512]),\n"," torch.Size([1, 10, 512]))"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bm4YTKKtOg60","executionInfo":{"status":"ok","timestamp":1619807873957,"user_tz":-120,"elapsed":529,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"549c58dc-4bd8-4930-f18a-49e5f8620461"},"source":["# the torch.size must be torch.size([1,...])\n","# but the test_train_labels has the torch.size([10])\n","# must make it into torch.size([1,10])\n","good_test_train_start_labels=torch.unsqueeze(test_train_start_labels, 0)\n","good_test_train_end_labels=torch.unsqueeze(test_train_end_labels, 0)\n","\n","#test:\n","good_test_train_start_labels.shape, good_test_train_end_labels.shape"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([1, 10]), torch.Size([1, 10]))"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"40z8-fA1Oak8","executionInfo":{"status":"ok","timestamp":1619807877610,"user_tz":-120,"elapsed":701,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"01deb324-c6d8-4851-9cf8-b7f9eab395f0"},"source":["# Test out the BatchedIterator:\n","\n","batch_size = 5\n","train_iter = BatchedIterator(test_train_inputs, good_test_train_start_labels,good_test_train_end_labels, test_train_segment_masks,test_train_attention_masks, batch_size=batch_size,shuffle=True)\n","\n","for train_batch, start_batch, end_batch, segment_mask_batch, attention_batch in train_iter.iterate_once():\n","  print(f'train_batch.type= {train_batch.shape}\\tstart_batch.type={start_batch.shape}\\tend_batch={end_batch.shape}\\tsegment_mask_batch={segment_mask_batch.shape}\\tattention_mask_batch={attention_batch.shape}')\n","  print(train_batch)"],"execution_count":35,"outputs":[{"output_type":"stream","text":["train_batch.type= torch.Size([5, 512])\tstart_batch.type=torch.Size([5])\tend_batch=torch.Size([5])\tsegment_mask_batch=torch.Size([5, 512])\tattention_mask_batch=torch.Size([5, 512])\n","tensor([[  101,  1999,  2249,  ...,     0,     0,     0],\n","        [  101, 20773, 21025,  ...,     0,     0,     0],\n","        [  101,  2087,  1997,  ...,     0,     0,     0],\n","        [  101,  1999,  2238,  ...,     0,     0,     0],\n","        [  101,  1037,  2193,  ...,     0,     0,     0]])\n","train_batch.type= torch.Size([5, 512])\tstart_batch.type=torch.Size([5])\tend_batch=torch.Size([5])\tsegment_mask_batch=torch.Size([5, 512])\tattention_mask_batch=torch.Size([5, 512])\n","tensor([[  101,  1996,  4145,  ...,     0,     0,     0],\n","        [  101,  1996,  2048,  ...,     0,     0,     0],\n","        [  101, 20773,  1005,  ...,     0,     0,     0],\n","        [  101,  2006,  2410,  ...,     0,     0,     0],\n","        [  101,  5148,  1024,  ...,     0,     0,     0]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"19vUl9BMOtxW"},"source":["#DEFINE BERT MODEL FOR TRAINING"]},{"cell_type":"markdown","metadata":{"id":"8dvypaBoOyDo"},"source":["**Bert model is too large, i would use DistlledBert model which is 40 percent smaller than Bert model but still preserves 97 percent performace of the Bert model**"]},{"cell_type":"code","metadata":{"id":"Hn3nIldXPF4A","executionInfo":{"status":"ok","timestamp":1619807882098,"user_tz":-120,"elapsed":556,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["# https://huggingface.co/transformers/model_doc/bert.html#bertforquestionanswering\n","from transformers import DistilBertForQuestionAnswering\n","\n","# reference for config a pretrained model : https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n","from transformers import DistilBertConfig"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"f-uej9ESNyqw","executionInfo":{"status":"ok","timestamp":1619807886000,"user_tz":-120,"elapsed":631,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["class ForQuestionAnsweringClassifier(nn.Module):\n","    def __init__(self,model_out_sequence_length, output_size, freeze_bert = True):\n","        super(ForQuestionAnsweringClassifier, self).__init__()\n","        # Configure DistilBERT's initialization\n","        self.bert_config = DistilBertConfig(n_layers=1, n_heads=2,qa_dropout=0.2, dim = 100)\n","                          \n","        self.Distil_Bert_model= DistilBertForQuestionAnswering(config=self.bert_config)\n","\n","        # Make DistilBERT layers untrainable\n","        for p in self.Distil_Bert_model.parameters():\n","          p.requires_grad = False\n","          p.trainable = False\n","                \n","        #Classification layer\n","        self.start_cls_layer = nn.Linear(model_out_sequence_length, output_size)\n","        self.end_cls_layer = nn.Linear(model_out_sequence_length, output_size)\n","\n","    def forward(self, data, attn_masks, start_label, end_label):\n","        bertOut= self.Distil_Bert_model(data, # the tokens representing our input\n","                attention_mask=attn_masks, start_positions= start_label, end_positions= end_label ) # the segment ids to differentiate the question and the answer\n","        start_labels= self.start_cls_layer(bertOut.start_logits)\n","        end_labels=self.start_cls_layer(bertOut.end_logits)\n","        return start_labels, end_labels"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"id":"4DmKLTg-SNYJ","executionInfo":{"status":"ok","timestamp":1619807889873,"user_tz":-120,"elapsed":658,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["model = ForQuestionAnsweringClassifier(512,512) #model_out_sequence_length = output_size"],"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"id":"qddgpBfwi-Qy","executionInfo":{"status":"ok","timestamp":1619807891342,"user_tz":-120,"elapsed":530,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"],"execution_count":39,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3UsLwAn0S8Dp","executionInfo":{"status":"ok","timestamp":1619807902996,"user_tz":-120,"elapsed":10784,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"0772b3fe-6422-4484-d28c-0b4e3b2049d9"},"source":["# Tell Pytorch to run this model on the GPU:\n","#for p in model.parameters():\n","#      p.requires_grad = False\n","model.to(device)"],"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ForQuestionAnsweringClassifier(\n","  (Distil_Bert_model): DistilBertForQuestionAnswering(\n","    (distilbert): DistilBertModel(\n","      (embeddings): Embeddings(\n","        (word_embeddings): Embedding(30522, 100, padding_idx=0)\n","        (position_embeddings): Embedding(512, 100)\n","        (LayerNorm): LayerNorm((100,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (transformer): Transformer(\n","        (layer): ModuleList(\n","          (0): TransformerBlock(\n","            (attention): MultiHeadSelfAttention(\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (q_lin): Linear(in_features=100, out_features=100, bias=True)\n","              (k_lin): Linear(in_features=100, out_features=100, bias=True)\n","              (v_lin): Linear(in_features=100, out_features=100, bias=True)\n","              (out_lin): Linear(in_features=100, out_features=100, bias=True)\n","            )\n","            (sa_layer_norm): LayerNorm((100,), eps=1e-12, elementwise_affine=True)\n","            (ffn): FFN(\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (lin1): Linear(in_features=100, out_features=3072, bias=True)\n","              (lin2): Linear(in_features=3072, out_features=100, bias=True)\n","            )\n","            (output_layer_norm): LayerNorm((100,), eps=1e-12, elementwise_affine=True)\n","          )\n","        )\n","      )\n","    )\n","    (qa_outputs): Linear(in_features=100, out_features=2, bias=True)\n","    (dropout): Dropout(p=0.2, inplace=False)\n","  )\n","  (start_cls_layer): Linear(in_features=512, out_features=512, bias=True)\n","  (end_cls_layer): Linear(in_features=512, out_features=512, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"markdown","metadata":{"id":"OZKtKU2CPv1l"},"source":["#TRAINING AND EVALUATION:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oyq8pf0tPygT","executionInfo":{"status":"ok","timestamp":1619807930181,"user_tz":-120,"elapsed":632,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"2d09c7e6-8011-4e39-e666-97d9e9db78d7"},"source":["# Convert the data into tensor so that the model can run on the data\n","# extract for test sample:\n","train_inputs\n","evl_inputs\n","\n","train_inputs= train_inputs.to(device)\n","evl_inputs = evl_inputs.to(device)\n","\n","train_start_labels\n","train_end_labels\n","\n","train_start_labels= train_start_labels.to(device)\n","train_end_labels= train_end_labels.to(device) \n","\n","\n","evl_start_labels\n","evl_end_labels\n","\n","evl_start_labels= evl_start_labels.to(device)\n","evl_end_labels= evl_end_labels.to(device)\n","\n","train_segment_masks\n","evl_segment_masks\n","\n","\n","train_segment_masks= train_segment_masks.to(device) \n","evl_segment_masks= evl_segment_masks.to(device) \n","\n","\n","\n","train_attention_masks\n","evl_attention_masks\n","\n","\n","train_attention_masks = train_attention_masks.to(device) \n","evl_attention_masks= evl_attention_masks.to(device) \n","\n","train_inputs.shape, train_start_labels.shape, train_end_labels.shape, train_segment_masks.shape, train_attention_masks.shape"],"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([1, 900, 512]),\n"," torch.Size([900]),\n"," torch.Size([900]),\n"," torch.Size([1, 900, 512]),\n"," torch.Size([1, 900, 512]))"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H1tbHvc8P5Z3","executionInfo":{"status":"ok","timestamp":1619807933868,"user_tz":-120,"elapsed":660,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"e9fd7bc9-6f3a-46f1-f152-d485a562f3b4"},"source":["# the torch.size must be torch.size([1,...])\n","# but the test_train_labels has the torch.size([10])\n","# must make it into torch.size([1,10])\n","good_train_start_labels=torch.unsqueeze(train_start_labels, 0)\n","good_train_end_labels=torch.unsqueeze(train_end_labels, 0)\n","\n","good_evl_start_labels=torch.unsqueeze(evl_start_labels, 0)\n","good_evl_end_labels=torch.unsqueeze(evl_end_labels, 0)\n","\n","#test:\n","good_train_start_labels.shape, good_train_end_labels.shape"],"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([1, 900]), torch.Size([1, 900]))"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"code","metadata":{"id":"CwUtfXUHP7GI","executionInfo":{"status":"ok","timestamp":1619807935853,"user_tz":-120,"elapsed":872,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["batch_size = 10\n","train_iter = BatchedIterator(train_inputs, good_train_start_labels, good_train_end_labels, train_segment_masks, train_attention_masks, batch_size=batch_size, shuffle=True)"],"execution_count":43,"outputs":[]},{"cell_type":"code","metadata":{"id":"s9n2XMBJQB4r","executionInfo":{"status":"ok","timestamp":1619807937530,"user_tz":-120,"elapsed":919,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["criterion = nn.CrossEntropyLoss()\n","criterion = criterion.cuda()\n","optimizer = optim.Adam(model.parameters())"],"execution_count":44,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q6iVr4YOQDv1","executionInfo":{"status":"ok","timestamp":1619807940137,"user_tz":-120,"elapsed":651,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["num_epochs = 4"],"execution_count":45,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":453},"id":"6rUns3QxQEhH","executionInfo":{"status":"error","timestamp":1619807944553,"user_tz":-120,"elapsed":3111,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"af0d029f-f8e8-4555-bde9-085f6d0cdfc4"},"source":["for epoch in range(num_epochs):\n","    model.train()\n","    # Training on train data\n","  \n","    for train_batch, start_batch, end_batch, segment_mask_batch, attention_batch in train_iter.iterate_once():\n","        \n","        #train_batch = train_batch.to(device)\n","        #attention_batch = attention_batch.to(device)\n","        #start_batch = start_batch.to(device)\n","        #end_batch = end_batch.to(device)\n","        # run the model on the inputs:\n","        #print(f'train_batch.shape= {train_batch.shape}\\nattention_batch.shape= {attention_batch.shape}\\nstart_batch.shape= {start_batch.shape}\\nend_batch.shape= {end_batch.shape}')\n","        start_predicts, end_predicts = model(train_batch, # the tokens representing our input\n","                attention_batch, start_batch, end_batch ) # the segment ids to differentiate the question and the answer\n","        \n","\n","        # FOR TASK 1:\n","        \n","        #y_out = model(X_batch)\n","        #print(y_out)\n","        # FOR TASK 1:\n","        \n","        # To understand the ouput of the model:\n","        # y_out.shape = [batch_size, output_size] # output_size = number of labels\n","        #print(f'y_out.shape= {y_out.shape} \\ny_batch.shape= {y_batch.shape} ')\n","\n","        #print(f'y_out.shape= {y_out.shape} \\ny_batch.shape= {y_batch.shape} ')\n","        #train_loss= outputs.loss\n","        #print(f'start_predict.shape={start_predicts.shape}\\tstart_batch={start_batch.shape}')\n","        start_loss= criterion(start_predicts, start_batch)\n","        end_loss= criterion(end_predicts, end_batch)\n","        #print(\"loss:\")\n","        #print(loss)\n","        optimizer.zero_grad()\n","        start_loss.backward()\n","        end_loss.backward()\n","        optimizer.step()\n","        \n","    model.eval()  # or model.train(False)\n","    \n","    #Move the training data for evaluation\n","    #train_inputs[0] = train_inputs[0].to(device)\n","    #train_attention_masks[0] = train_attention_masks[0].to(device)\n","    #train_start_labels = train_start_labels.to(device)\n","    #train_end_labels = train_end_labels.to(device)\n","        \n","    # evaluation on train data:\n","    start_predicts, end_predicts = model(train_inputs[0], # the tokens representing our input\n","                train_attention_masks[0], train_start_labels , train_end_labels) # the segment ids to differentiate the question and the answer\n","\n","    start_labels= start_predicts.argmax(axis=1)\n","    end_labels=end_predicts.argmax(axis=1)\n","\n","    #train_loss= outputs.loss\n","    start_train_loss = criterion(start_predicts, train_start_labels).item()\n","    end_train_loss = criterion(end_predicts, end_labels).item()\n","    train_loss= (start_train_loss + end_train_loss)/2\n","\n","    train_start_accuracy = (torch.eq(start_labels, train_start_labels).sum() / float(len(train_start_labels))).item()\n","    train_end_accuracy = (torch.eq(end_labels, train_end_labels).sum() / float(len(train_end_labels))).item()\n","    train_accuracy = (train_start_accuracy + train_end_accuracy) /2\n","    \n","\n","    \n","    # evaluation on eval data:\n","    #Move the training data for evaluation\n","    #evl_inputs[0] = evl_inputs[0].to(device)\n","    #evl_attention_masks[0] = evl_attention_masks[0].to(device)\n","    #evl_start_labels = evl_start_labels.to(device)\n","    #evl_end_labels = evl_end_labels.to(device)\n","    \n","\n","    evl_start_predicts, evl_end_predicts = model(evl_inputs[0], # the tokens representing our input\n","                evl_attention_masks[0], evl_start_labels , evl_end_labels) # the segment ids to differentiate the question and the answer\n","\n","    start_labels= evl_start_predicts.argmax(axis=1)\n","    end_labels=evl_end_predicts.argmax(axis=1)\n","\n","    start_dev_loss = criterion(evl_start_predicts, evl_start_labels).item()\n","    end_dev_loss = criterion(evl_end_predicts, evl_end_labels).item()\n","    dev_loss= (start_train_loss + end_train_loss)/2\n","    \n","    evl_start_accuracy = (torch.eq(start_labels, evl_start_labels).sum() / float(len(evl_start_labels))).item()\n","    evl_end_accuracy = (torch.eq(end_labels, evl_end_labels).sum() / float(len(evl_end_labels))).item()\n","    evl_accuracy = (evl_start_accuracy + evl_end_accuracy) /2\n","    \n","\n","    print(f\"Epoch: {epoch} -- train loss: {train_loss} - train acc: {train_accuracy*100} - \"\n","          f\"dev loss: {dev_loss} - dev acc: {evl_accuracy*100}\")"],"execution_count":46,"outputs":[{"output_type":"stream","text":["Epoch: 0 -- train loss: 4.329398274421692 - train acc: 2.7222222648561 - dev loss: 4.329398274421692 - dev acc: 1.4999999664723873\n"],"name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-46-227d9bb09ba5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# evaluation on train data:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     start_predicts, end_predicts = model(train_inputs[0], # the tokens representing our input\n\u001b[0;32m---> 49\u001b[0;31m                 train_attention_masks[0], train_start_labels , train_end_labels) # the segment ids to differentiate the question and the answer\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mstart_labels\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mstart_predicts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-37-83bd5c95d0c2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data, attn_masks, start_label, end_label)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         bertOut= self.Distil_Bert_model(data, # the tokens representing our input\n\u001b[0;32m---> 20\u001b[0;31m                 attention_mask=attn_masks, start_positions= start_label, end_positions= end_label ) # the segment ids to differentiate the question and the answer\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mstart_labels\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_cls_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbertOut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mend_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_cls_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbertOut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    707\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m         )\n\u001b[1;32m    711\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistilbert_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# (bs, max_query_len, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m         )\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             layer_outputs = layer_module(\n\u001b[0;32m--> 307\u001b[0;31m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m             )\n\u001b[1;32m    309\u001b[0m             \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;31m# Feed Forward Network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mffn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0mffn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mffn_output\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msa_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mapply_chunking_to_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mff_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   1815\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1817\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mff_chunk\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mff_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mgelu\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m   1457\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_unary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1458\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1459\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 5.27 GiB (GPU 0; 14.76 GiB total capacity; 5.65 GiB already allocated; 3.03 GiB free; 10.59 GiB reserved in total by PyTorch)"]}]},{"cell_type":"markdown","metadata":{"id":"pRh6bYOkhmy3"},"source":["#SAVE & LOAD trained model:"]},{"cell_type":"markdown","metadata":{"id":"UFmtvnp1iJdv"},"source":["##TO SAVE"]},{"cell_type":"code","metadata":{"id":"udLMKfl3hrFK"},"source":["torch.save(model.state_dict(), './model/Trained_model.tsv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qt2hSNthiZ9z"},"source":["##TO LOAD:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"swovu72_ib74","executionInfo":{"status":"ok","timestamp":1619296828684,"user_tz":-120,"elapsed":1693,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"104861e3-3ea9-4ce9-e144-4ba81928502d"},"source":["device = torch.device(\"cuda\")\n","model = ForQuestionAnsweringClassifier(449,449)\n","model.load_state_dict(torch.load('./model/Trained_model.tsv'))\n","model.to(device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["ForQuestionAnsweringClassifier(\n","  (Distil_Bert_model): DistilBertForQuestionAnswering(\n","    (distilbert): DistilBertModel(\n","      (embeddings): Embeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (transformer): Transformer(\n","        (layer): ModuleList(\n","          (0): TransformerBlock(\n","            (attention): MultiHeadSelfAttention(\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (ffn): FFN(\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          )\n","        )\n","      )\n","    )\n","    (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n","    (dropout): Dropout(p=0.2, inplace=False)\n","  )\n","  (start_cls_layer): Linear(in_features=449, out_features=449, bias=True)\n","  (end_cls_layer): Linear(in_features=449, out_features=449, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":178}]},{"cell_type":"markdown","metadata":{"id":"0Q_PbhaRkz02"},"source":["**To Run on the loaded model, just go back to 'TRAINING AND EVALUATION' section.**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":183},"id":"2ky4CxrWbzDx","executionInfo":{"status":"error","timestamp":1619273686423,"user_tz":-120,"elapsed":733,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"38804bde-cc97-4e9b-a949-3587f65159f5"},"source":["import gc\n","del model\n","gc.collect()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-56-03fd7919080e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'gc' is not defined"]}]}]}