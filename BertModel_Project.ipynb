{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BertModel_Project.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM+rkQXousgmaQCPZFhtu6k"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"9xRzApEvQ3Ex"},"source":["#PREPARE GPU FOR the training model"]},{"cell_type":"markdown","metadata":{"id":"TRdXGjr9lw_M"},"source":["#REFERENCES:"]},{"cell_type":"markdown","metadata":{"id":"wGFVI6dulzLn"},"source":["## Reference_1: Fine tune Bert https://www.youtube.com/watch?v=x66kkDnbzi4 by ChrisMcCormickAI\n"]},{"cell_type":"markdown","metadata":{"id":"_kqyH5Nrl68Y"},"source":["## Reference_2: applying SQUAD 1.0 dataset to BertForAnsweringQuestion already trained with SQUAD: https://www.youtube.com/watch?v=l8ZYCvgGu0o&list=WL&index=118&t=878s by ChrisMcCormickAI"]},{"cell_type":"markdown","metadata":{"id":"8bB-NNl9mKlK"},"source":["## Reference_3: 'Question Answering with SQuAD 2.0' section from: https://huggingface.co/transformers/custom_datasets"]},{"cell_type":"markdown","metadata":{"id":"Xpehpj9xmDHU"},"source":["## Reference_4: Basic knowledge about fine tuning, input formate and output format of BERT models : https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/?fbclid=IwAR3uWlc8mUlrJ3QnYoYyQOfze3yDYkacgVyKSk24YjYE04Gs-7XiM3b9gTA "]},{"cell_type":"markdown","metadata":{"id":"x0RdLqN2mg1v"},"source":["#SET UP GPU FOR TRAINING MODEL:"]},{"cell_type":"markdown","metadata":{"id":"mjFzznLlln9t"},"source":["**FIRSTLY, Setup GPU for training**\n","edit -> Notebook setting -> Hardware accelerator -> GPU"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2UWDmvmpQ6cT","executionInfo":{"status":"ok","timestamp":1619296672888,"user_tz":-120,"elapsed":532,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"3ecda33a-2776-488e-a5fd-c628e50f2ae4"},"source":["import tensorflow as tf\n","\n","# Get GPU device name:\n","device_name= tf.test.gpu_device_name()\n","\n","# GPU device should have the following name:\n","if device_name == \"/device:GPU:0\":\n","  print(\"Found GPU at: \" + device_name)\n","else:\n","  raise SystemError(\"GPU not found\") # \"GPU not found\" a parameter to pass to the SystemError for printing out  "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pSkc_2hLSt_h","executionInfo":{"status":"ok","timestamp":1619296674416,"user_tz":-120,"elapsed":516,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"6bdef18c-efed-4f93-ff70-da07e730ea97"},"source":["import torch\n","\n","# if there is a GPU device available..\n","if torch.cuda.is_available():\n","\n","  # Tell TORCH to use this GPU:\n","  device= torch.device(\"cuda\")\n","\n","  print(\"There are %d GPU(s) available\" % torch.cuda.device_count())\n","\n","  print('we will use the GPU: ', torch.cuda.get_device_name(0))\n","\n","# if not:\n","else:\n","  print('NO GPU available, using CPU instead')\n","  device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available\n","we will use the GPU:  Tesla P4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XRl4SHS9-DHW"},"source":["#IMPORT DATASET"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":589},"id":"6IVbSVon9-cd","executionInfo":{"status":"ok","timestamp":1619297436106,"user_tz":-120,"elapsed":551,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"0a46a25a-0b0c-457d-ecac-1a5ea7ed56e1"},"source":["\n","\n","# libraries for project:\n","import pandas as pd\n","import tensorflow as tf\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from collections import defaultdict\n","# necessary libraries for project:\n","\n","dataset = pd.read_table('./data/final.tsv')  \n","dataset"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>question</th>\n","      <th>is_impossible</th>\n","      <th>text</th>\n","      <th>answer_start</th>\n","      <th>context</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>When did Beyonce start becoming popular?</td>\n","      <td>False</td>\n","      <td>in the late 1990s</td>\n","      <td>269</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>What areas did Beyonce compete in when she was...</td>\n","      <td>False</td>\n","      <td>singing and dancing</td>\n","      <td>207</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>When did Beyonce leave Destiny's Child and bec...</td>\n","      <td>False</td>\n","      <td>2003</td>\n","      <td>526</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>In what city and state did Beyonce  grow up?</td>\n","      <td>False</td>\n","      <td>Houston, Texas</td>\n","      <td>166</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>In which decade did Beyonce become famous?</td>\n","      <td>False</td>\n","      <td>late 1990s</td>\n","      <td>276</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9996</th>\n","      <td>What famous World War II battle was the Canadi...</td>\n","      <td>False</td>\n","      <td>the Normandy Landings</td>\n","      <td>166</td>\n","      <td>Battles which are particularly notable to the ...</td>\n","    </tr>\n","    <tr>\n","      <th>9997</th>\n","      <td>What effort was the Canadian Military known fo...</td>\n","      <td>False</td>\n","      <td>the strategic bombing of German cities</td>\n","      <td>288</td>\n","      <td>Battles which are particularly notable to the ...</td>\n","    </tr>\n","    <tr>\n","      <th>9998</th>\n","      <td>What Battle in France was the Canadian Militar...</td>\n","      <td>False</td>\n","      <td>the Battle of Vimy Ridge</td>\n","      <td>72</td>\n","      <td>Battles which are particularly notable to the ...</td>\n","    </tr>\n","    <tr>\n","      <th>9999</th>\n","      <td>What country was the latest Canadian Military ...</td>\n","      <td>False</td>\n","      <td>Croatia</td>\n","      <td>377</td>\n","      <td>Battles which are particularly notable to the ...</td>\n","    </tr>\n","    <tr>\n","      <th>10000</th>\n","      <td>Who are the Battle of Normandy Landings and th...</td>\n","      <td>True</td>\n","      <td>the Canadian military</td>\n","      <td>42</td>\n","      <td>Battles which are particularly notable to the ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10001 rows × 5 columns</p>\n","</div>"],"text/plain":["                                                question  ...                                            context\n","0               When did Beyonce start becoming popular?  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","1      What areas did Beyonce compete in when she was...  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","2      When did Beyonce leave Destiny's Child and bec...  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","3          In what city and state did Beyonce  grow up?   ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","4             In which decade did Beyonce become famous?  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","...                                                  ...  ...                                                ...\n","9996   What famous World War II battle was the Canadi...  ...  Battles which are particularly notable to the ...\n","9997   What effort was the Canadian Military known fo...  ...  Battles which are particularly notable to the ...\n","9998   What Battle in France was the Canadian Militar...  ...  Battles which are particularly notable to the ...\n","9999   What country was the latest Canadian Military ...  ...  Battles which are particularly notable to the ...\n","10000  Who are the Battle of Normandy Landings and th...  ...  Battles which are particularly notable to the ...\n","\n","[10001 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":229}]},{"cell_type":"markdown","metadata":{"id":"BX4IdqIz-lL4"},"source":["##TRIM (Or SAMPLE) DOWN THE DATASET FOR TRAINING:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":589},"id":"lqSTataa-hpY","executionInfo":{"status":"ok","timestamp":1619296682036,"user_tz":-120,"elapsed":539,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"e3949dce-ff31-45b4-8657-76f3f5907538"},"source":["# shuffling the dataset first beforing triming down:\n","# reference: https://stackoverflow.com/questions/29576430/shuffle-dataframe-rows\n","dataset = dataset.sample(frac=1).reset_index(drop=True)\n","dataset"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>question</th>\n","      <th>is_impossible</th>\n","      <th>text</th>\n","      <th>answer_start</th>\n","      <th>context</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>When did she appear on the cover of GQ?</td>\n","      <td>False</td>\n","      <td>2013</td>\n","      <td>236</td>\n","      <td>In September 2010, Beyoncé made her runway mod...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>How many people watched the season 14 finale?</td>\n","      <td>False</td>\n","      <td>8.03 million</td>\n","      <td>621</td>\n","      <td>The continuing decline influenced further chan...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>How many people were in the earthquake emergen...</td>\n","      <td>False</td>\n","      <td>184</td>\n","      <td>39</td>\n","      <td>An earthquake emergency relief team of 184 peo...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>What were Fantasia Barrino, LaToya London and ...</td>\n","      <td>False</td>\n","      <td>the Three Divas</td>\n","      <td>141</td>\n","      <td>Much media attention on the season had been fo...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The Buddha refused to pay respect to who, duri...</td>\n","      <td>False</td>\n","      <td>Vedas</td>\n","      <td>575</td>\n","      <td>A particular criticism of the Buddha was Vedic...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9996</th>\n","      <td>Who was the 8th Karmapa Lama?</td>\n","      <td>False</td>\n","      <td>Mikyö Dorje</td>\n","      <td>245</td>\n","      <td>The Zhengde Emperor (r. 1505–1521), who enjoye...</td>\n","    </tr>\n","    <tr>\n","      <th>9997</th>\n","      <td>How did Washizawa refer to the city having the...</td>\n","      <td>False</td>\n","      <td>great nuisance</td>\n","      <td>76</td>\n","      <td>In Japan, the Mayor of Nagano, Shoichi Washiza...</td>\n","    </tr>\n","    <tr>\n","      <th>9998</th>\n","      <td>In what year did the population of New York re...</td>\n","      <td>False</td>\n","      <td>2010</td>\n","      <td>738</td>\n","      <td>In the 1970s, job losses due to industrial res...</td>\n","    </tr>\n","    <tr>\n","      <th>9999</th>\n","      <td>Principal photography began on what date?</td>\n","      <td>True</td>\n","      <td>5 July 2015</td>\n","      <td>418</td>\n","      <td>After wrapping up in England, production trave...</td>\n","    </tr>\n","    <tr>\n","      <th>10000</th>\n","      <td>What does the acronym NYPD stand for?</td>\n","      <td>False</td>\n","      <td>New York City Police Department</td>\n","      <td>4</td>\n","      <td>The New York City Police Department (NYPD) has...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10001 rows × 5 columns</p>\n","</div>"],"text/plain":["                                                question  ...                                            context\n","0                When did she appear on the cover of GQ?  ...  In September 2010, Beyoncé made her runway mod...\n","1          How many people watched the season 14 finale?  ...  The continuing decline influenced further chan...\n","2      How many people were in the earthquake emergen...  ...  An earthquake emergency relief team of 184 peo...\n","3      What were Fantasia Barrino, LaToya London and ...  ...  Much media attention on the season had been fo...\n","4      The Buddha refused to pay respect to who, duri...  ...  A particular criticism of the Buddha was Vedic...\n","...                                                  ...  ...                                                ...\n","9996                       Who was the 8th Karmapa Lama?  ...  The Zhengde Emperor (r. 1505–1521), who enjoye...\n","9997   How did Washizawa refer to the city having the...  ...  In Japan, the Mayor of Nagano, Shoichi Washiza...\n","9998   In what year did the population of New York re...  ...  In the 1970s, job losses due to industrial res...\n","9999           Principal photography began on what date?  ...  After wrapping up in England, production trave...\n","10000              What does the acronym NYPD stand for?  ...  The New York City Police Department (NYPD) has...\n","\n","[10001 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":140}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":589},"id":"bg9aHfVx-wYJ","executionInfo":{"status":"ok","timestamp":1619297445077,"user_tz":-120,"elapsed":508,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"1b0771b7-a5fe-41c6-edfa-9c86f108aa9f"},"source":["# SAMPLE DOWN NUMBER OF DATASET FOR TRAINING AND EVALUATION:\n","dataset=dataset.iloc[:500,:]\n","dataset"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>question</th>\n","      <th>is_impossible</th>\n","      <th>text</th>\n","      <th>answer_start</th>\n","      <th>context</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>When did Beyonce start becoming popular?</td>\n","      <td>False</td>\n","      <td>in the late 1990s</td>\n","      <td>269</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>What areas did Beyonce compete in when she was...</td>\n","      <td>False</td>\n","      <td>singing and dancing</td>\n","      <td>207</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>When did Beyonce leave Destiny's Child and bec...</td>\n","      <td>False</td>\n","      <td>2003</td>\n","      <td>526</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>In what city and state did Beyonce  grow up?</td>\n","      <td>False</td>\n","      <td>Houston, Texas</td>\n","      <td>166</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>In which decade did Beyonce become famous?</td>\n","      <td>False</td>\n","      <td>late 1990s</td>\n","      <td>276</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>495</th>\n","      <td>What are Beyoncé's backup singers called?</td>\n","      <td>False</td>\n","      <td>The Mamas</td>\n","      <td>216</td>\n","      <td>In 2006, Beyoncé introduced her all-female tou...</td>\n","    </tr>\n","    <tr>\n","      <th>496</th>\n","      <td>When did The Mamas make their debut?</td>\n","      <td>False</td>\n","      <td>the 2006 BET Awards</td>\n","      <td>343</td>\n","      <td>In 2006, Beyoncé introduced her all-female tou...</td>\n","    </tr>\n","    <tr>\n","      <th>497</th>\n","      <td>What characteristics has Beyonce received accl...</td>\n","      <td>False</td>\n","      <td>stage presence and voice</td>\n","      <td>36</td>\n","      <td>Beyoncé has received praise for her stage pres...</td>\n","    </tr>\n","    <tr>\n","      <th>498</th>\n","      <td>Which former president of Def Jam called Beyon...</td>\n","      <td>False</td>\n","      <td>L.A. Reid</td>\n","      <td>445</td>\n","      <td>Beyoncé has received praise for her stage pres...</td>\n","    </tr>\n","    <tr>\n","      <th>499</th>\n","      <td>For what does Beyonce receive praise?</td>\n","      <td>False</td>\n","      <td>stage presence</td>\n","      <td>36</td>\n","      <td>Beyoncé has received praise for her stage pres...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>500 rows × 5 columns</p>\n","</div>"],"text/plain":["                                              question  ...                                            context\n","0             When did Beyonce start becoming popular?  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","1    What areas did Beyonce compete in when she was...  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","2    When did Beyonce leave Destiny's Child and bec...  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","3        In what city and state did Beyonce  grow up?   ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","4           In which decade did Beyonce become famous?  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","..                                                 ...  ...                                                ...\n","495          What are Beyoncé's backup singers called?  ...  In 2006, Beyoncé introduced her all-female tou...\n","496               When did The Mamas make their debut?  ...  In 2006, Beyoncé introduced her all-female tou...\n","497  What characteristics has Beyonce received accl...  ...  Beyoncé has received praise for her stage pres...\n","498  Which former president of Def Jam called Beyon...  ...  Beyoncé has received praise for her stage pres...\n","499              For what does Beyonce receive praise?  ...  Beyoncé has received praise for her stage pres...\n","\n","[500 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":230}]},{"cell_type":"markdown","metadata":{"id":"TUaLEj7c_CS2"},"source":["#EXTRACTING THE START AND END TOKENS OF AN ANSWER IN A CONTEXT"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":713},"id":"UhhB5pVh_J3i","executionInfo":{"status":"ok","timestamp":1619297450791,"user_tz":-120,"elapsed":539,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"a13e50d9-8112-492a-d8a7-5c0fa3157324"},"source":["# Small run:\n","# drop the question mark in the 'question' column:\n","def Drop_question_mark (quest):\n","  l= len(quest)\n","  return quest[:l-1]\n","\n","dataset['question'] = dataset['question'].apply(Drop_question_mark)\n","dataset"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  import sys\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>question</th>\n","      <th>is_impossible</th>\n","      <th>text</th>\n","      <th>answer_start</th>\n","      <th>context</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>When did Beyonce start becoming popular</td>\n","      <td>False</td>\n","      <td>in the late 1990s</td>\n","      <td>269</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>What areas did Beyonce compete in when she was...</td>\n","      <td>False</td>\n","      <td>singing and dancing</td>\n","      <td>207</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>When did Beyonce leave Destiny's Child and bec...</td>\n","      <td>False</td>\n","      <td>2003</td>\n","      <td>526</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>In what city and state did Beyonce  grow up?</td>\n","      <td>False</td>\n","      <td>Houston, Texas</td>\n","      <td>166</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>In which decade did Beyonce become famous</td>\n","      <td>False</td>\n","      <td>late 1990s</td>\n","      <td>276</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>495</th>\n","      <td>What are Beyoncé's backup singers called</td>\n","      <td>False</td>\n","      <td>The Mamas</td>\n","      <td>216</td>\n","      <td>In 2006, Beyoncé introduced her all-female tou...</td>\n","    </tr>\n","    <tr>\n","      <th>496</th>\n","      <td>When did The Mamas make their debut</td>\n","      <td>False</td>\n","      <td>the 2006 BET Awards</td>\n","      <td>343</td>\n","      <td>In 2006, Beyoncé introduced her all-female tou...</td>\n","    </tr>\n","    <tr>\n","      <th>497</th>\n","      <td>What characteristics has Beyonce received accl...</td>\n","      <td>False</td>\n","      <td>stage presence and voice</td>\n","      <td>36</td>\n","      <td>Beyoncé has received praise for her stage pres...</td>\n","    </tr>\n","    <tr>\n","      <th>498</th>\n","      <td>Which former president of Def Jam called Beyon...</td>\n","      <td>False</td>\n","      <td>L.A. Reid</td>\n","      <td>445</td>\n","      <td>Beyoncé has received praise for her stage pres...</td>\n","    </tr>\n","    <tr>\n","      <th>499</th>\n","      <td>For what does Beyonce receive praise</td>\n","      <td>False</td>\n","      <td>stage presence</td>\n","      <td>36</td>\n","      <td>Beyoncé has received praise for her stage pres...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>500 rows × 5 columns</p>\n","</div>"],"text/plain":["                                              question  ...                                            context\n","0              When did Beyonce start becoming popular  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","1    What areas did Beyonce compete in when she was...  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","2    When did Beyonce leave Destiny's Child and bec...  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","3         In what city and state did Beyonce  grow up?  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","4            In which decade did Beyonce become famous  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n","..                                                 ...  ...                                                ...\n","495           What are Beyoncé's backup singers called  ...  In 2006, Beyoncé introduced her all-female tou...\n","496                When did The Mamas make their debut  ...  In 2006, Beyoncé introduced her all-female tou...\n","497  What characteristics has Beyonce received accl...  ...  Beyoncé has received praise for her stage pres...\n","498  Which former president of Def Jam called Beyon...  ...  Beyoncé has received praise for her stage pres...\n","499               For what does Beyonce receive praise  ...  Beyoncé has received praise for her stage pres...\n","\n","[500 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":231}]},{"cell_type":"markdown","metadata":{"id":"b0Eugu8h_ixA"},"source":["##Import BertTokenizer for extracting the end and start tokens of an answer (in this case, the 'text') in a 'context'. [BertTokenizer also later used for tokenized and add special tokens [CLS], [SEP] for the input data of the Bert model]"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"csiKNpsL_e7D","executionInfo":{"status":"ok","timestamp":1619296694625,"user_tz":-120,"elapsed":2804,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"6d281b45-576e-4b99-9621-802f52c391a3"},"source":["# INSTALL TRANSFORMER TO IMPORT BERT:\n","! pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AnXal1rUANd1"},"source":["from transformers import DistilBertTokenizerFast"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EFch0LTeAQtp"},"source":["#Load the BERT tokenizer:\n","tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f9OHS7DoAX12"},"source":["##Extracting the necessary data from the 'dataset', such as 'context', 'text', 'question' as lists of data elements for easily hanlding this task. [Later these data lists also used as input for step TRANSFORMING the dataset into appropriate format input of Bert model]"]},{"cell_type":"code","metadata":{"id":"ac_PNbRYBAK1"},"source":["# list of all the 'context' from the dataset\n","context= dataset.context.values\n","\n","# list of all the 'question' from the dataset\n","question = dataset.question.values\n","\n","# list of all the 'answer' from the dataset\n","answers = dataset.text.values\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RlpqKw7-BNMk"},"source":["##THE MAIN STEP : for extracting the end and start tokens of an 'answer' in a 'context'"]},{"cell_type":"code","metadata":{"id":"MnfbCBj0BUSX"},"source":["def extract_answer_start_end_tokens(answer, context):\n","  for i in range (0, len(context)-(len(answer)-2)+1):\n","    if context[i] == answer[1]: # find the first token of the answer in the context\n","      \n","      for j in range (1, len(answer)-1):\n","        if context[i+j-1] != answer[j]: # if the next tokens in the context are not in the answer\n","          break; # stop\n","      \n","      if j == len(answer)-2: # reach the end of the answer, in other words, the 'for-loop' of j reaches the end:\n","        # we have found the answer start and end indices in the context:\n","        start_token = i\n","        end_token= i+j-1\n","        return start_token, end_token\n","      # else: we move on to the next value in the context to keep searching for the answer start and end indices in the context.\n","  return 0, 0 # can not find the answer in the context"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tX9IvUrTBWCU","executionInfo":{"status":"ok","timestamp":1619296706491,"user_tz":-120,"elapsed":501,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"39d7e7d2-214c-4ca4-9d06-a32eea3d9f16"},"source":["# TEST:\n","# tokenize a 'context'\n","tokenized_context= tokenizer(context[1])\n","\n","# extract only the input_ids from the tokenized result:\n","input_ids_tokenized_context = tokenized_context['input_ids'] \n","\n","# tokenize an 'answer'\n","tokenized_answer= tokenizer(answers[1])\n","\n","# extract only the input_ids from the tokenized result:\n","input_ids_tokenized_answer = tokenized_answer['input_ids']\n","\n","start, end = extract_answer_start_end_tokens(input_ids_tokenized_answer, input_ids_tokenized_context)\n","print(start)\n","print(end)\n","test_list=[]\n","for i in range (start, end+1):\n","  test_list.append(input_ids_tokenized_context[i])\n","\n","tokens= tokenizer.convert_ids_to_tokens(test_list)\n","\n","for token, id in zip(tokens, test_list):\n","  if id == tokenizer.sep_token_id:\n","    print(\" \")\n","  print('{:12} {:>6}'.format(token,id))\n","  if id == tokenizer.sep_token_id:\n","    print(\" \")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["125\n","128\n","8              1022\n",".              1012\n","03             6021\n","million        2454\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OFIWI6W-Bo2M"},"source":["**EXTRACTING START AND END TOKENS OF AN 'ANSWER' IN EVERY 'CONTEXT' IN THE DATASET**"]},{"cell_type":"code","metadata":{"id":"ts7D9IS1Byz6"},"source":["start_labels = []\n","end_labels = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"15gysNjEB0Gq"},"source":["assert len(context) == len(answers) # must be TRUE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bmLeqDPAB2f1"},"source":["for i in range (0, len(context)):\n","  # tokenize a 'context'\n","  tokenized_context= tokenizer(context[i])\n","\n","  # extract only the input_ids from the tokenized result:\n","  input_ids_tokenized_context = tokenized_context['input_ids'] \n","\n","  # tokenize an 'answer'\n","  tokenized_answer= tokenizer(answers[i])\n","\n","  # extract only the input_ids from the tokenized result:\n","  input_ids_tokenized_answer = tokenized_answer['input_ids']\n","\n","  start, end = extract_answer_start_end_tokens(input_ids_tokenized_answer, input_ids_tokenized_context)\n","\n","  start_labels.append(start)\n","  end_labels.append(end)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"37lG9XREB9lx"},"source":["# TEST:\n","assert len(start_labels) == len(end_labels)\n","assert len(start_labels) == len(context)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UqW4qZfPB-Ot","executionInfo":{"status":"ok","timestamp":1619297469436,"user_tz":-120,"elapsed":552,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"7bcade99-e36e-4642-e2b6-d7c0d6142d72"},"source":["# TEST:\n","# TEST:\n","# tokenize a 'context'\n","index= 4 # choose an examplary 'context', by choose a random value for the index in range (0, len(context))\n","\n","tokenized_context= tokenizer(context[index])\n","\n","# extract only the input_ids from the tokenized result:\n","input_ids_tokenized_context = tokenized_context['input_ids'] \n","\n","# tokenize an 'answer'\n","tokenized_answer= tokenizer(answers[index])\n","\n","# extract only the input_ids from the tokenized result:\n","input_ids_tokenized_answer = tokenized_answer['input_ids']\n","\n","tst_start = start_labels[index]\n","tst_end = end_labels[index]\n","print(f'{tst_start}\\t{tst_end}')\n","\n","test_list=[]\n","for i in range (tst_start, tst_end+1):\n","  test_list.append(input_ids_tokenized_context[i])\n","\n","tokens= tokenizer.convert_ids_to_tokens(test_list)\n","\n","for token, id in zip(tokens, test_list):\n","  if id == tokenizer.sep_token_id:\n","    print(\" \")\n","  print('{:12} {:>6}'.format(token,id))\n","  if id == tokenizer.sep_token_id:\n","    print(\" \")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["69\t70\n","late           2397\n","1990s          4134\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5UnCR8MUChKn"},"source":["#TRANSFORMING the dataset into appropriate format input of Bert model"]},{"cell_type":"markdown","metadata":{"id":"_B_yWIe4EcVb"},"source":["##STEP 1: Tokenize the dataset and add special tokens [CLS], [SEP]. Then convert the tokenized the dataset into appropriate ids which are the indices of the lookup vocab table of the Bert model [Because the BertTokenizer is used for this task]"]},{"cell_type":"code","metadata":{"id":"ykJxrsx1Co88"},"source":["# the maximum length of input sequence for bert-base-uncase is 512\n","\n","# check the length of the input sequence:\n","# because input sequence = a 'context' + a'question' \n","# the tokenized input is [CLS] + a 'context' + [SEP] + a'question'  +[SEP]\n","# => length of the tokenized input <= 512\n","\n","# => Check the len(tokenized input), if it > 512, drop some tokens in the 'context' to make len = 512.\n","\n","import random\n","\n","def fit_max_length (input, max_len):\n","\n","  # find the range of the 'context' tokens:\n","  for i in range (0, len(input)):\n","\n","    if input[i] == 102: # encouter the first [SEP], which is the end of 'context\n","      range_context= i\n","      break\n","  \n","  # randomly drop some tokens in the 'context' to make the input len = 512:\n","\n","  len_difference= len(input) - max_len\n","  \n","  # generate a list of random indices from 0 to range_context (not include (0 and value of range_context) ):\n","  # reference:\n","  \n","  for i in range (0, len_difference):\n","    d_index=random.randint( 1, range_context-1)\n","    del input[d_index]\n","\n","    # the range_conext should be decreased by 1 due to the deleted tokens:\n","    range_context= range_context-1\n","  \n","  return input"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"negOrGKsC5bq","executionInfo":{"status":"ok","timestamp":1619296725436,"user_tz":-120,"elapsed":519,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"1be29cb5-adb9-401d-a95f-c88f073b7a14"},"source":["# Test: fit_max_length()\n","tokenized_sentences = tokenizer(context[10],question[10], add_special_tokens= True)\n","# BECAUSE input for the Bert model will be [CLS] + 'context' + [SEP] + 'question' + [SEP] (requirement_1),\n","# => tokenizer(context[10],question[10], add_special_tokens= True) takes care of the requirement_1.\n","# context[c_index], question[1_index] : c_index must be the same as q_question ('conext' must correspond to its own 'question')\n","# these indices can be a integer number in range (0, len (context))\n","\n","ids= tokenized_sentences['input_ids']\n","\n","\n","max_len=200 # test with a random length\n","\n","new_input_ids= fit_max_length(ids,max_len)\n","\n","#len(new_input_ids), new_input_ids\n","\n","tokens= tokenizer.convert_ids_to_tokens(new_input_ids)\n","\n","for token, id in zip(tokens, new_input_ids):\n","  if id == tokenizer.sep_token_id:\n","    print(\" \")\n","  print('{:12} {:>6}'.format(token,id))\n","  if id == tokenizer.sep_token_id:\n","    print(\" \")\n","\n","len(new_input_ids)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[CLS]           101\n","a              1037\n","brief          4766\n","shoot          5607\n","at             2012\n","london         2414\n","'              1005\n","s              1055\n","city           2103\n","hall           2534\n","was            2001\n","filmed         6361\n","on             2006\n","18             2324\n","april          2258\n","2015           2325\n",",              1010\n","while          2096\n","was            2001\n","on             2006\n",".              1012\n","on             2006\n","may            2089\n","2015           2325\n","took           2165\n","place          2173\n","on             2006\n","thames        11076\n","in             1999\n","london         2414\n",".              1012\n","scenes         5019\n","involving      5994\n","and            1998\n","##yd          25688\n","##oux         28700\n","on             2006\n","a              1037\n","##boat        11975\n","as             2004\n","well           2092\n","as             2004\n","a              1037\n","low            2659\n","flying         3909\n","helicopter     7739\n","near           2379\n","bridge         2958\n","were           2020\n","shot           2915\n","at             2012\n",",              1010\n","with           2007\n","filming        7467\n","temporarily    8184\n","closing        5494\n","both           2119\n","westminster    9434\n","and            1998\n","lamb          12559\n","bridges        7346\n",".              1012\n","scenes         5019\n","were           2020\n","also           2036\n","shot           2915\n","on             2006\n","the            1996\n","near           2379\n","mi             2771\n","##6            2575\n","s              1055\n","headquarters   4075\n","at             2012\n","va            12436\n","##ux           5602\n","##hall         9892\n","cross          2892\n",".              1012\n","the            1996\n","crew           3626\n","returned       2513\n","the            1996\n","river          2314\n","less           2625\n","than           2084\n","a              1037\n","week           2733\n","to             2000\n","film           2143\n","solely         9578\n","set            2275\n","westminster    9434\n","bridge         2958\n",".              1012\n","the            1996\n","london         2414\n","fire           2543\n","brigade        4250\n","was            2001\n","on             2006\n","set            2275\n","to             2000\n","simulate      26633\n","rain           4542\n","as             2004\n","well           2092\n","as             2004\n","monitor        8080\n","smoke          5610\n","used           2109\n","for            2005\n","filming        7467\n",".              1012\n","craig          7010\n",",              1010\n","se             7367\n","##yd          25688\n","##oux         28700\n",",              1010\n","and            1998\n","waltz         17569\n",",              1010\n","as             2004\n","well           2092\n","harris         5671\n","fi            10882\n","##enne        24336\n","##s            2015\n","were           2020\n","seen           2464\n","being          2108\n","filmed         6361\n",".              1012\n","prior          3188\n","to             2000\n","this           2023\n",",              1010\n","scenes         5019\n","involving      5994\n","fi            10882\n","##enne        24336\n","##s            2015\n","were           2020\n","shot           2915\n","at             2012\n","a              1037\n","restaurant     4825\n","in             1999\n","garden         3871\n",".              1012\n","then           2059\n","took           2165\n","place          2173\n","in             1999\n","tr            19817\n","##af          10354\n","##gar          6843\n","square         2675\n","in             1999\n","early          2220\n","june           2238\n",",              1010\n","crew           3626\n",",              1010\n","as             2004\n","well           2092\n","craig          7010\n",",              1010\n","##yd          25688\n","##oux         28700\n",",              1010\n","and            1998\n","returned       2513\n","to             2000\n","the            1996\n","thames        11076\n","for            2005\n","a              1037\n","final          2345\n","time           2051\n","to             2000\n","continue       3613\n","filming        7467\n","scenes         5019\n","previously     3130\n","shot           2915\n","on             2006\n","the            1996\n",".              1012\n"," \n","[SEP]           102\n"," \n","which          2029\n","bridges        7346\n","were           2020\n","shut           3844\n","down           2091\n","because        2138\n","of             1997\n","filming        7467\n"," \n","[SEP]           102\n"," \n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["200"]},"metadata":{"tags":[]},"execution_count":155}]},{"cell_type":"code","metadata":{"id":"eeX7c1udDyZy"},"source":["# Each entry at ith index of the input_ids is 'ids' like in the 'TEST' cell right above.\n","# Maximum sequence length for this model (512)\n","# => Each entry of the input_ids must be 512.\n","\n","input_ids=[]\n","max_len= 512\n","for cntx, quest in zip(context, question):\n","\n","  tokenized_sentences = tokenizer(cntx,quest, add_special_tokens= True)\n","  ids= tokenized_sentences['input_ids']\n","\n","  if len(ids) > 512: # if the sequence length > 512\n","    ids= fit_max_length(ids, max_len) # decrease it to 512\n","\n","  input_ids.append(ids)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I9ViYSDIELPD","executionInfo":{"status":"ok","timestamp":1619297492370,"user_tz":-120,"elapsed":519,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"75405023-fef4-4b96-adc1-78fd9560fae8"},"source":["# TEST:\n","test_1=input_ids[4]\n","\n","tokens= tokenizer.convert_ids_to_tokens(test_1)\n","\n","for token, id in zip(tokens, test_1):\n","  if id == tokenizer.sep_token_id:\n","    print(\" \")\n","  print('{:12} {:>6}'.format(token,id))\n","  if id == tokenizer.sep_token_id:\n","    print(\" \")\n","print(len(test_1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[CLS]           101\n","beyonce       20773\n","gi            21025\n","##selle       19358\n","knowles       22815\n","-              1011\n","carter         5708\n","(              1006\n","/              1013\n","bi            12170\n","##ː           23432\n","##ˈ           29715\n","##j            3501\n","##ɒ           29678\n","##nse         12325\n","##ɪ           29685\n","/              1013\n","bee           10506\n","-              1011\n","yo            10930\n","##n            2078\n","-              1011\n","say            2360\n",")              1007\n","(              1006\n","born           2141\n","september      2244\n","4              1018\n",",              1010\n","1981           3261\n",")              1007\n","is             2003\n","an             2019\n","american       2137\n","singer         3220\n",",              1010\n","songwriter     6009\n",",              1010\n","record         2501\n","producer       3135\n","and            1998\n","actress        3883\n",".              1012\n","born           2141\n","and            1998\n","raised         2992\n","in             1999\n","houston        5395\n",",              1010\n","texas          3146\n",",              1010\n","she            2016\n","performed      2864\n","in             1999\n","various        2536\n","singing        4823\n","and            1998\n","dancing        5613\n","competitions   6479\n","as             2004\n","a              1037\n","child          2775\n",",              1010\n","and            1998\n","rose           3123\n","to             2000\n","fame           4476\n","in             1999\n","the            1996\n","late           2397\n","1990s          4134\n","as             2004\n","lead           2599\n","singer         3220\n","of             1997\n","r              1054\n","&              1004\n","b              1038\n","girl           2611\n","-              1011\n","group          2177\n","destiny       10461\n","'              1005\n","s              1055\n","child          2775\n",".              1012\n","managed        3266\n","by             2011\n","her            2014\n","father         2269\n",",              1010\n","mathew        25436\n","knowles       22815\n",",              1010\n","the            1996\n","group          2177\n","became         2150\n","one            2028\n","of             1997\n","the            1996\n","world          2088\n","'              1005\n","s              1055\n","best           2190\n","-              1011\n","selling        4855\n","girl           2611\n","groups         2967\n","of             1997\n","all            2035\n","time           2051\n",".              1012\n","their          2037\n","hiatus        14221\n","saw            2387\n","the            1996\n","release        2713\n","of             1997\n","beyonce       20773\n","'              1005\n","s              1055\n","debut          2834\n","album          2201\n",",              1010\n","dangerously   20754\n","in             1999\n","love           2293\n","(              1006\n","2003           2494\n",")              1007\n",",              1010\n","which          2029\n","established    2511\n","her            2014\n","as             2004\n","a              1037\n","solo           3948\n","artist         3063\n","worldwide      4969\n",",              1010\n","earned         3687\n","five           2274\n","grammy         8922\n","awards         2982\n","and            1998\n","featured       2956\n","the            1996\n","billboard      4908\n","hot            2980\n","100            2531\n","number         2193\n","-              1011\n","one            2028\n","singles        3895\n","\"              1000\n","crazy          4689\n","in             1999\n","love           2293\n","\"              1000\n","and            1998\n","\"              1000\n","baby           3336\n","boy            2879\n","\"              1000\n",".              1012\n"," \n","[SEP]           102\n"," \n","in             1999\n","which          2029\n","decade         5476\n","did            2106\n","beyonce       20773\n","become         2468\n","famous         3297\n"," \n","[SEP]           102\n"," \n","174\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"omi0sUy4EVW-"},"source":["# CHECK:\n","input_ids[10]\n","for i in range (0,len(input_ids)):\n","  if len(input_ids[i]) > 512:\n","    print(\"NOT OK\")\n","    break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i49UiYw1FAgO"},"source":["##PADDING: to make all the input sequences (in this case, an entry of the 'input_ids' list) the same length [because Bert model requires such a thing]"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SPD1bbAdFRSt","executionInfo":{"status":"ok","timestamp":1619297500315,"user_tz":-120,"elapsed":507,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"8cc74ce6-f380-418b-d4c6-caef13867b6e"},"source":["# CHECK The maximum length of each sequence in input_ids:\n","max_len_input_ids=0\n","for i in range (0, len(input_ids)):\n","  if len(input_ids[i])> max_len_input_ids:\n","    max_len_input_ids= len(input_ids[i])\n","\n","max_len_input_ids "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["450"]},"metadata":{"tags":[]},"execution_count":240}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a0WC05ZdFUQ0","executionInfo":{"status":"ok","timestamp":1619297506058,"user_tz":-120,"elapsed":507,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"61025332-3e3a-4a09-c713-0b9e1077e2fc"},"source":["# PADDING: FOR the input_ids\n","# [PAD] in Bert has value of 0\n","\n","from keras.preprocessing.sequence import pad_sequences\n","\n","# set the Max_len:\n","Max_len= 450 # set as the same value with the 'max_len_input_ids'\n","\n","# because '[PAD]' in Bert vocab look-up take has id (or index) =0 \n","# => we can padd 0 values at the end of each entries of 'input_ids'\n","pad_input_ids= pad_sequences(input_ids, maxlen=Max_len, dtype='long', value=0, truncating='post', padding='post')\n","\n","# check whether the padding and truncating after padding work as we expect:\n","assert len(pad_input_ids[10]) == Max_len\n","print(len(input_ids[0])) # original length of the input_ids[0]\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["173\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bZPD0Yg0FthU","executionInfo":{"status":"ok","timestamp":1619297510209,"user_tz":-120,"elapsed":497,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"174384db-13b6-4dd1-e16f-af8219a9367c"},"source":["# TEST:\n","test_1=pad_input_ids[5]\n","\n","tokens= tokenizer.convert_ids_to_tokens(test_1)\n","\n","for token, id in zip(tokens, test_1):\n","  if id == tokenizer.sep_token_id:\n","    print(\" \")\n","  print('{:12} {:>6}'.format(token,id))\n","  if id == tokenizer.sep_token_id:\n","    print(\" \")\n","print(len(test_1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[CLS]           101\n","beyonce       20773\n","gi            21025\n","##selle       19358\n","knowles       22815\n","-              1011\n","carter         5708\n","(              1006\n","/              1013\n","bi            12170\n","##ː           23432\n","##ˈ           29715\n","##j            3501\n","##ɒ           29678\n","##nse         12325\n","##ɪ           29685\n","/              1013\n","bee           10506\n","-              1011\n","yo            10930\n","##n            2078\n","-              1011\n","say            2360\n",")              1007\n","(              1006\n","born           2141\n","september      2244\n","4              1018\n",",              1010\n","1981           3261\n",")              1007\n","is             2003\n","an             2019\n","american       2137\n","singer         3220\n",",              1010\n","songwriter     6009\n",",              1010\n","record         2501\n","producer       3135\n","and            1998\n","actress        3883\n",".              1012\n","born           2141\n","and            1998\n","raised         2992\n","in             1999\n","houston        5395\n",",              1010\n","texas          3146\n",",              1010\n","she            2016\n","performed      2864\n","in             1999\n","various        2536\n","singing        4823\n","and            1998\n","dancing        5613\n","competitions   6479\n","as             2004\n","a              1037\n","child          2775\n",",              1010\n","and            1998\n","rose           3123\n","to             2000\n","fame           4476\n","in             1999\n","the            1996\n","late           2397\n","1990s          4134\n","as             2004\n","lead           2599\n","singer         3220\n","of             1997\n","r              1054\n","&              1004\n","b              1038\n","girl           2611\n","-              1011\n","group          2177\n","destiny       10461\n","'              1005\n","s              1055\n","child          2775\n",".              1012\n","managed        3266\n","by             2011\n","her            2014\n","father         2269\n",",              1010\n","mathew        25436\n","knowles       22815\n",",              1010\n","the            1996\n","group          2177\n","became         2150\n","one            2028\n","of             1997\n","the            1996\n","world          2088\n","'              1005\n","s              1055\n","best           2190\n","-              1011\n","selling        4855\n","girl           2611\n","groups         2967\n","of             1997\n","all            2035\n","time           2051\n",".              1012\n","their          2037\n","hiatus        14221\n","saw            2387\n","the            1996\n","release        2713\n","of             1997\n","beyonce       20773\n","'              1005\n","s              1055\n","debut          2834\n","album          2201\n",",              1010\n","dangerously   20754\n","in             1999\n","love           2293\n","(              1006\n","2003           2494\n",")              1007\n",",              1010\n","which          2029\n","established    2511\n","her            2014\n","as             2004\n","a              1037\n","solo           3948\n","artist         3063\n","worldwide      4969\n",",              1010\n","earned         3687\n","five           2274\n","grammy         8922\n","awards         2982\n","and            1998\n","featured       2956\n","the            1996\n","billboard      4908\n","hot            2980\n","100            2531\n","number         2193\n","-              1011\n","one            2028\n","singles        3895\n","\"              1000\n","crazy          4689\n","in             1999\n","love           2293\n","\"              1000\n","and            1998\n","\"              1000\n","baby           3336\n","boy            2879\n","\"              1000\n",".              1012\n"," \n","[SEP]           102\n"," \n","in             1999\n","what           2054\n","r              1054\n","&              1004\n","b              1038\n","group          2177\n","was            2001\n","she            2016\n","the            1996\n","lead           2599\n","singer         3220\n"," \n","[SEP]           102\n"," \n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","[PAD]             0\n","450\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Sybc2GZRF1fE"},"source":["#BESIDES, the input sequences, the SEGMENT MASK is also required as input for the Bert model"]},{"cell_type":"markdown","metadata":{"id":"CgVS2jp2F8oE"},"source":["as a sequence input in the 'input_ids' has format:\n","[CLS] + 'context' + [SEP] + 'question' + [SEP] + [PAD]s.\n","\n","Then we would assign a sequence of 1s (1, 1, 1, ...) for the first part [CLS] + 'context' + [SEP]; and assign a sequence of 0s (0, 0, 0, ...) for the second part 'question' + [SEP] + [PAD]s."]},{"cell_type":"code","metadata":{"id":"HALuVq_dGmU8"},"source":["segment_masks=[] # consider [PAD]s belonging to the second sequence\n","for i in range (0, len(pad_input_ids)):\n","\n","  convert_to_list= pad_input_ids[i].tolist()\n","  sep_index= convert_to_list.index(tokenizer.sep_token_id)\n","\n","  # number of the 'context' (='answer') tokens includes the [SEP] also\n","  num_seg_a= sep_index+1\n","\n","  # the remainder is the 'question':\n","  num_seg_b= len(convert_to_list) - num_seg_a\n","\n","  # construct list of 0s and 1s:\n","  segment_ids= [1]*num_seg_a + [0]*num_seg_b # a segment mask for [CLS]+ a 'context' +[SEP] + 'text' + [SEP]\n","\n","  # add the segment_ids to the list of segment masks:\n","  segment_masks.append(segment_ids)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bx0DdaaHG69n"},"source":["# TEST:\n","number_of_ones=0\n","position= 99 # take a segment mask in the list segment_maks for testing\n","for i in range (0, len(segment_masks[position])):\n","  if segment_masks[position][i] == 1:\n","    number_of_ones= number_of_ones+1\n","\n","number_of_zeros = len(segment_masks[position]) - number_of_ones\n","\n","# the real number of ones in the segment masks:\n","convert_to_list= pad_input_ids[position].tolist()\n","sep_index= convert_to_list.index(tokenizer.sep_token_id)\n","real_number_of_ones= sep_index+1\n","real_number_of_zeros= len(convert_to_list) - real_number_of_ones\n","#\n","assert number_of_ones== real_number_of_ones\n","assert number_of_zeros== real_number_of_zeros"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mrbFRGw3InJf","executionInfo":{"status":"ok","timestamp":1619297519868,"user_tz":-120,"elapsed":495,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"5371e9ae-f9f2-4639-a26d-3bb6533d0736"},"source":["# TEST:\n","\n","for i in range (0, 15): # all segment_masks must have the same length\n","  print(len(segment_masks[i]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["450\n","450\n","450\n","450\n","450\n","450\n","450\n","450\n","450\n","450\n","450\n","450\n","450\n","450\n","450\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2loM3Md0IzF-"},"source":["#ATTTENTION MASK is also required together with the segment mask"]},{"cell_type":"markdown","metadata":{"id":"kFZ0qW_nJAU6"},"source":["as a sequence input in the 'input_ids' has format:\n","[CLS] + 'context' + [SEP] + 'question' + [SEP] + [PAD]s.\n","\n","Then we would assign a sequence of 1s (1, 1, 1, ...) for the first part [CLS] + 'context' + [SEP] + 'question' + [SEP]; and assign a sequence of 0s (0, 0, 0, ...) for the second part [PAD]s."]},{"cell_type":"code","metadata":{"id":"D-6gIcfPIxDR"},"source":["# Create attention mask:\n","attention_masks= []\n","\n","for pad_sequence in pad_input_ids:\n","\n","  # because [PAD] has id = 0 => we could use this condition to apply the attension mask:\n","  attention_mask=[int(token_id >0) for token_id in pad_sequence]\n","\n","  # aggregate each mask of each padded sequence into a list attention_masks\n","  # with this method, we could preserve the corresponding order between a padded sequence and its mask:\n","  attention_masks.append(attention_mask)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q2PlyuTGJ9M3","executionInfo":{"status":"ok","timestamp":1619297406924,"user_tz":-120,"elapsed":512,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"6aa8fbd7-4c95-44c8-e680-45102ee8f25e"},"source":["# CHECK attention_masks:\n","# check the mask for first sentence:\n","\n","# the first encoded sentence:\n","#print(pad_input_ids[0])\n","\n","# the mask of the first encoded sentence:\n","#print(attention_masks[0]) # there should be 19 values of '1' at the beginning.\n","\n","# count '1' values in the first attention mask, the result should be 174\n","c=0\n","for i in range (0, len(attention_masks[2])):\n","  if attention_masks[2][i]==1:\n","    c=c+1\n","print(c)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["73\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ref1hjyaZDGn"},"source":["assert len(attention_masks[10]) == 445"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5OdG8mx5Ljch"},"source":["#SPLIT the dataset, the segment masks, the attention maks, the start labels, the end labels into the train set and evaluation set"]},{"cell_type":"code","metadata":{"id":"TY_e8T1kL-FT"},"source":["from sklearn.model_selection import train_test_split\n","\n","# split for the conformed input dataset of BERT (input_ids) and the labels list\n","train_inputs, evl_inputs, train_start_labels, evl_start_labels= train_test_split(pad_input_ids, start_labels, random_state=2018, test_size=0.1)\n","\n","# do the same for segment masks of conformed input dataset:\n","train_segment_masks, evl_segment_masks, train_end_labels, evl_end_labels= train_test_split(segment_masks, end_labels, random_state=2018, test_size=0.1)\n","# _,_ is used because masks need no labels.\n","\n","# do the same for segment masks of conformed input dataset:\n","train_attention_masks, evl_attention_masks,_,_= train_test_split(attention_masks, start_labels, random_state=2018, test_size=0.1)\n","# _,_ is used because masks need no labels."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tkX8y4T4MCOg","executionInfo":{"status":"ok","timestamp":1619297527676,"user_tz":-120,"elapsed":558,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"0a410fa5-6a64-44ee-ce99-d0563d854d2d"},"source":["#train_inputs.shape, train_labels.shape\n","print(train_inputs.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(450, 450)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cf-KetB2Mkzv"},"source":["#CONVERT the train set and the evaluation set into 'torch.tensor' type becuase Bert model requires 'torch.tensor' type as its valid input type."]},{"cell_type":"code","metadata":{"id":"pdGY_8yYMjwE"},"source":["# for the input data:\n","train_inputs= torch.tensor([train_inputs])\n","evl_inputs= torch.tensor([evl_inputs])\n","\n","# for the start_labels:\n","train_start_labels= torch.tensor(train_start_labels)\n","evl_start_labels= torch.tensor(evl_start_labels)\n","\n","# for the end_labels:\n","train_end_labels= torch.tensor(train_end_labels)\n","evl_end_labels= torch.tensor(evl_end_labels)\n","\n","# for segment masks:\n","train_segment_masks= torch.tensor([train_segment_masks])\n","evl_segment_masks= torch.tensor([evl_segment_masks])\n","\n","# for attention masks:\n","train_attention_masks=torch.tensor([train_attention_masks])\n","evl_attention_masks=torch.tensor([evl_attention_masks])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"swJd6nxPNJOA","executionInfo":{"status":"ok","timestamp":1619297533669,"user_tz":-120,"elapsed":530,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"6c016853-59b7-49d1-d45e-144bdc89bf9e"},"source":["# check the shape:\n","train_inputs.shape, train_start_labels.shape, train_end_labels.shape, train_segment_masks.shape, train_attention_masks.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([1, 450, 450]),\n"," torch.Size([450]),\n"," torch.Size([450]),\n"," torch.Size([1, 450, 450]),\n"," torch.Size([1, 450, 450]))"]},"metadata":{"tags":[]},"execution_count":250}]},{"cell_type":"markdown","metadata":{"id":"wCqi6FVaNOJg"},"source":["#Generate the train set and evaluation set in batches:"]},{"cell_type":"markdown","metadata":{"id":"WKUpzSs7NWqB"},"source":["**FUNCTION FOR GENERATING BATCHES with batch size chosen by user**"]},{"cell_type":"code","metadata":{"id":"Y8gWBSk0NV1m"},"source":["# LEARN FROM PREVIOUS LECTURES AND LABS in the class:\n","class BatchedIterator:\n","    def __init__(self, *tensors, batch_size,**kwarg):\n","        # all tensors must have the same first dimension\n","        assert len(set(len(tensor) for tensor in tensors)) == 1\n","        #print(type(tensors))\n","        #print(tensors[1])\n","        self.tensors = tensors\n","        self.batch_size = batch_size\n","\n","        for keyword, value in kwarg.items():\n","            if keyword == \"shuffle\":\n","                self.shuffle=value\n","    \n","    def iterate_once(self):\n","        num_data = len(self.tensors[0][0]) # the length of the data\n","\n","        if self.shuffle== False:\n","          for start in range(0, num_data, self.batch_size):\n","              end = start + self.batch_size\n","              yield tuple(tensor[0][start:end] for tensor in self.tensors) \n","              #must be tensor[0], to access to real data\n","              # cause tensor size [1,..]; 1: is unecessary dimension\n","              # => must exclude it by using tensor[0]  \n","        else:\n","          all_batches=[] # to gather all the batches formed form the dataset\n","          for start in range(0, num_data, self.batch_size):\n","              end = start + self.batch_size\n","              all_batches.append(tuple(tensor[0][start:end] for tensor in self.tensors))\n","          \n","          # shuffle the batches: \n","          # reference: https://note.nkmk.me/en/python-random-shuffle/#:~:text=To%20randomly%20shuffle%20elements%20of,Python%2C%20use%20the%20random%20module.&text=random%20provides%20shuffle()%20that,used%20for%20strings%20and%20tuples.\n","          shuf_batches = random.sample(all_batches, len(all_batches))\n","\n","          # yield a batch in the list at a time:\n","          for i in range (0,len(shuf_batches)):\n","            yield shuf_batches[i]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K-EKsJ47N_4r","executionInfo":{"status":"ok","timestamp":1619296790371,"user_tz":-120,"elapsed":518,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"4c20a0db-a8d0-45d5-aa29-0245da2bf446"},"source":["# extract for test sample:\n","test_train_inputs= train_inputs[:,:10,:]\n","\n","test_train_start_labels= train_start_labels[:10]\n","test_train_end_labels= train_end_labels[:10]\n","\n","test_train_segment_masks = train_segment_masks[:,:10,:]\n","test_train_attention_masks = train_attention_masks[:,:10,:]\n","\n","test_train_inputs.shape, test_train_start_labels.shape, test_train_end_labels.shape, test_train_segment_masks.shape, test_train_attention_masks.shape,"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([1, 10, 444]),\n"," torch.Size([10]),\n"," torch.Size([10]),\n"," torch.Size([1, 10, 444]),\n"," torch.Size([1, 10, 444]))"]},"metadata":{"tags":[]},"execution_count":174}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bm4YTKKtOg60","executionInfo":{"status":"ok","timestamp":1619294127537,"user_tz":-120,"elapsed":530,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"f8a5824f-1c15-411f-c244-2cd65352d1ab"},"source":["# the torch.size must be torch.size([1,...])\n","# but the test_train_labels has the torch.size([10])\n","# must make it into torch.size([1,10])\n","good_test_train_start_labels=torch.unsqueeze(test_train_start_labels, 0)\n","good_test_train_end_labels=torch.unsqueeze(test_train_end_labels, 0)\n","\n","#test:\n","good_test_train_start_labels.shape, good_test_train_end_labels.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([1, 10]), torch.Size([1, 10]))"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"40z8-fA1Oak8","executionInfo":{"status":"ok","timestamp":1619294128990,"user_tz":-120,"elapsed":462,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"862169f4-f6b5-4e61-a875-11ac950fa8ed"},"source":["# Test out the BatchedIterator:\n","\n","batch_size = 5\n","train_iter = BatchedIterator(test_train_inputs, good_test_train_start_labels,good_test_train_end_labels, test_train_segment_masks,test_train_attention_masks, batch_size=batch_size)\n","\n","for train_batch, start_batch, end_batch, segment_mask_batch, attention_batch in train_iter.iterate_once():\n","  print(f'train_batch.type= {train_batch.shape}\\tstart_batch.type={start_batch.shape}\\tend_batch={end_batch.shape}\\tsegment_mask_batch={segment_mask_batch.shape}\\tattention_mask_batch={attention_batch.shape}')\n","  print(train_batch)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["train_batch.type= torch.Size([5, 449])\tstart_batch.type=torch.Size([5])\tend_batch=torch.Size([5])\tsegment_mask_batch=torch.Size([5, 449])\tattention_mask_batch=torch.Size([5, 449])\n","tensor([[  101, 25479,  2001,  ...,     0,     0,     0],\n","        [  101,  2014,  2034,  ...,     0,     0,     0],\n","        [  101, 26322,  2015,  ...,     0,     0,     0],\n","        [  101, 20773,  1005,  ...,     0,     0,     0],\n","        [  101,  3225,  1999,  ...,     0,     0,     0]])\n","train_batch.type= torch.Size([5, 449])\tstart_batch.type=torch.Size([5])\tend_batch=torch.Size([5])\tsegment_mask_batch=torch.Size([5, 449])\tattention_mask_batch=torch.Size([5, 449])\n","tensor([[  101,  6207,  3107,  ...,     0,     0,     0],\n","        [  101, 26880,  1998,  ...,     0,     0,     0],\n","        [  101,  1999,  2244,  ...,     0,     0,     0],\n","        [  101,  1999, 17403,  ...,     0,     0,     0],\n","        [  101,  5943, 16966,  ...,     0,     0,     0]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"19vUl9BMOtxW"},"source":["#DEFINE BERT MODEL FOR TRAINING"]},{"cell_type":"markdown","metadata":{"id":"8dvypaBoOyDo"},"source":["**Bert model is too large, i would use DistlledBert model which is 40 percent smaller than Bert model but still preserves 97 percent performace of the Bert model**"]},{"cell_type":"code","metadata":{"id":"Hn3nIldXPF4A"},"source":["# https://huggingface.co/transformers/model_doc/bert.html#bertforquestionanswering\n","from transformers import DistilBertForQuestionAnswering\n","\n","# reference for config a pretrained model : https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n","from transformers import DistilBertConfig"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f-uej9ESNyqw"},"source":["class ForQuestionAnsweringClassifier(nn.Module):\n","    def __init__(self,model_out_sequence_length, output_size, freeze_bert = True):\n","        super(ForQuestionAnsweringClassifier, self).__init__()\n","        # Configure DistilBERT's initialization\n","        self.bert_config = DistilBertConfig(n_layers=1, n_heads=2,qa_dropout=0.2, dim = 312)\n","                          \n","        self.Distil_Bert_model= DistilBertForQuestionAnswering(config=self.bert_config)\n","\n","        # Make DistilBERT layers untrainable\n","        for p in self.Distil_Bert_model.parameters():\n","          p.requires_grad = False\n","          p.trainable = False\n","                \n","        #Classification layer\n","        self.start_cls_layer = nn.Linear(model_out_sequence_length, output_size)\n","        self.end_cls_layer = nn.Linear(model_out_sequence_length, output_size)\n","\n","    def forward(self, data, attn_masks, start_label, end_label):\n","        bertOut= self.Distil_Bert_model(data, # the tokens representing our input\n","                attention_mask=attn_masks, start_positions= start_label, end_positions= end_label ) # the segment ids to differentiate the question and the answer\n","        start_labels= self.start_cls_layer(bertOut.start_logits)\n","        end_labels=self.start_cls_layer(bertOut.end_logits)\n","        return start_labels, end_labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4DmKLTg-SNYJ"},"source":["model = ForQuestionAnsweringClassifier(450,450) #model_out_sequence_length = output_size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qddgpBfwi-Qy"},"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3UsLwAn0S8Dp","executionInfo":{"status":"ok","timestamp":1619297937701,"user_tz":-120,"elapsed":662,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"7db15f53-11e0-4731-9861-1bf07160638d"},"source":["# Tell Pytorch to run this model on the GPU:\n","#for p in model.parameters():\n","#      p.requires_grad = False\n","model.to(device)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ForQuestionAnsweringClassifier(\n","  (Distil_Bert_model): DistilBertForQuestionAnswering(\n","    (distilbert): DistilBertModel(\n","      (embeddings): Embeddings(\n","        (word_embeddings): Embedding(30522, 312, padding_idx=0)\n","        (position_embeddings): Embedding(512, 312)\n","        (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (transformer): Transformer(\n","        (layer): ModuleList(\n","          (0): TransformerBlock(\n","            (attention): MultiHeadSelfAttention(\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (q_lin): Linear(in_features=312, out_features=312, bias=True)\n","              (k_lin): Linear(in_features=312, out_features=312, bias=True)\n","              (v_lin): Linear(in_features=312, out_features=312, bias=True)\n","              (out_lin): Linear(in_features=312, out_features=312, bias=True)\n","            )\n","            (sa_layer_norm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n","            (ffn): FFN(\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (lin1): Linear(in_features=312, out_features=3072, bias=True)\n","              (lin2): Linear(in_features=3072, out_features=312, bias=True)\n","            )\n","            (output_layer_norm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n","          )\n","        )\n","      )\n","    )\n","    (qa_outputs): Linear(in_features=312, out_features=2, bias=True)\n","    (dropout): Dropout(p=0.2, inplace=False)\n","  )\n","  (start_cls_layer): Linear(in_features=450, out_features=450, bias=True)\n","  (end_cls_layer): Linear(in_features=450, out_features=450, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":267}]},{"cell_type":"markdown","metadata":{"id":"OZKtKU2CPv1l"},"source":["#TRAINING AND EVALUATION:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oyq8pf0tPygT","executionInfo":{"status":"ok","timestamp":1619297946689,"user_tz":-120,"elapsed":522,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"3e6d34f5-f2f6-4b31-880a-8b4ef03dec69"},"source":["# Convert the data into tensor so that the model can run on the data\n","# extract for test sample:\n","train_inputs\n","evl_inputs\n","\n","train_inputs= train_inputs.to(device)\n","evl_inputs = evl_inputs.to(device)\n","\n","train_start_labels\n","train_end_labels\n","\n","train_start_labels= train_start_labels.to(device)\n","train_end_labels= train_end_labels.to(device) \n","\n","\n","evl_start_labels\n","evl_end_labels\n","\n","evl_start_labels= evl_start_labels.to(device)\n","evl_end_labels= evl_end_labels.to(device)\n","\n","train_segment_masks\n","evl_segment_masks\n","\n","\n","train_segment_masks= train_segment_masks.to(device) \n","evl_segment_masks= evl_segment_masks.to(device) \n","\n","\n","\n","train_attention_masks\n","evl_attention_masks\n","\n","\n","train_attention_masks = train_attention_masks.to(device) \n","evl_attention_masks= evl_attention_masks.to(device) \n","\n","train_inputs.shape, train_start_labels.shape, train_end_labels.shape, train_segment_masks.shape, train_attention_masks.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([1, 450, 450]),\n"," torch.Size([450]),\n"," torch.Size([450]),\n"," torch.Size([1, 450, 450]),\n"," torch.Size([1, 450, 450]))"]},"metadata":{"tags":[]},"execution_count":268}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H1tbHvc8P5Z3","executionInfo":{"status":"ok","timestamp":1619297950048,"user_tz":-120,"elapsed":513,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"30a7f791-ce23-4405-ec4a-0edafe569026"},"source":["# the torch.size must be torch.size([1,...])\n","# but the test_train_labels has the torch.size([10])\n","# must make it into torch.size([1,10])\n","good_train_start_labels=torch.unsqueeze(train_start_labels, 0)\n","good_train_end_labels=torch.unsqueeze(train_end_labels, 0)\n","\n","good_evl_start_labels=torch.unsqueeze(evl_start_labels, 0)\n","good_evl_end_labels=torch.unsqueeze(evl_end_labels, 0)\n","\n","#test:\n","good_train_start_labels.shape, good_train_end_labels.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([1, 450]), torch.Size([1, 450]))"]},"metadata":{"tags":[]},"execution_count":269}]},{"cell_type":"code","metadata":{"id":"CwUtfXUHP7GI"},"source":["batch_size = 100\n","train_iter = BatchedIterator(train_inputs, good_train_start_labels, good_train_end_labels, train_segment_masks, train_attention_masks, batch_size=batch_size, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s9n2XMBJQB4r"},"source":["criterion = nn.CrossEntropyLoss()\n","criterion = criterion.cuda()\n","optimizer = optim.Adam(model.parameters())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q6iVr4YOQDv1"},"source":["num_epochs = 15"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":502},"id":"6rUns3QxQEhH","executionInfo":{"status":"error","timestamp":1619297958743,"user_tz":-120,"elapsed":1399,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"f796d156-d654-4f94-cbf9-bcb3f665d544"},"source":["for epoch in range(num_epochs):\n","    model.train()\n","    # Training on train data\n","  \n","    for train_batch, start_batch, end_batch, segment_mask_batch, attention_batch in train_iter.iterate_once():\n","        \n","        #train_batch = train_batch.to(device)\n","        #attention_batch = attention_batch.to(device)\n","        #start_batch = start_batch.to(device)\n","        #end_batch = end_batch.to(device)\n","        # run the model on the inputs:\n","        start_predicts, end_predicts = model(train_batch, # the tokens representing our input\n","                attention_batch, start_batch, end_batch ) # the segment ids to differentiate the question and the answer\n","        \n","\n","        # FOR TASK 1:\n","        \n","        #y_out = model(X_batch)\n","        #print(y_out)\n","        # FOR TASK 1:\n","        \n","        # To understand the ouput of the model:\n","        # y_out.shape = [batch_size, output_size] # output_size = number of labels\n","        #print(f'y_out.shape= {y_out.shape} \\ny_batch.shape= {y_batch.shape} ')\n","\n","        #print(f'y_out.shape= {y_out.shape} \\ny_batch.shape= {y_batch.shape} ')\n","        #train_loss= outputs.loss\n","        print(f'start_predict.shape={start_predicts.shape}\\tstart_batch={start_batch.shape}')\n","        start_loss= criterion(start_predicts, start_batch)\n","        end_loss= criterion(end_predicts, end_batch)\n","        #print(\"loss:\")\n","        #print(loss)\n","        optimizer.zero_grad()\n","        start_loss.backward()\n","        end_loss.backward()\n","        optimizer.step()\n","        \n","    model.eval()  # or model.train(False)\n","    \n","    #Move the training data for evaluation\n","    #train_inputs[0] = train_inputs[0].to(device)\n","    #train_attention_masks[0] = train_attention_masks[0].to(device)\n","    #train_start_labels = train_start_labels.to(device)\n","    #train_end_labels = train_end_labels.to(device)\n","        \n","    # evaluation on train data:\n","    start_predicts, end_predicts = model(train_inputs[0], # the tokens representing our input\n","                train_attention_masks[0], train_start_labels , train_end_labels) # the segment ids to differentiate the question and the answer\n","\n","    start_labels= start_predicts.argmax(axis=1)\n","    end_labels=end_predicts.argmax(axis=1)\n","\n","    #train_loss= outputs.loss\n","    start_train_loss = criterion(start_predicts, train_start_labels).item()\n","    end_train_loss = criterion(end_predicts, end_labels).item()\n","    train_loss= (start_train_loss + end_train_loss)/2\n","\n","    train_start_accuracy = (torch.eq(start_labels, train_start_labels).sum() / float(len(train_start_labels))).item()\n","    train_end_accuracy = (torch.eq(end_labels, train_end_labels).sum() / float(len(train_end_labels))).item()\n","    train_accuracy = (train_start_accuracy + train_end_accuracy) /2\n","    \n","\n","    \n","    # evaluation on eval data:\n","    #Move the training data for evaluation\n","    #evl_inputs[0] = evl_inputs[0].to(device)\n","    #evl_attention_masks[0] = evl_attention_masks[0].to(device)\n","    #evl_start_labels = evl_start_labels.to(device)\n","    #evl_end_labels = evl_end_labels.to(device)\n","    \n","\n","    evl_start_predicts, evl_end_predicts = model(evl_inputs[0], # the tokens representing our input\n","                evl_attention_masks[0], evl_start_labels , evl_end_labels) # the segment ids to differentiate the question and the answer\n","\n","    start_labels= evl_start_predicts.argmax(axis=1)\n","    end_labels=evl_end_predicts.argmax(axis=1)\n","\n","    start_dev_loss = criterion(evl_start_predicts, evl_start_labels).item()\n","    end_dev_loss = criterion(evl_end_predicts, evl_end_labels).item()\n","    dev_loss= (start_train_loss + end_train_loss)/2\n","    \n","    evl_start_accuracy = (torch.eq(start_labels, evl_start_labels).sum() / float(len(evl_start_labels))).item()\n","    evl_end_accuracy = (torch.eq(end_labels, evl_end_labels).sum() / float(len(evl_end_labels))).item()\n","    evl_accuracy = (evl_start_accuracy + evl_end_accuracy) /2\n","    \n","\n","    print(f\"Epoch: {epoch} -- train loss: {train_loss} - train acc: {train_accuracy*100} - \"\n","          f\"dev loss: {dev_loss} - dev acc: {evl_accuracy*100}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["start_predict.shape=torch.Size([100, 450])\tstart_batch=torch.Size([100])\n","start_predict.shape=torch.Size([100, 450])\tstart_batch=torch.Size([100])\n","start_predict.shape=torch.Size([100, 450])\tstart_batch=torch.Size([100])\n","start_predict.shape=torch.Size([50, 450])\tstart_batch=torch.Size([50])\n","start_predict.shape=torch.Size([100, 450])\tstart_batch=torch.Size([100])\n"],"name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-273-47a2a8fe0ac3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# evaluation on train data:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     start_predicts, end_predicts = model(train_inputs[0], # the tokens representing our input\n\u001b[0;32m---> 48\u001b[0;31m                 train_attention_masks[0], train_start_labels , train_end_labels) # the segment ids to differentiate the question and the answer\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mstart_labels\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mstart_predicts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-264-90973558d185>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data, attn_masks, start_label, end_label)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         bertOut= self.Distil_Bert_model(data, # the tokens representing our input\n\u001b[0;32m---> 20\u001b[0;31m                 attention_mask=attn_masks, start_positions= start_label, end_positions= end_label ) # the segment ids to differentiate the question and the answer\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mstart_labels\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_cls_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbertOut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mend_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_cls_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbertOut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    707\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m         )\n\u001b[1;32m    711\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistilbert_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# (bs, max_query_len, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m         )\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             layer_outputs = layer_module(\n\u001b[0;32m--> 307\u001b[0;31m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m             )\n\u001b[1;32m    309\u001b[0m             \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         )\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, q_length, k_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, q_length, k_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, q_length, k_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1581\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1583\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1584\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1585\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 696.00 MiB (GPU 0; 7.43 GiB total capacity; 5.86 GiB already allocated; 288.81 MiB free; 6.36 GiB reserved in total by PyTorch)"]}]},{"cell_type":"markdown","metadata":{"id":"pRh6bYOkhmy3"},"source":["#SAVE & LOAD trained model:"]},{"cell_type":"markdown","metadata":{"id":"UFmtvnp1iJdv"},"source":["##TO SAVE"]},{"cell_type":"code","metadata":{"id":"udLMKfl3hrFK"},"source":["torch.save(model.state_dict(), './model/Trained_model.tsv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qt2hSNthiZ9z"},"source":["##TO LOAD:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"swovu72_ib74","executionInfo":{"status":"ok","timestamp":1619296828684,"user_tz":-120,"elapsed":1693,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"104861e3-3ea9-4ce9-e144-4ba81928502d"},"source":["device = torch.device(\"cuda\")\n","model = ForQuestionAnsweringClassifier(449,449)\n","model.load_state_dict(torch.load('./model/Trained_model.tsv'))\n","model.to(device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["ForQuestionAnsweringClassifier(\n","  (Distil_Bert_model): DistilBertForQuestionAnswering(\n","    (distilbert): DistilBertModel(\n","      (embeddings): Embeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (transformer): Transformer(\n","        (layer): ModuleList(\n","          (0): TransformerBlock(\n","            (attention): MultiHeadSelfAttention(\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (ffn): FFN(\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          )\n","        )\n","      )\n","    )\n","    (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n","    (dropout): Dropout(p=0.2, inplace=False)\n","  )\n","  (start_cls_layer): Linear(in_features=449, out_features=449, bias=True)\n","  (end_cls_layer): Linear(in_features=449, out_features=449, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":178}]},{"cell_type":"markdown","metadata":{"id":"0Q_PbhaRkz02"},"source":["**To Run on the loaded model, just go back to 'TRAINING AND EVALUATION' section.**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":183},"id":"2ky4CxrWbzDx","executionInfo":{"status":"error","timestamp":1619273686423,"user_tz":-120,"elapsed":733,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"38804bde-cc97-4e9b-a949-3587f65159f5"},"source":["import gc\n","del model\n","gc.collect()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-56-03fd7919080e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'gc' is not defined"]}]}]}